
% The \phantomsection command is needed to create a link to a place in the document that is not a
% figure, equation, table, section, subsection, chapter, etc.
% https://tex.stackexchange.com/questions/44088/when-do-i-need-to-invoke-phantomsection
\phantomsection

\chapter{Solution Prediction Models for MILP Problems}\label{chap:solution-prediction}


This chapter introduces the methods available for training deep learning models for predicting solutions of MILP problems.
The ability to efficiently predict solutions plays a pivotal role in the development of learning-based heuristics.
In other words, this chapter is a bridge between Chapters \ref{chap:integer-programming} and \ref{chap:deep-learning} with a focus on (mat)heuristics.

This chapter begins by discussing the process of embedding of MILP problems, which involves transforming problem instances into a suitable format for deep learning models.
Within this context, feature engineering and graph approaches are explored to represent the intricate relationships between the components of MILP problems.
Moving forward, the methodologies employed in training deep learning models fed with embeddings of MILP problem instances are presented, highlighting the challenges and opportunities posed by the availability of multiple feasible solutions.
The chapter ends with the approaches one can use to create primal (mat)heuristics from solution prediction models.


\section{Embedding Optimization Problems}\label{sec:embedding}

The first requirement that needs to be satisfied for training deep learning models to predict solutions to MILP problems is to be able to feed instances of MILP problems to such models.
For this, it is necessary to convert an instance to a numerical format that the model can handle.

Naturally, an instance can be specified by a tuple $\left( \bm{c}, \bm{b}, A, n \right)$, as discussed in Sec.~\ref{sec:milp-definition}, which could be vectorized and input to a vanilla NN.
This form of embedding, which is going to be referred to as \emph{naïve} embedding, has several shortcomings.
First, it does not represent the \emph{symmetries} of the formulation, which are operations applied to the parameters that do not alter its solutions.
For example, changing the order of the constraints, which can be seen as permutations of rows of $\left[ A\, | \,\bm{b} \right] $, does not affect in any way the feasible space nor the objectives associated to feasible solutions, but generates different embeddings.

Furthermore, the naïve embedding can easily be an over-parametrization of the instance distribution, which often are sampled from a lower-dimensional space.
For example, take the MILP formulation of the TSP by \citeonline{millerIntegerProgrammingFormulation1960},
\begin{align*}
    \min_{\bm{u},\bm{y}} \quad & \sum_{\substack{i,j=0 \\ i\neq j}}^{n} d_{ij} y_{ij} & \\
    \textrm{s.t.} \quad & \sum_{\substack{i=0\\i\neq j}}^{n} y_{ij} = 1, &\, j=1,\ldots,n \\
			& \sum_{\substack{j=0\\j\neq i}}^{n} y_{ij} = 1, &\, i=1,\ldots,n \\
			& u_i - u_j + n \cdot y_{ij} \le n - 1, &\, i,j=1,\ldots,n,\, i\neq j\\
			& y_{ij} \in \left\{ 0,1 \right\}, &\, i,j=0,\ldots,n  \\
			& \bm{u} \in \R^{n} &
\end{align*}
and suppose one wants to solve it for instance $I\in \mathcal{I}$ over the same graph but with varying edge costs $d_{ij}$.
Of course, embedding such instances naïvely would encode all the static parameters of the constraints, i.e., the information that does not change between the instances of interest, which do not carry relevant information for the model.


\subsection{Feature Engineering}

One way to mitigate the shortcomings of the naïve embedding is to extract \emph{features} that well represent the instance with respect to their solutions.
This approach is based on the hypothesis that, for a given application, the instances are sampled from a lower-dimensional space, i.e., that there exists a mapping $g^{-1}: X\subseteq\R^{d} \longrightarrow \mathcal{I}$ that associates features $x\in X$ to instances $I\in \mathcal{I}$, and that $d$ is significantly smaller than the number of parameters (e.g., from the naïve embedding).
The mapping is written as the \emph{inverse} of a function $g$ because, in practice, it is not necessary to know $g^{-1}$ to be able to train a deep learning model, only $g$, i.e., it is only necessary to compute features given instances, and assume that the inverse is possible.

Continuing with the TSP example from above, suppose that the goal is to solve the TSP for a given city (which fixes the graph over which the tours are to be found) but with different traffic conditions and, therefore, different edge costs $d_{ij}$.
The cost vector $\bm{c}$ (which is a vectorization of the $d_{ij}$ parameters) can be said a feature vector for the instances, but calling this feature engineering would be a controversial statement.
However, one could investigate what are the variables that influence the traffic conditions, e.g., hour of the day, day of the week, gas price, weather.
Ideally, then, it would be possible to use these variables to define a feature space $X$, such that a mapping $g^{-1}: X \longrightarrow \mathcal{I}$ exists, and train models that are input with $x\in X$.

Embedding MILP problem instances as feature vectors is an approach suitable for NNs, as they require vector-valued inputs.
However, there is an underlying restriction that is a fixed number of features.
Although it seems natural, it is not always the case that all instances of a problem have the same number of variables or constraints.
If in the TSP example above the underlying graph changes over the instance space, then the instances will have varying numbers of variables and constraints.
In the naïve embedding, this translates directly to vectors of varying size, which are not directly suitable for NNs.
To generate features that are suitable for NNs even when the instances have varying size, the feature engineer must be able to translate the process that changes the size of the instances into a fixed number of features, which is not always easy or even feasible.

\subsection{Graph Embedding}\label{sec:graph-embedding}

A well-used approach in the intersection between deep learning and combinatorial optimization is to embed MILP problem instances is through bipartite graphs~\cite{gasseExactCombinatorialOptimization2019,nairSolvingMixedInteger2021,dingAcceleratingPrimalSolution2020,khalilMIPGNNDataDrivenFramework2022,hanGNNGuidedPredictandSearchFramework2023}.
Any instance of an LP problem can be represented as a weighted bipartite graph.
Consider the problem
\begin{equation}\label{eq:example-lp-graph}
\begin{aligned}
    \max_{\bm{y}} & \quad \bm{c}^T \bm{y} \\
    \text{s.t.:} & \quad A\bm{y} \le\bm{b} 
,\end{aligned}
\end{equation}
where $\bm{y}\in Y \subseteq\mathbb{R}^n$ and $\bm{b}\in \mathbb{R}^m$.
It is possible to build a bipartite graph $G=(V_{\textrm{var}}\cup V_{\textrm{con}}, E)$, in which $v_{\textrm{con},i}\in V_{\textrm{con}}$ is the node associated to the $i$-th constraint, $v_{\textrm{var},j}\in V_{\textrm{var}}$ is the node associated to $y_j$, and $E=\{(v_{{\rm con},i},v_{{\rm var},j}) : A_{i,j} \neq 0\}$.
Furthermore, a weight function $w: V_{\textrm{var}}\cup V_\textrm{con}\cup E \longrightarrow \R$ such that $w(v_{\textrm{var},j}) = c_j$, $w(v_{\textrm{con},i}) = b_i$, and $w(e_{i,j}=(v_{\textrm{con},i},v_{\textrm{var},j})) = A_{i,j}$, renders the weighted graph $(G,w)$ a complete representation of any instance of the LP, i.e., the original LP instance can be reconstructed using solely the information in such weighted graph.

The extension to MILP problems requires solely the distinction between continuous and integer variables.
This can be done, for example, by extending the weight function to a vector-valued function such that $w(v_{\textrm{var},j}) = (c_j,0)$ if the $j$-th variable is continuous or $w(v_{\textrm{var},j}) = (c_j,1)$ if $x_j$ is an integer variable.
In practice, however, the graph fed to a GNN is usually ``weighted'' with feature vectors $\bm{h}_v^{(0)}, \forall v\in V$ of arbitrary size, as seen in Sec.~\ref{sec:gnns}.
In other words, the information contained in the weights (feature vectors) provided to the network is a design choice: it can contain the weights described above, but many other features might also help the model learn the graph-related task (see, for example, \citeonline{gasseExactCombinatorialOptimization2019} and \citeonline{nairSolvingMixedInteger2021}).

The graph embedding is perfectly suitable for GNNs.
In comparison to the feature engineering approach, the graph embedding requires no effort from an human expert, and provides an effective result in terms of representation power and scalability.
First, because the resulting graph contains all of the information present in the instance while being invariant to constraint and variable permutations.
On top of that, the size of the GNN does no scale with the size of the graph, but solely with the number of weights (dimension of the feature vector) associated to each node.


\section{Training Under Supervision}\label{sec:training-solution-prediction}

Ideally, a solution prediction model is capable of predicting the \emph{bias} of the integer variables in the optimal solutions of a given instance of an MILP problem~\cite{khalilMIPGNNDataDrivenFramework2022}.
Intuitively, the bias of a variable towards a value indicates how likely that variable is to assume that value in an optimal solution.
As the problem is linear over the continuous variables, their optimal value value can be determined in polynomial time given an optimal assignment for the integer variables, as the resulting problem is an LP.
Therefore, the focus of solution prediction models for MILP is usually the integer variables.

More precisely, let $I\in \mathcal{I}$ be an instance of an MILP problem as in \eqref{eq:general-milp}.
The \emph{bias} of variable $y_j$ towards value $k\in \Z$ in the optimal solution will be denoted $p(y_j^*=k|I)$, in an allusion to its \emph{probability} of taking said value in an optimal solution $\bm{y}^*$, which also implies that it is expected that $\sum_{k} p(y_j^*=k|I) = 1$, $\forall j$.
Therefore, given an embedding $x\in \mathcal{X}$\footnote{Here, $\mathcal{X}$ is used to denote a more general embedding space, that can that of feature vectors or of graph embeddings.} associated to an instance of an optimization problem, a solution prediction deep learning model $f_{\theta}: \mathcal{X} \longrightarrow \mathcal{P}$ will ideally be such that, for $\hat{\bm{p}}=f_\theta(x)$, it is expected that, $\forall j$, $\hat{p}_{j,k}\approx p(y_j^*=k|I)$.

Given an embedding function and a suitable deep learning model (e.g., naïve embedding or engineered features and a NN, or graph embedding and GNN), the usual training algorithms for supervised learning apply.
In other words, following a match between instance embedding and model architectures, the algorithmic approach presented in Sec.~\ref{sec:supervised-learning} applies.
Therefore, the dataset required for training is composed of embeddings of instances associated to optimal solutions.
Let $\bm{y}^{*}_I$ denote an optimal solution for instance $I\in \mathcal{I}$ of the MILP problem at hand, and let $g: \mathcal{I} \longrightarrow \mathcal{X}$ be a suitable embedding function.
Then, the dataset necessary for training can be written as a set \[
    \mathcal{D} = \left\{ (x_I, \bm{y}^{*}_I) : I \in \mathcal{I}, x_I = g(I) \text{, and } \bm{y}^*_I\text{ is an optimal solution of }I \right\} 
.\] 

Given such dataset, the training algorithm can be defined by picking any loss function that penalizes the distance between the predicted bias and the actual value.
For example, following a maximum likelihood estimation approach~\cite{goodfellowQualitativelyCharacterizingNeural2015} for a problem solely with binary variables, the binary cross-entropy loss can be applied to a model $f_{\theta}: \mathcal{X} \longrightarrow \left[ 0,1 \right]^n$ such that
\begin{equation}\label{eq:bce-loss}
    \ell(\bm{y}, \hat{\bm{p}}) = \sum_{j=1}^{n} y_j \log \hat{p}_j + (1-y_j) \log (1 - \hat{p}_j)
.\end{equation}
Note that, because there are only binary variables, the model is designed with output only for the bias towards $k=1$, as $\hat{p}_j\approx p(y_j^*=1|I) \iff 1-\hat{p}_j\approx p(y_j^*=0|I)$.


\subsection{Multiple Targets}

Instead of approximating the bias of the optimal solution, \citeonline{nairSolvingMixedInteger2021} proposed to approximate the bias of the \emph{near}-optimal solutions.
Intuitively, this approach provides the model with more information on the feasible region of the problem, and empirical results suggest that it has improved performance in the construction of heuristics~\cite{khalilMIPGNNDataDrivenFramework2022,hanGNNGuidedPredictandSearchFramework2023}.
A proper definition of what will be referred to as a \emph{multiple targets} training follows.

Given an instance of an optimization problem $I\in \mathcal{I}$\footnote{In the following, the reference to $I$ is omitted to ease the notation, but the definitions are specific to an instance.}, let \[
Y_\varepsilon = \left\{ \bm{y} \in Y: \bm{c}^T\bm{y} \le \allowbreak (1+\varepsilon) \bm{c}^T\bm{y}^*_{I} \right\}
\] be the set of $\varepsilon$-optimal solutions, that is, the set of feasible solutions that are within $\varepsilon$ distance (in relative terms of the cost) of the optimal solution $\bm{y}^*$.
The multiple-targets approach implies that the output of a solution prediction deep learning model $f_{\theta}$ approximates the bias of the variables in the solutions in a set $Y_\varepsilon$, i.e., $\hat{p}_{j,k} \approx p(y_j = k | \bm{y} \in Y_\varepsilon)$.
For that, \citeonline{nairSolvingMixedInteger2021} propose to weight a loss function such as \eqref{eq:bce-loss} by the cost associated to each solution in $Y_\varepsilon$.
Therefore, the dataset $\mathcal{D}$ necessary for training will contain pairs of the form $(x,Y_\varepsilon)$, where $x$ is an embedding of an instance of an MILP problem, and $Y_\varepsilon$ is a set of $\varepsilon$-optimal solutions of the same instance.
In other words, the cost function becomes \[
    \mathcal{L}(\theta) = \frac{1}{|\mathcal{D}|} \sum_{(x,Y_\varepsilon)\in \mathcal{D}} \sum_{\bm{y}\in Y_\varepsilon}  \frac{e^{-\bm{c}^T \bm{y}}}{\sum_{\bm{y}'\in Y_\varepsilon} e^{-\bm{c}^T \bm{y}'}} \ell(\bm{y},f_\theta(x))
.\] Note that multiple feasible solutions are taken into consideration for each instance of the MILP problem in our dataset, hence the name ``multiple targets.''

\section{Learning-based Heuristics}\label{sec:learning-based-heuristics}

Given a properly trained solution prediction deep learning model, there are many ways to generate a primal heuristic.
The most naïve heuristic, perhaps, would be to take the model's output directly.
However, it is very unlikely that, even for models trained extensively on large datasets, the output will have a high feasibility rate on realistic problems of a reasonable size.
That is so because the characteristics of the feasible region usually make so that a single deviation (i.e., a single bit flip) can render an optimal solution infeasible.
Therefore, the probabilistic nature of deep learning makes it very difficult to achieve a reasonable feasibility rate on problems with many variables, as ensuring output constraints on deep learning models is a difficult challenge (see, e.g., \citeonline{chamonConstrainedLearningInference2020}).

An alternative to balance the speed of solution prediction models with better feasibility (and optimality) expectations is to explore matheuristics (see Sec.~\ref{sec:matheuristics}).
In this section, three structures of matheuristics that use solution prediction models are presented.

\subsection{Warm-starting MILP Solvers}

A straightforward approach to is to use the output of a solution prediction model to provide (partial) solutions to a solver, warm-starting the optimization.
For example, the SCIP solver~\cite{bestuzhevaSCIPOptimizationSuite2021} accepts complete and partial solutions, which are used to guide the inner heuristics of the optimization algorithm.
We use the output of the model to determine which variables will compose the partial solution provided to the solver based on the \emph{confidence} of the model's prediction.
Such confidence is based on how strong the predicted bias is, i.e., the probability of the predicted value.
In other words, the closer the model's output $\hat{p}_{j,k}$ is to 1, the more confident the model is that the $y_j$ variable should take value $k$ in an optimal solution.

Formally, given an instance $I\in \mathcal{I}$ for which $x\in X$ is an adequate embedding, we have $\hat{\bm{p}} = f_\theta(x)$ the output of the model.
The model's predicted solution is a vector $\hat{\bm{y}}$ such that \[
    \hat{y}_j = \arg\max_{k} \hat{p}_{j,k},\, j=1,\ldots,n
.\] 
A partial solution based on the model's confidence is a set
\begin{equation}\label{eq:partial-solution}
    \overline{\bm{y}}^{(\alpha)} = \left\{ (j,\hat{y}_j) : \hat{y}_j = \arg\max_{k} \hat{p}_{j,k}\text{ and } \hat{p}_{j,\hat{y}_j} \ge  \alpha \right\}
,\end{equation}
in which $\alpha \in [0,1]$ is the \emph{confidence threshold}.
The diagram of Figure~\ref{fig:warm-starting-diagram} illustrates the building blocks of a warm-start based on a solution prediction deep learning model.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
	\node (I) {$I$};
	\begin{scope}[every node/.style={draw}, align = center, minimum height = 1cm, minimum width = 1cm]
	    \node[right = of I] (g) {$g$};
	    \node[right = 1.5cm of g] (f) {$f_{\theta}$};
	    \node[right = 1.5cm of f] (conf) {Confidence};
	    \node[right = 1.5cm of conf] (solver) {MILP\\Solver};
	\end{scope}
	\node[right = of solver] (opt) {$\bm{y}^*$};

	\path [-latex] (I) edge (g);
	\path [-latex] (g) edge node [above] {$x$} (f);
	\path [-latex] (f) edge node [above] {$\hat{\bm{p}}$} (conf);
	\path [-latex] (conf) edge node [above] {$\overline{\bm{y}}^{(\alpha)}$} (solver);
	\path [-latex] (solver) edge (opt);
	\draw [-latex] (I.south) -- ++(0,-0.6cm) -| (solver.south);

    \end{tikzpicture}
    \caption{Warm-starting an MILP solver with the output of a solution prediction deep learning model. In the diagram,  $I$ is an instance of an MILP problem for which the model was trained. $g$ is an adequate embedding function. The ``Confidence'' block indicates the construction of a partial solution as described in Equation \eqref{eq:partial-solution}.}
    \label{fig:warm-starting-diagram}
\end{figure}

Note that warm-starting an MILP solver by itself does not configure a heuristic solution, as the optimality guarantees are maintained.
More precisely, warm starting a solver will only (potentially) change the order in which the nodes of the branch-and-bound tree are explored, e.g., by influencing branching decisions.
Because of that, it also maintains the optimality (and feasibility) guarantees of the MILP solver used, even if the solution prediction model provides an infeasible (partial) solution.
However, under limited time, those guarantees are lost, as the solver may be interrupted before finding even a feasible solution.
In fact, even without the warm-starting approach, the simplest matheuristic is to interrupt an MILP solver after a fixed amount of time.


\subsection{Early-fixing Variable Assignments}

Beyond merely indicating to the solver the partial solution that the deep learning model provides, it is possible to constrain the problem with that partial solution.
This early-fixing approach, also called neural diving by \citeonline{nairSolvingMixedInteger2021}, can be interpreted, given an instance $I\in \mathcal{I}$ and a partial solution as in Equation~\eqref{eq:partial-solution}, as the addition of constraints
\begin{equation}\label{eq:early-fixing-constraints}
    y_{j} = \hat{y}_j,\,\forall (j,\hat{y}_j) \in \overline{\bm{y}}^{(\alpha)}
\end{equation}
to the optimization problem.
Because such constraints limit those variables to assuming a single value, which is effectively the same as removing them from the poll of decision variables and treating them as parameters of the problem, the branch-and-bound tree gets significantly pruned.
The diagram of Figure~\ref{fig:early-fixing-diagram} illustrates this process.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
	\node (I) {$I$};
	\begin{scope}[every node/.style={draw}, align = center, minimum height = 1cm, minimum width = 1cm]
	    \node[right = of I] (g) {$g$};
	    \node[right = 1.5cm of g] (f) {$f_{\theta}$};
	    \node[right = 1.5cm of f] (conf) {Confidence};
	    \node[right = 1.5cm of conf] (ef) {Early\\Fixing};
	    \node[right = 1.5cm of ef] (solver) {MILP\\Solver};
	\end{scope}
	\node[right = of solver] (opt) {$\hat{\bm{y}}^*$};

	\path [-latex] (I) edge (g);
	\path [-latex] (g) edge node [above] {$x$} (f);
	\path [-latex] (f) edge node [above] {$\hat{\bm{p}}$} (conf);
	\path [-latex] (conf) edge node [above] {$\overline{\bm{y}}^{(\alpha)}$} (ef);
	\path [-latex] (ef) edge node [above] {$\overline{I}$} (solver);
	\path [-latex] (solver) edge (opt);
	\draw [-latex] (I.south) -- ++(0,-0.6cm) -| (ef.south);

    \end{tikzpicture}
    \caption{Early-fixing integer variables based on the output of a solution prediction deep learning model. In the diagram,  $I$ is an instance of an MILP problem for which the model was trained. $g$ is an adequate embedding function. The ``Confidence'' block indicates the construction of a partial solution as described in Equation \eqref{eq:partial-solution}. The ``Early Fixing'' block indicates the addition of constraints \eqref{eq:early-fixing-constraints}, resulting in the early-fixed instance $\overline{I}$. The solver output is written $\hat{\bm{y}}^*$ to indicate it as being a heuristic solution.}
    \label{fig:early-fixing-diagram}
\end{figure}

Naturally, the addition of the early-fixed constraints implies that there are no guarantees that the solution found is optimal.
In fact, the resulting problem instance (denoted $\overline{I}$, as in Figure~\ref{fig:early-fixing-diagram}) might be infeasible, if the added constraints come from an infeasible assignment.
However, by adjusting the parameter $\alpha$, which controls the size of the partial solution, it is possible to indirectly adjust the size of the resulting branch-and-bound tree, as a smaller value of $\alpha$ implies in more variables in the partial solution, thus, more fixing constraints, which result in a smaller tree. 
In fact, $\alpha\to 0$ reduces the early-fixing matheuristic to the naïve use of the solution predicted by the deep learning model.
Furthermore, it is easy to see that, instead of a fixed threshold $\alpha$, one can build partial solutions by picking the $n-n'$ ($n' < n$) variables that the model is most confident of, such that the resulting problem instance always has $n'$ variables and, thus, can be solved in a tractable manner.


\subsection{Trust-region}

Instead of strictly fixing the variables based on the model's output, \citeonline{hanGNNGuidedPredictandSearchFramework2023} have proposed to allow a small deviation from that value.
In other words, the solution prediction model's output is used to define a \emph{trust region} in which an MILP solver can search for the optimal solution.
Instead of constraints like in Equation~\eqref{eq:early-fixing-constraints}, the instance is modified with the addition of constraints of the form
\begin{equation}\label{eq:trust-region-constraint}
    \sum_{(j,\hat{y}_j) \in \overline{\bm{y}}^{(\alpha)}} |y_{j} - \hat{y}_j| \le \Delta
,\end{equation}
where $\Delta \in \R_+$ defines the size of the trust region.
Note how the above equation limits the space of feasible solutions to a neighborhood of the partial solution derived from the model's output.
The diagram in Figure~\ref{fig:trust-region-diagram} illustrates the trust region heuristic approach.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
	\node (I) {$I$};
	\begin{scope}[every node/.style={draw}, align = center, minimum height = 1cm, minimum width = 1cm]
	    \node[right = of I] (g) {$g$};
	    \node[right = 1.5cm of g] (f) {$f_{\theta}$};
	    \node[right = 1.5cm of f] (conf) {Confidence};
	    \node[right = 1.5cm of conf] (tr) {Trust\\Region};
	    \node[right = 1.5cm of tr] (solver) {MILP\\Solver};
	\end{scope}
	\node[right = of solver] (opt) {$\hat{\bm{y}}^*$};

	\path [-latex] (I) edge (g);
	\path [-latex] (g) edge node [above] {$x$} (f);
	\path [-latex] (f) edge node [above] {$\hat{\bm{p}}$} (conf);
	\path [-latex] (conf) edge node [above] {$\overline{\bm{y}}^{(\alpha)}$} (tr);
	\path [-latex] (tr) edge node [above] {$\overline{I}^{(\Delta)}$} (solver);
	\path [-latex] (solver) edge (opt);
	\draw [-latex] (I.south) -- ++(0,-0.6cm) -| (ef.south);

    \end{tikzpicture}
    \caption{Solving an instance of an MILP problem within a trust region based on the output of a solution prediction deep learning model. In the diagram,  $I$ is an instance of an MILP problem for which the model was trained. $g$ is an adequate embedding function. The ``Confidence'' block indicates the construction of a partial solution as described in Equation \eqref{eq:partial-solution}. The ``Trust Region'' block indicates the addition of the constraint \eqref{eq:trust-region-constraint}, resulting in the instance $\overline{I}^{(\Delta)}$ with limited solution space. The solver output is written $\hat{\bm{y}}^*$ to indicate it as being a heuristic solution.}
    \label{fig:trust-region-diagram}
\end{figure}

It is easy to see that picking $\Delta=0$ results in the early-fixing approach.
A distinguishing feature of the trust region approach is that the parameter $\Delta$ can be adjusted to turn an infeasible instance into a feasible one, or, perhaps, to include a better solution in the feasible region.
However, no optimality nor feasibility guarantees can be provided by this approach.

