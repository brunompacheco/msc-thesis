
% The \phantomsection command is needed to create a link to a place in the document that is not a
% figure, equation, table, section, subsection, chapter, etc.
% https://tex.stackexchange.com/questions/44088/when-do-i-need-to-invoke-phantomsection
\phantomsection

\chapter{Solution Prediction Models for MILP Problems}\label{chap:solution-prediction}


This chapter introduces the methods available for training deep learning models for predicting solutions of MILP problems.
The ability to efficiently predict solutions plays a pivotal role in the development of learning-based heuristics.
In other words, this chapter is a bridge between Chapters \ref{chap:integer-programming} and \ref{chap:deep-learning} with a focus on (mat)heuristics.

This chapter begins by discussing the process of embedding of MILP problems, which involves transforming problem instances into a suitable format for deep learning models.
Within this context, feature engineering and graph approaches are explored to represent the intricate relationships between the components of MILP problems.
Moving forwards, the methodologies employed in training deep learning models fed with embeddings of MILP problem instances are presented, highlighting the challenges and opportunities posed by the availability of multiple feasible solutions.
The chapter ends with the approaches one can use to create primal (mat)heuristics from solution prediction models.


\section{Embedding Optimization Problems}

% TODO: see SatGNN paper

The first requirement that needs to be satisfied for training deep learning models to predict solutions to MILP problems is to be able to feed instances of MILP problems to such models.
For this, it is necessary to convert an instance to a numerical format that the model can handle.

Naturally, an instance can be specified by a tuple $\left( \bm{c}, \bm{b}, A, n \right)$, as discussed in Sec.~\ref{sec:milp-definition}, which could be vectorized an be fed to a vanilla NN.
This form of embedding, which is going to be referred to as \emph{naïve} embedding, has several shortcomings.
First, it does not represent the \emph{symmetries} of the formulation, which are operations applied to the parameters that do not alter its solutions.
For example, changing the order of the constraints, which can be seen as permutations of rows of $\left[ A\, | \,\bm{b} \right] $, does not affect in any way the feasible space nor the objectives associated to feasible solutions, but generates different embeddings.

Furthermore, the naïve embedding can easily be an over-parametrization of the instance distribution, which often are sampled from a lower-dimensional space.
For example, take the MILP formulation of the TSP by \citeonline{millerIntegerProgrammingFormulation1960}
\begin{align*}
    \min_{\bm{u},\bm{y}} \quad & \sum_{\substack{i,j=0 \\ i\neq j}}^{n} d_{ij} y_{ij} & \\
    \textrm{s.t.} \quad & \sum_{\substack{i=0\\i\neq j}}^{n} y_{ij} = 1, &\, j=1,\ldots,n \\
			& \sum_{\substack{j=0\\j\neq i}}^{n} y_{ij} = 1, &\, i=1,\ldots,n \\
			& u_i - u_j + n \cdot y_{ij} \le n - 1, &\, i,j=1,\ldots,n,\, i\neq j\\
			& y_{ij} \in \left\{ 0,1 \right\}, &\, i,j=0,\ldots,n  \\
			& \bm{u} \in \R^{n} &
\end{align*}
and suppose one wants to solve it for instance $I\in \mathcal{I}$ over the same graph but with varying edge costs $d_{ij}$.
Of course, embedding such instances naïvely would encode all the static parameters of the constraints, i.e., the information that does not change between the instances of interest, which do not carry relevant information for the model.


\subsection{Feature Engineering}

One way to mitigate the shortcomings of the naïve embedding is to extract \emph{features} that well represent the instance with respect to their solutions.
This approach is based on the hypothesis that, for a given application, the instances are sampled from a lower-dimensional space, i.e., that there exists a mapping $g^{-1}: X\subseteq\R^{d} \longrightarrow \mathcal{I}$ that associates features $x\in X$ to instances $I\in \mathcal{I}$, and that $d$ is significantly smaller than the number of parameters (e.g., from the naïve embedding).
The mapping is written as the \emph{inverse} of a function $g$ because, in practice, it is not necessary to know $g^{-1}$ to be able to train a deep learning model, only $g$, i.e., it is only necessary to compute features given instances, and assume that the inverse is possible.

Continuing with the TSP example from above, suppose that the goal is to solve the TSP for a given city (which fixes the graph over which the tours are to be found) but with different traffic conditions and, therefore, different edge costs $d_{ij}$.
The cost vector $\bm{c}$ (which is a vectorization of the $d_{ij}$ parameters) can be said a feature vector for the instances, but calling this feature engineering would be controversial statement.
However, one could investigate what are the variables that influence the traffic conditions, e.g., hour of the day, day of the week, gas price, weather.
Ideally, then, it would be possible to use these variables to define a feature space $X$, such that a mapping $g^{-1}: X \longrightarrow \mathcal{I}$ exists, and train models that are fed with $x\in X$.

Embedding MILP problem instances as feature vectors is an approach suitable for NNs, as they require vector-valued inputs.
However, there is an underlying restriction that is a fixed number of features.
Although it seems natural, it is not always the case that all instances of a problem have the same number of variables or constraints.
If in the TSP example above the underlying graph changes over the instance space, then the instances will have varying numbers of variables and constraints.
In the naïve embedding, this translates directly to vectors of varying size, which are not directly suitable for NNs.
To generate features that are suitable for NNs even when the instances have varying size, the feature engineer must be able to translate the process that changes the size of the instances into a fixed number of features, which is not always easy or even feasible.

\subsection{Graph Embedding}

A well-used approach in the intersection between deep learning and combinatorial optimization is to embed MILP problem instances is through bipartite graphs~\cite{gasseExactCombinatorialOptimization2019,nairSolvingMixedInteger2021,dingAcceleratingPrimalSolution2020,khalilMIPGNNDataDrivenFramework2022,hanGNNGuidedPredictandSearchFramework2023}.
Any instance of an LP problem can be represented as a weighted bipartite graph.
Consider the problem
\begin{equation}\label{eq:example-lp-graph}
\begin{aligned}
    \max_{\bm{y}} & \quad \bm{c}^T \bm{y} \\
    \text{s.t.:} & \quad A\bm{y} \le\bm{b} 
,\end{aligned}
\end{equation}
where $\bm{y}\in Y \subseteq\mathbb{R}^n$ and $\bm{b}\in \mathbb{R}^m$.
It is possible to build a bipartite graph $G=(V_{\textrm{var}}\cup V_{\textrm{con}}, E)$, in which $v_{\textrm{con},i}\in V_{\textrm{con}}$ is the node associated to the $i$-th constraint, $v_{\textrm{var},j}\in V_{\textrm{var}}$ is the node associated to $y_j$, and $E=\{(v_{{\rm con},i},v_{{\rm var},j}) : A_{i,j} \neq 0\}$.
Furthermore, a weight function $w: V_{\textrm{var}}\cup V_\textrm{con}\cup E \longrightarrow \R$ such that $w(v_{\textrm{var},i}) = c_j$, $w(v_{\textrm{con},j}) = b_j$, and $w(e_{i,j}=(v_{\textrm{con},i},v_{\textrm{var},j})) = A_{i,j}$, renders the weighted graph $(G,w)$ a complete representation of any instance of the LP, i.e., the original LP instance can be reconstructed using solely the information in such weighted graph.

The extension to MILP problems requires solely the distinction between continuous and integer variables.
This can be done, for example, by extending the weight function to a vector-valued function such that $w(v_{\textrm{var},j}) = (c_j,0)$ if the $j$-th variable is continuous or $w(v_{\textrm{var},j}) = (c_j,1)$ if $x_j$ is an integer variable.
In practice, however, the graph fed to a GNN is usually "weighted" with feature vectors $\bm{h}_v^{(0)}, \forall v\in V$ of arbitrary size, as seen in Sec.~\ref{sec:gnns}.
In other words, the information contained in the weights (feature vectors) provided to the network is a design choice.
It can contain the weights described above, but many other features might also help the model learn the graph-related task (see, for example, \citeonline{gasseExactCombinatorialOptimization2019} and \citeonline{nairSolvingMixedInteger2021}).

The graph embedding is perfectly suitable for GNNs.
In comparison to the feature engineering approach, the graph embedding requires no effort from an human expert, and provides an effective result in terms of representation power and scalability.
First, because the resulting graph contains all of the information present in the instance while being invariant to constraint and variable permutations.
On top of that, the size of the GNN does no scale with the size of the graph, but solely with the dimension of the weight (feature vector) associated to each node.


\section{Training Under Supervision}

Ideally, a solution prediction model is capable of predicting the \emph{bias} of the integer variables in the optimal solutions of a given instance of an MILP problem~\cite{khalilMIPGNNDataDrivenFramework2022}.
Intuitively, the bias of a variable towards a value indicates how likely that variable is to assume that value in an optimal solution.
As the problem is linear over the continuous variables, their optimal value value can be determined in polynomial time given an optimal assignment for the integer variables, as the resulting problem is an LP.
Therefore, the focus of solution prediction models for MILP is usually the integer variables.

More precisely, let $I\in \mathcal{I}$ be an instance of an MILP problem as in \eqref{eq:general-milp}.
The \emph{bias} of variable $y_j$ towards value $k\in \Z$ in the optimal solution will be denoted $p(y_j^*=k|I)$, in an allusion to its \emph{probability} of taking said value in an optimal solution $\bm{y}^*$, which also implies that it is expected that $\sum_{k} p(y_j^*=k|I) = 1$, $\forall j$.
Therefore, given an embedding $x\in \mathcal{X}$\footnote{Here, $\mathcal{X}$ is used to denote a more general embedding space, that can that of feature vectors or of graph embeddings.} associated to an instance of an optimization problem, a solution prediction deep learning model $f_{\theta}: \mathcal{X} \longrightarrow \mathcal{P}$ will ideally be such that, for $\hat{\bm{p}}=f(x)$, it is expected that $\hat{p}_{j,k}\approx p(y_j^*=k|I)$.

Given an embedding function and a suitable deep learning model (e.g., naïve embedding or engineered features and a NN, graph embedding and GNN), the usual training algorithms for supervised learning apply.
In other words, following a match between instance embedding and model architectures, the algorithmic approach presented in Sec.~\ref{sec:supervised-learning} applies.
Therefore, the dataset required for training is composed of embeddings of instances associated to optimal solutions.
Let $\bm{y}^{*}_I$ denote an optimal solution for instance $I\in \mathcal{I}$ of the MILP problem at hand, and let $g: \mathcal{I} \longrightarrow \mathcal{X}$ be a suitable embedding function.
Then, the dataset necessary for training can be written as a set \[
    \mathcal{D} = \left\{ (x_I, \bm{y}^{*}_I) : x_I = g(I), \bm{y}^*_I\text{ is an optimal solution of }I \right\} 
.\] 

Given such dataset, the training algorithm can be defined by picking any loss function that penalizes the distance between the predicted bias and the actual value.
For example, following a maximum likelihood estimation approach~\cite{goodfellowQualitativelyCharacterizingNeural2015} for a problem solely with binary variables, the binary cross-entropy loss can be applied to a model $f_{\theta}: \mathcal{X} \longrightarrow \left[ 0,1 \right]^n$ such that
\begin{equation}\label{eq:bce-loss}
    \ell(\bm{y}, \hat{\bm{p}}) = \sum_{j=1}^{n} y_j \log \hat{p}_j + (1-y_j) \log (1 - \hat{p}_j)
.\end{equation}
Note that, because there are only binary variables, the model is designed with output only for the bias towards $k=1$, as $\hat{p}_j\approx p(y_j^*=1|I) \iff 1-\hat{p}_j\approx p(y_j^*=0|I)$.


\subsection{Multiple Targets}

Instead of approximating the bias of the optimal solution, \citeonline{nairSolvingMixedInteger2021} proposed to approximate the bias of the \emph{near}-optimal solutions.
Intuitively, this approach provides the model with more information on the feasible region of the problem, and empirical results suggest that it has improved performance in the construction of heuristics~\cite{khalilMIPGNNDataDrivenFramework2022,hanGNNGuidedPredictandSearchFramework2023}.
A proper definition of what will be referred to as a \emph{multiple targets} training follows.

Given an instance of an optimization problem $I\in \mathcal{I}$\footnote{In the following, the reference to $I$ is omitted to ease the notation, but the definitions are specific to an instance.}, let $Y_\epsilon = \left\{ \bm{y} \in Y: \bm{c}^T\bm{y} \le (1+\epsilon) \bm{c}^T\bm{y}^*_{I} \right\}$ be the set of $\epsilon$-optimal solutions, that is, the set of feasible solutions that are within $\epsilon$ distance (in relative terms of the cost) of the optimal solution $\bm{y}^*$.
The multiple targets approach implies that the output of a solution prediction deep learning model $f_{\theta}$ approximates the bias of the variables in the solutions in a set $Y_\epsilon$, i.e., $\hat{p}_{j,k} \approx p(y_j = k | \bm{y} \in Y_\epsilon)$.
For that, \citeonline{nairSolvingMixedInteger2021} propose to weight a loss function such as \eqref{eq:bce-loss} by the cost associated to each solution in $Y_\epsilon$.
Therefore, the dataset $\mathcal{D}$ necessary for training will contain pairs of the form $(x,Y_\epsilon)$ associated to instances of an MILP problem.
In other words, the cost function becomes \[
    \mathcal{L}(\theta) = \frac{1}{|\mathcal{D}|} \sum_{(x,Y_\epsilon)\in \mathcal{D}} \sum_{\bm{y}\in Y_\epsilon}  \frac{e^{-\bm{c}^T \bm{y}}}{\sum_{\bm{y}'\in Y_\epsilon} e^{-\bm{c}^T \bm{y}'}} \ell(\bm{y},f_\theta(x))
.\] 


\section{Learning-based Heuristics}\label{sec:learning-based-heuristics}

\subsection{Early-fixing Variable Assignments}

\subsection{Trust-region}

\subsection{Warm-starting MILP Solvers}

