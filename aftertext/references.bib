
@article{sato_random_2020,
	title = {Random {Features} {Strengthen} {Graph} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2002.03155},
	doi = {10.48550/ARXIV.2002.03155},
	abstract = {Graph neural networks (GNNs) are powerful machine learning models for various graph learning tasks. Recently, the limitations of the expressive power of various GNN models have been revealed. For example, GNNs cannot distinguish some non-isomorphic graphs and they cannot learn efficient graph algorithms. In this paper, we demonstrate that GNNs become powerful just by adding a random feature to each node. We prove that the random features enable GNNs to learn almost optimal polynomial-time approximation algorithms for the minimum dominating set problem and maximum matching problem in terms of approximation ratios. The main advantage of our method is that it can be combined with off-the-shelf GNN models with slight modifications. Through experiments, we show that the addition of random features enables GNNs to solve various problems that normal GNNs, including the graph convolutional networks (GCNs) and graph isomorphism networks (GINs), cannot solve.},
	urldate = {2024-01-18},
	author = {Sato, Ryoma and Yamada, Makoto and Kashima, Hisashi},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {http://arxiv.org/abs/1704.01212},
	doi = {10.48550/arXiv.1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jun,
	year = {2017},
	note = {arXiv:1704.01212 [cs]},
	keywords = {Computer Science - Machine Learning, GNN, I.2.6, background},
}

@book{maniezzo_matheuristics_2021,
	address = {Cham, Switzerland},
	title = {Matheuristics: algorithms and implementations},
	isbn = {978-3-030-70277-9},
	shorttitle = {Matheuristics},
	abstract = {This book is the first comprehensive tutorial on matheuristics. Matheuristics are based on mathematical extensions of previously known heuristics, mainly metaheuristics, and on original, area-specific approaches. This tutorial provides a detailed discussion of both contributions, presenting the pseudocodes of over 40 algorithms, abundant literature references, and for each case a step-by-step description of a sample run on a common Generalized Assignment Problem example. C++ source codes of all algorithms are available in an associated SW repository},
	language = {eng},
	publisher = {Springer},
	author = {Maniezzo, Vittorio and Boschetti, Marco Antonio and Stützle, Thomas},
	year = {2021},
	note = {OCLC: 1249091063},
	keywords = {Optimization},
}

@article{boschetti_matheuristics_2022,
	title = {Matheuristics: using mathematics for heuristic design},
	volume = {20},
	issn = {1614-2411},
	shorttitle = {Matheuristics},
	url = {https://doi.org/10.1007/s10288-022-00510-8},
	doi = {10.1007/s10288-022-00510-8},
	abstract = {Matheuristics are heuristic algorithms based on mathematical tools such as the ones provided by mathematical programming, that are structurally general enough to be applied to different problems with little adaptations to their abstract structure. The result can be metaheuristic hybrids having components derived from the mathematical model of the problems of interest, but the mathematical techniques themselves can define general heuristic solution frameworks. In this paper, we focus our attention on mathematical programming and its contributions to developing effective heuristics. We briefly describe the mathematical tools available and then some matheuristic approaches, reporting some representative examples from the literature. We also take the opportunity to provide some ideas for possible future development.},
	language = {en},
	number = {2},
	urldate = {2023-06-06},
	journal = {4OR},
	author = {Boschetti, Marco Antonio and Maniezzo, Vittorio},
	month = jun,
	year = {2022},
	keywords = {90-00, 90-02, 90C11, Heuristics, Mathematical programming, Matheuristics, Optimization},
	pages = {173--208},
}

@misc{cappart_combinatorial_2022,
	title = {Combinatorial optimization and reasoning with graph neural networks},
	url = {http://arxiv.org/abs/2102.09544},
	doi = {10.48550/arXiv.2102.09544},
	abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks (GNNs), as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. The inductive bias of GNNs effectively encodes combinatorial and relational input due to their invariance to permutations and awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization and machine learning researchers.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Cappart, Quentin and Chételat, Didier and Khalil, Elias and Lodi, Andrea and Morris, Christopher and Veličković, Petar},
	month = sep,
	year = {2022},
	note = {arXiv:2102.09544 [cs, math, stat]},
	keywords = {*, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, GNN, Mathematics - Optimization and Control, Optimization, SURVEY, Statistics - Machine Learning},
}

@misc{maragno_mixed-integer_2023,
	title = {Mixed-{Integer} {Optimization} with {Constraint} {Learning}},
	url = {http://arxiv.org/abs/2111.04469},
	doi = {10.48550/arXiv.2111.04469},
	abstract = {We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization-representability of many machine learning methods, including linear models, decision trees, ensembles, and multi-layer perceptrons, which allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also introduce two approaches for handling the inherent uncertainty of learning from data. First, we characterize a decision trust region using the convex hull of the observations, to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and propose a more flexible formulation to deal with low-density regions and high-dimensional datasets. Then, we propose an ensemble learning approach that enforces constraint satisfaction over multiple bootstrapped estimators or multiple algorithms. In combination with domain-driven components, the embedded models and trust region define a mixed-integer optimization problem for prescription generation. We implement this framework as a Python package (OptiCL) for practitioners. We demonstrate the method in both World Food Programme planning and chemotherapy optimization. The case studies illustrate the framework's ability to generate high-quality prescriptions as well as the value added by the trust region, the use of ensembles to control model robustness, the consideration of multiple machine learning methods, and the inclusion of multiple learned constraints.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Maragno, Donato and Wiberg, Holly and Bertsimas, Dimitris and Birbil, S. Ilker and Hertog, Dick den and Fajemisin, Adejuyigbe},
	month = jan,
	year = {2023},
	note = {arXiv:2111.04469 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Optimization, Statistics - Machine Learning},
}

@misc{quirynen_tailored_2022,
	title = {Tailored {Presolve} {Techniques} in {Branch}-and-{Bound} {Method} for {Fast} {Mixed}-{Integer} {Optimal} {Control} {Applications}},
	url = {http://arxiv.org/abs/2211.12700},
	doi = {10.48550/arXiv.2211.12700},
	abstract = {Mixed-integer model predictive control (MI-MPC) can be a powerful tool for modeling hybrid control systems. In case of a linear-quadratic objective in combination with linear or piecewise-linear system dynamics and inequality constraints, MI-MPC needs to solve a mixed-integer quadratic program (MIQP) at each sampling time step. This paper presents a collection of block-sparse presolve techniques to efficiently remove decision variables, and to remove or tighten inequality constraints, tailored to mixed-integer optimal control problems (MIOCP). In addition, we describe a novel heuristic approach based on an iterative presolve algorithm to compute a feasible but possibly suboptimal MIQP solution. We present benchmarking results for a C code implementation of the proposed BB-ASIPM solver, including a branch-and-bound (B\&B) method with the proposed tailored presolve techniques and an active-set based interior point method (ASIPM), compared against multiple state-of-the-art MIQP solvers on a case study of motion planning with obstacle avoidance constraints. Finally, we demonstrate the computational performance of the BB-ASIPM solver on the dSPACE Scalexio real-time embedded hardware using a second case study of stabilization for an underactuated cart-pole with soft contacts.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Quirynen, Rien and Di Cairano, Stefano},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12700 [cs, eess, math]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Optimization, Pred-and-opt},
}

@misc{sugishita_warmstarting_2021,
	title = {Warmstarting column generation for unit commitment},
	url = {http://arxiv.org/abs/2110.06872},
	doi = {10.48550/arXiv.2110.06872},
	abstract = {This work focuses on a solution method for unit commitment problems which are to be solved repeatedly with different data but with the same problem structures. Dantzig-Wolfe decomposition with a column generation procedure has been proved successful for the unit commitment problem. We demonstrate use of a neural network to generate initial dual values for the column generation procedure, and show how the neural network can be trained efficiently using dual decomposition. Our numerical experiments compare our methods with baselines and reveal that our approach solves test instances to high precision in shorter computational time and scales to handle larger-scale instances.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Sugishita, Nagisa and Grothey, Andreas and McKinnon, Ken},
	month = oct,
	year = {2021},
	note = {arXiv:2110.06872 [math]},
	keywords = {Mathematics - Optimization and Control, Optimization, end-to-end, imitation learning},
}

@article{ibaraki_integer_1976,
	title = {Integer programming formulation of combinatorial optimization problems},
	volume = {16},
	issn = {0012-365X},
	url = {https://www.sciencedirect.com/science/article/pii/0012365X76900911},
	doi = {10.1016/0012-365X(76)90091-1},
	abstract = {This paper considers in a somewhat general setting when a combinatorial optimization problem can be formulated as an (all-integer) integer programming (IP) problem. The main result is that any combinatorial optimization problem can be formulated as an IP problem if its feasible region S is finite but there are many rather sample problems that have no IP formulation if their S is infinite. The approach used for finite S usually gives a formulation with a relatively small number of additional variables for example, an integer polynomial of n 0−1 variables requires at most n + 1 additional variables by our approach, whereas 2n - (n + 1) additional variables at maximum are required by other existing methods. Finally, the decision problem of deciding whether an arbitrarily given combinatorial optimization problem has an IP formulation is considered and it is shown by an argument closely related to Hilbert's tenth problem (drophantine equations) that no such algorithm exists.},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {Discrete Mathematics},
	author = {Ibaraki, Toshimde},
	month = sep,
	year = {1976},
	keywords = {Optimization},
	pages = {39--52},
}

@incollection{nemhauser_scope_2014,
	address = {Hoboken, NJ, USA},
	title = {The {Scope} of {Integer} and {Combinatorial} {Optimization}},
	isbn = {978-1-118-62737-2 978-0-471-82819-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118627372.ch1},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Integer and {Combinatorial} {Optimization}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Nemhauser, George and Wolsey, Laurence},
	collaborator = {Nemhauser, George and Wolsey, Laurence},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118627372.ch1},
	keywords = {Optimization},
	pages = {1--26},
}

@article{savelsbergh_branch-and-price_1997,
	title = {A {Branch}-and-{Price} {Algorithm} for the {Generalized} {Assignment} {Problem}},
	volume = {45},
	issn = {0030-364X, 1526-5463},
	url = {https://pubsonline.informs.org/doi/10.1287/opre.45.6.831},
	doi = {10.1287/opre.45.6.831},
	abstract = {The generalized assignment problem examines the maximum profit assignment of jobs to agents such that each job is assigned to precisely one agent subject to capacity restrictions on the agents. A new algorithm for the generalized assignment problem is presented that employs both column generation and branch-and-bound to obtain optimal integer solutions to a set partitioning formulation of the problem.},
	language = {en},
	number = {6},
	urldate = {2023-06-13},
	journal = {Operations Research},
	author = {Savelsbergh, Martin},
	month = dec,
	year = {1997},
	keywords = {Optimization},
	pages = {831--841},
}

@article{padberg_branch-and-cut_1991,
	title = {A {Branch}-and-{Cut} {Algorithm} for the {Resolution} of {Large}-{Scale} {Symmetric} {Traveling} {Salesman} {Problems}},
	volume = {33},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/1033004},
	doi = {10.1137/1033004},
	language = {en},
	number = {1},
	urldate = {2023-06-13},
	journal = {SIAM Review},
	author = {Padberg, Manfred and Rinaldi, Giovanni},
	month = mar,
	year = {1991},
	keywords = {Optimization},
	pages = {60--100},
}

@incollection{land_automatic_2010,
	address = {Berlin, Heidelberg},
	title = {An {Automatic} {Method} for {Solving} {Discrete} {Programming} {Problems}},
	isbn = {978-3-540-68279-0},
	url = {https://doi.org/10.1007/978-3-540-68279-0_5},
	abstract = {In the late 1950s there was a group of teachers and research assistants at the London School of Economics interested in linear programming and its extensions, in particular Helen Makower, George Morton, Ailsa Land and Alison Doig. We had considered the ‘Laundry Van Problem’ until we discovered that it was known as the Traveling Salesman Problem, and had looked at aircraft timetabling, until quickly realizing that even the planning for the Scottish sector was beyond our capability! Alison Doig (now Harcourt) had studied the paper trim problem for her Masters project in Melbourne before coming to England.},
	language = {en},
	urldate = {2023-06-13},
	booktitle = {50 {Years} of {Integer} {Programming} 1958-2008: {From} the {Early} {Years} to the {State}-of-the-{Art}},
	publisher = {Springer},
	author = {Land, Ailsa H. and Doig, Alison G.},
	editor = {Jünger, Michael and Liebling, Thomas M. and Naddef, Denis and Nemhauser, George L. and Pulleyblank, William R. and Reinelt, Gerhard and Rinaldi, Giovanni and Wolsey, Laurence A.},
	year = {2010},
	doi = {10.1007/978-3-540-68279-0_5},
	keywords = {British Petroleum, Discrete Variable, London School, Optimization, Storage Tank, Travel Salesman Problem},
	pages = {105--132},
}

@techreport{bestuzheva_scip_2021,
	type = {{ZIB}-{Report}},
	title = {The {SCIP} {Optimization} {Suite} 8.0},
	url = {http://nbn-resolving.de/urn:nbn:de:0297-zib-85309},
	number = {21-41},
	institution = {Zuse Institute Berlin},
	author = {Bestuzheva, Ksenia and Besançon, Mathieu and Chen, Wei-Kun and Chmiela, Antonia and Donkiewicz, Tim and Doornmalen, Jasper van and Eifler, Leon and Gaul, Oliver and Gamrath, Gerald and Gleixner, Ambros and Gottwald, Leona and Graczyk, Christoph and Halbig, Katrin and Hoen, Alexander and Hojny, Christopher and Hulst, Rolf van der and Koch, Thorsten and Lübbecke, Marco and Maher, Stephen J. and Matter, Frederic and Mühmer, Erik and Müller, Benjamin and Pfetsch, Marc E. and Rehfeldt, Daniel and Schlein, Steffan and Schlösser, Franziska and Serrano, Felipe and Shinano, Yuji and Sofranac, Boro and Turner, Mark and Vigerske, Stefan and Wegscheider, Fabian and Wellner, Philipp and Weninger, Dieter and Witzig, Jakob},
	month = dec,
	year = {2021},
	keywords = {Optimization},
}

@article{morrison_branch-and-bound_2016,
	title = {Branch-and-bound algorithms: {A} survey of recent advances in searching, branching, and pruning},
	volume = {19},
	issn = {1572-5286},
	shorttitle = {Branch-and-bound algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/S1572528616000062},
	doi = {10.1016/j.disopt.2016.01.005},
	abstract = {The branch-and-bound (B\&B) algorithmic framework has been used successfully to find exact solutions for a wide array of optimization problems. B\&B uses a tree search strategy to implicitly enumerate all possible solutions to a given problem, applying pruning rules to eliminate regions of the search space that cannot lead to a better solution. There are three algorithmic components in B\&B that can be specified by the user to fine-tune the behavior of the algorithm. These components are the search strategy, the branching strategy, and the pruning rules. This survey presents a description of recent research advances in the design of B\&B algorithms, particularly with regards to these three components. Moreover, three future research directions are provided in order to motivate further exploration in these areas.},
	language = {en},
	urldate = {2023-06-13},
	journal = {Discrete Optimization},
	author = {Morrison, David R. and Jacobson, Sheldon H. and Sauppe, Jason J. and Sewell, Edward C.},
	month = feb,
	year = {2016},
	keywords = {Branch-and-bound, Cyclic best first search, Discrete optimization, Integer programming, Optimization, Search strategies, Survey},
	pages = {79--102},
}

@article{malandraki_time_1992,
	title = {Time {Dependent} {Vehicle} {Routing} {Problems}: {Formulations}, {Properties} and {Heuristic} {Algorithms}},
	volume = {26},
	issn = {0041-1655, 1526-5447},
	shorttitle = {Time {Dependent} {Vehicle} {Routing} {Problems}},
	url = {https://pubsonline.informs.org/doi/10.1287/trsc.26.3.185},
	doi = {10.1287/trsc.26.3.185},
	abstract = {The time dependent vehicle routing problem (TDVRP) is defined as follows. A vehicle fleet of fixed capacities serves customers of fixed demands from a central depot. Customers are assigned to vehicles and the vehicles routed so that the total time of the routes is minimized. The travel time between two customers or between a customer and the depot depends on the distance between the points and time of day. Time windows for serving the customers may also be present. The time dependent traveling salesman problem (TDTSP) is a special case of the TDVRP in which only one vehicle of infinite capacity is available. Mixed integer linear programming formulations of the TDVRP and the TDTSP are presented that treat the travel time functions as step functions. The characteristics and properties of the TDVRP preclude modification of most of the algorithms that have been developed for the vehicle routing problem. Several simple heuristic algorithms are given for the TDTSP and TDVRP without time windows based on the nearest-neighbor heuristic. A mathematical-programming-based heuristic for the TDTSP without time windows using cutting planes is also briefly discussed. Test results on small, randomly generated problems are reported.},
	language = {en},
	number = {3},
	urldate = {2023-06-13},
	journal = {Transportation Science},
	author = {Malandraki, Chryssi and Daskin, Mark S.},
	month = aug,
	year = {1992},
	keywords = {Optimization},
	pages = {185--200},
}

@book{pochet_production_2006,
	address = {New York Berlin},
	series = {Springer series in operations research and financial engineering},
	title = {Production planning by mixed integer programming},
	isbn = {978-0-387-33477-6},
	language = {eng},
	publisher = {Springer},
	author = {Pochet, Yves and Wolsey, Laurence A.},
	year = {2006},
	keywords = {Optimization},
}

@book{sawik_scheduling_2011,
	address = {Hoboken, N.J},
	title = {Scheduling in supply chains using mixed integer programming},
	isbn = {978-0-470-93573-6},
	publisher = {Wiley},
	author = {Sawik, Tadeusz},
	year = {2011},
	note = {OCLC: ocn693553935},
	keywords = {Assembly-line methods, Business logistics, Data processing, Integer programming, Optimization, Production scheduling},
}

@article{dikin_iterative_1967,
	title = {Iterative solution of problems of linear and quadratic programming},
	volume = {8},
	issn = {0197-6788},
	language = {English},
	journal = {Soviet Mathematics. Doklady},
	author = {Dikin, I. I.},
	year = {1967},
	keywords = {65K10, 90C05, 90C20, Optimization},
	pages = {674--675},
}

@inproceedings{queiroga_10000_2021,
	title = {10,000 optimal {CVRP} solutions for testing machine learning based heuristics},
	url = {https://openreview.net/forum?id=yHiMXKN6nTl},
	abstract = {We introduce a benchmark of 10,000 instances with heterogeneous characteristics for the capacitated vehicle routing problem. We also provide optimal solutions for almost all of them along with a generator to produce additional training and validation data. This benchmark aims to permit a more systematic comparison of machine learning based search algorithms on this important problem. We also emit recommendations regarding the correct use of this dataset.},
	language = {en},
	urldate = {2023-04-28},
	author = {Queiroga, Eduardo and Sadykov, Ruslan and Uchoa, Eduardo and Vidal, Thibaut},
	month = nov,
	year = {2021},
	keywords = {Data generation, background, end-to-end, imitation learning},
}

@inproceedings{malitsky_structure-preserving_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Structure-{Preserving} {Instance} {Generation}},
	isbn = {978-3-319-50349-3},
	doi = {10.1007/978-3-319-50349-3_9},
	abstract = {Real-world instances are critical for the development of state-of-the-art algorithms, algorithm configuration techniques, and selection approaches. However, very few true industrial instances exist for most problems, which poses a problem both to algorithm designers and methods for algorithm selection. The lack of enough real data leads to an inability for algorithm designers to show the effectiveness of their techniques, and for algorithm selection it is difficult or even impossible to train a portfolio with so few training examples. This paper introduces a novel instance generator that creates instances that have the same structural properties as industrial instances. We generate instances through a large neighborhood search-like method that combines components of instances together to form new ones. We test our approach on the MaxSAT and SAT problems, and then demonstrate that portfolios trained on these generated instances perform just as well or even better than those trained on the real instances.},
	language = {en},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Malitsky, Yuri and Merschformann, Marius and O’Sullivan, Barry and Tierney, Kevin},
	editor = {Festa, Paola and Sellmann, Meinolf and Vanschoren, Joaquin},
	year = {2016},
	keywords = {Algorithm Configuration, Data generation, Industrial Instances, Maximum SAT (MaxSAT), Portfolio Techniques, Representative Genes},
	pages = {123--140},
}

@article{smith-miles_generating_2015,
	title = {Generating new test instances by evolving in instance space},
	volume = {63},
	issn = {0305-0548},
	url = {https://www.sciencedirect.com/science/article/pii/S0305054815001136},
	doi = {10.1016/j.cor.2015.04.022},
	abstract = {Our confidence in the future performance of any algorithm, including optimization algorithms, depends on how carefully we select test instances so that the generalization of algorithm performance on future instances can be inferred. In recent work, we have established a methodology to generate a 2-d representation of the instance space, comprising a set of known test instances. This instance space shows the similarities and differences between the instances using measurable features or properties, and enables the performance of algorithms to be viewed across the instance space, where generalizations can be inferred. The power of this methodology is the insights that can be generated into algorithm strengths and weaknesses by examining the regions in instance space where strong performance can be expected. The representation of the instance space is dependent on the choice of test instances however. In this paper we present a methodology for generating new test instances with controllable properties, by filling observed gaps in the instance space. This enables the generation of rich new sets of test instances to support better the understanding of algorithm strengths and weaknesses. The methodology is demonstrated on graph colouring as a case study.},
	language = {en},
	urldate = {2023-07-31},
	journal = {Computers \& Operations Research},
	author = {Smith-Miles, Kate and Bowly, Simon},
	month = nov,
	year = {2015},
	keywords = {Benchmarking, Data generation, Evolving instances, Graph colouring, Instance space, Test instances},
	pages = {102--113},
}

@inproceedings{yehuda_its_2020,
	title = {It’s {Not} {What} {Machines} {Can} {Learn}, {It}’s {What} {We} {Cannot} {Teach}},
	url = {https://proceedings.mlr.press/v119/yehuda20a.html},
	abstract = {Can deep neural networks learn to solve any task, and in particular problems of high complexity? This question attracts a lot of interest, with recent works tackling computationally hard tasks such as the traveling salesman problem and satisfiability. In this work we offer a different perspective on this question. Given the common assumption that NP != coNP we prove that any polynomial-time sample generator for an NP-hard problem samples, in fact, from an easier sub-problem. We empirically explore a case study, Conjunctive Query Containment, and show how common data generation techniques generate biased data-sets that lead practitioners to over-estimate model accuracy. Our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems, the reason being the difficulty of generating sufficiently large and unbiased training sets.},
	language = {en},
	urldate = {2023-09-12},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yehuda, Gal and Gabel, Moshe and Schuster, Assaf},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10831--10841},
}

@inproceedings{khalil_learning_2016,
	title = {Learning to branch in mixed integer programming},
	isbn = {978-1-57735-760-5},
	abstract = {The design of strategies for branching in Mixed Integer Programming (MIP) is guided by cycles of parameter tuning and offline experimentation on an extremely heterogeneous testbed, using the average performance. Once devised, these strategies (and their parameter settings) are essentially input-Agnostic. To address these issues, we propose a machine learning (ML) framework for variable branching in MIP. Our method observes the decisions made by Strong Branching (SB), a time-consuming strategy that produces small search trees, collecting features that characterize the candidate branching variables at each node of the tree. Based on the collected data, we learn an easy-To-evaluate surrogate function that mimics the SB strategy, by means of solving a learning-To-rank problem, common in ML. The learned ranking function is then used for branching. The learning is instance-specific, and is performed on-The-fly while executing a branch-And-bound search to solve the instance. Experiments on benchmark instances indicate that our method produces significantly smaller search trees than existing heuristics, and is competitive with a state-of-The-Art commercial solver. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	language = {English},
	author = {Khalil, E.B. and Bodic, P.L. and Song, L. and Nemhauser, G. and Dilkina, B.},
	year = {2016},
	pages = {724--731},
}

@article{alvarez_machine_2017,
	title = {A {Machine} {Learning}-{Based} {Approximation} of {Strong} {Branching}},
	volume = {29},
	issn = {1091-9856},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2016.0723},
	doi = {10.1287/ijoc.2016.0723},
	abstract = {We present in this paper a new generic approach to variable branching in branch and bound for mixed-integer linear problems. Our approach consists in imitating the decisions taken by a good branching strategy, namely strong branching, with a fast approximation. This approximated function is created by a machine learning technique from a set of observed branching decisions taken by strong branching. The philosophy of the approach is similar to reliability branching. However, our approach can catch more complex aspects of observed previous branchings to take a branching decision. The experiments performed on randomly generated and MIPLIB problems show promising results.},
	number = {1},
	urldate = {2023-09-01},
	journal = {INFORMS Journal on Computing},
	author = {Alvarez, Alejandro Marcos and Louveaux, Quentin and Wehenkel, Louis},
	month = jan,
	year = {2017},
	note = {Publisher: INFORMS},
	keywords = {branch and bound, strong branching, supervised machine learning, variable branching},
	pages = {185--195},
}

@article{tank_simple_1986,
	title = {Simple 'neural' optimization networks: {An} {A}/{D} converter, signal decision circuit, and a linear programming circuit},
	volume = {33},
	issn = {1558-1276},
	shorttitle = {Simple 'neural' optimization networks},
	doi = {10.1109/TCS.1986.1085953},
	abstract = {We describe how several optimization problems can be rapidly solved by highly interconnected networks of simple analog processors. Analog-to-digital (A/D) conversion was considered as a simple optimization problem, and an A/D converter of novel architecture was designed. A/D conversion is a simple example of a more general class of signal-decision problems which we show could also be solved by appropriately constructed networks. Circuits to solve these problems were designed using general principles which result from an understanding of the basic collective computational properties of a specific class of analog-processor networks. We also show that a network which solves linear programming problems can be understood from the same concepts.},
	number = {5},
	journal = {IEEE Transactions on Circuits and Systems},
	author = {Tank, D. and Hopfield, J.},
	month = may,
	year = {1986},
	note = {Conference Name: IEEE Transactions on Circuits and Systems},
	keywords = {Analog computers, Analog-digital conversion, Computer networks, Design optimization, Integrated circuit interconnections, Linear programming, Signal processing, Space exploration, Space technology, Traveling salesman problems},
	pages = {533--541},
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Adam},
	url = {https://arxiv.org/abs/1412.6980},
	doi = {10.48550/ARXIV.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-08-04},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 9},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{hopfield_neural_1985,
	title = {“{Neural}” computation of decisions in optimization problems},
	volume = {52},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00339943},
	doi = {10.1007/BF00339943},
	abstract = {Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.},
	language = {en},
	number = {3},
	urldate = {2023-08-03},
	journal = {Biological Cybernetics},
	author = {Hopfield, J. J. and Tank, D. W.},
	month = jul,
	year = {1985},
	keywords = {Computational Power, General Principle, Specific Problem, Time Constant, Wide Class},
	pages = {141--152},
}

@inproceedings{etheve_reinforcement_2020,
	address = {Cham},
	title = {Reinforcement {Learning} for {Variable} {Selection} in a {Branch} and {Bound} {Algorithm}},
	isbn = {978-3-030-58942-4},
	abstract = {Mixed integer linear programs are commonly solved by Branch and Bound algorithms. A key factor of the efficiency of the most successful commercial solvers is their fine-tuned heuristics. In this paper, we leverage patterns in real-world instances to learn from scratch a new branching strategy optimised for a given problem and compare it with a commercial solver. We propose FMSTS, a novel Reinforcement Learning approach specifically designed for this task. The strength of our method lies in the consistency between a local value function and a global metric of interest. In addition, we provide insights for adapting known RL techniques to the Branch and Bound setting, and present a new neural network architecture inspired from the literature. To our knowledge, it is the first time Reinforcement Learning has been used to fully optimise the branching strategy. Computational experiments show that our method is appropriate and able to generalise well to new instances.},
	booktitle = {Integration of {Constraint} {Programming}, {Artificial} {Intelligence}, and {Operations} {Research}},
	publisher = {Springer International Publishing},
	author = {Etheve, Marc and Alès, Zacharie and Bissuel, Côme and Juan, Olivier and Kedad-Sidhoum, Safia},
	editor = {Hebrard, Emmanuel and Musliu, Nysret},
	year = {2020},
	pages = {176--185},
}

@inproceedings{khalil_learning_2017,
	title = {Learning {Combinatorial} {Optimization} {Algorithms} over {Graphs}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{grimstad_relu_2019,
	title = {{ReLU} networks as surrogate models in mixed-integer linear programs},
	volume = {131},
	issn = {00981354},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135419307203},
	doi = {10.1016/j.compchemeng.2019.106580},
	language = {en},
	urldate = {2023-06-13},
	journal = {Computers \& Chemical Engineering},
	author = {Grimstad, Bjarne and Andersson, Henrik},
	month = dec,
	year = {2019},
	pages = {106580},
}

@book{larson_urban_1981,
	address = {Englewood Cliffs, N.J},
	title = {Urban operations research},
	isbn = {978-0-13-939447-8},
	publisher = {Prentice-Hall},
	author = {Larson, Richard C. and Odoni, Amedeo R.},
	year = {1981},
	keywords = {Municipal services, Operations research, United States},
}

@article{mckelvey_review_1983,
	title = {Review of {Urban} {Operations} {Research}.},
	volume = {25},
	issn = {0036-1445},
	url = {https://www.jstor.org/stable/2030052},
	number = {1},
	urldate = {2023-06-13},
	journal = {SIAM Review},
	author = {McKelvey, Robert and Nguyen, Hien},
	collaborator = {Larson, Richard C. and Odoni, Amedeo R.},
	year = {1983},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {129--131},
}

@article{muller_short-term_2022,
	title = {Short-term steady-state production optimization of offshore oil platforms: wells with dual completion (gas-lift and {ESP}) and flow assurance},
	volume = {30},
	issn = {1863-8279},
	shorttitle = {Short-term steady-state production optimization of offshore oil platforms},
	url = {https://doi.org/10.1007/s11750-021-00604-2},
	doi = {10.1007/s11750-021-00604-2},
	abstract = {Research on short-term steady-sate production optimization of oilfields led to the development of models and solution methods, several of which have found their way into practice. Early models considered satellite wells that operate with a fixed topside pressure and gas-lift injection, while recent approaches address distinct types of artificial lifting, pressure control, and processing equipment. By integrating existing approaches, this work presents a flexible model for production optimization that considers new features, including flow assurance constraints and smart selection of artificial lifting operating modes (gas-lift with multiple valves, electrical submersible pumping, and dual completion). Given that the proposed model is conceptual, piecewise-linear functions are obtained from field and simulation process data to approximate nonlinear relations. This way, the methodology decides the best combinations of routing and operation modes to maximize production gains. Simulated results are reported considering a representative asset that illustrates complex behavior.},
	language = {en},
	number = {1},
	urldate = {2023-03-27},
	journal = {TOP},
	author = {Müller, Eduardo Rauh and Camponogara, Eduardo and Seman, Laio Oriel and Hülse, Eduardo Otte and Vieira, Bruno Ferreira and Miyatake, Luis Kin and Teixeira, Alex Furtado},
	month = apr,
	year = {2022},
	keywords = {90-10, 90B30, 90C11, Dual well completion, Flow assurance, Mixed-integer optimization, Short-term production optimization, Steady-state optimization},
	pages = {152--180},
}

@article{rigo_branch-and-price_2022,
	title = {A branch-and-price algorithm for nanosatellite task scheduling to improve mission quality-of-service},
	volume = {303},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221722001606},
	doi = {https://doi.org/10.1016/j.ejor.2022.02.040},
	number = {1},
	journal = {European Journal of Operational Research},
	author = {Rigo, Cezar Antônio and Seman, Laio Oriel and Camponogara, Eduardo and Filho, Edemar Morsch and Bezerra, Eduardo Augusto and Munari, Pedro},
	year = {2022},
	keywords = {Branch and price, Dantzig-Wolfe decomposition, Nanosatellite, Quality of service, Scheduling},
	pages = {168--183},
}

@article{seman_energy-aware_2022,
	title = {An {Energy}-{Aware} {Task} {Scheduling} for {Quality}-of-{Service} {Assurance} in {Constellations} of {Nanosatellites}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/10/3715},
	doi = {10.3390/s22103715},
	abstract = {When managing a constellation of nanosatellites, one may leverage this structure to improve the mission\&rsquo;s quality-of-service (QoS) by optimally distributing the tasks during an orbit. In this sense, this research proposes an offline energy-aware task scheduling problem formulation regarding the specifics of constellations, by considering whether the tasks are individual, collective, or stimulated to be redundant. By providing such an optimization framework, the idea of estimating an offline task schedule can serve as a baseline for the constellation design phase. For example, given a particular orbit, from the simulation of an irradiance model, the engineer can estimate how the mission value is affected by the inclusion or exclusion of individuals objects. The proposed model, given in the form of a multi-objective mixed-integer linear programming model, is illustrated in this work for several illustrative scenarios considering different sets of tasks and constellations. We also perform an analysis of the Pareto-optimal frontier of the problem, identifying the feasible trade-off points between constellation and individual tasks. This information can be useful to the decision-maker (mission operator) when planning the behavior in orbit.},
	number = {10},
	journal = {Sensors},
	author = {Seman, Laio Oriel and Ribeiro, Brenda F. and Rigo, Cezar A. and Filho, Edemar Morsch and Camponogara, Eduardo and Leonardi, Rodrigo and Bezerra, Eduardo A.},
	year = {2022},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{smith_neural_1999,
	title = {Neural {Networks} for {Combinatorial} {Optimization}: {A} {Review} of {More} {Than} a {Decade} of {Research}},
	volume = {11},
	issn = {1091-9856},
	shorttitle = {Neural {Networks} for {Combinatorial} {Optimization}},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.11.1.15},
	doi = {10.1287/ijoc.11.1.15},
	abstract = {It has been over a decade since neural networks were first applied to solve combinatorial optimization problems. During this period, enthusiasm has been erratic as new approaches are developed and (sometimes years later) their limitations are realized. This article briefly summarizes the work that has been done and presents the current standing of neural networks for combinatorial optimization by considering each of the major classes of combinatorial optimization problems. Areas which have not yet been studied are identified for future research.},
	number = {1},
	urldate = {2023-06-12},
	journal = {INFORMS Journal on Computing},
	author = {Smith, Kate A.},
	month = feb,
	year = {1999},
	note = {Publisher: INFORMS},
	keywords = {SURVEY, background, combinatorial optimization, neural networks},
	pages = {15--34},
}

@article{bengio_machine_2021,
	title = {Machine learning for combinatorial optimization: {A} methodological tour d’horizon},
	volume = {290},
	issn = {0377-2217},
	shorttitle = {Machine learning for combinatorial optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221720306895},
	doi = {10.1016/j.ejor.2020.07.063},
	abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
	language = {en},
	number = {2},
	urldate = {2023-04-19},
	journal = {European Journal of Operational Research},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	month = apr,
	year = {2021},
	note = {[A] DONE},
	keywords = {*, Branch and bound, Combinatorial optimization, DONE, Machine learning, Mixed-integer programming solvers, SURVEY, background},
	pages = {405--421},
}

@article{ding_accelerating_2020,
	title = {Accelerating {Primal} {Solution} {Findings} for {Mixed} {Integer} {Programs} {Based} on {Solution} {Prediction}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5503},
	doi = {10.1609/aaai.v34i02.5503},
	abstract = {Mixed Integer Programming (MIP) is one of the most widely used modeling techniques for combinatorial optimization problems. In many applications, a similar MIP model is solved on a regular basis, maintaining remarkable similarities in model structures and solution appearances but differing in formulation coefficients. This offers the opportunity for machine learning methods to explore the correlations between model structures and the resulting solution values. To address this issue, we propose to represent a MIP instance using a tripartite graph, based on which a Graph Convolutional Network (GCN) is constructed to predict solution values for binary variables. The predicted solutions are used to generate a local branching type cut which can be either treated as a global (invalid) inequality in the formulation resulting in a heuristic approach to solve the MIP, or as a root branching rule resulting in an exact approach. Computational evaluations on 8 distinct types of MIP problems show that the proposed framework improves the primal solution finding performance significantly on a state-of-the-art open-source MIP solver.},
	number = {02},
	urldate = {2023-05-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ding, Jian-Ya and Zhang, Chao and Shen, Lei and Li, Shengyin and Wang, Bing and Xu, Yinghui and Song, Le},
	month = apr,
	year = {2020},
	keywords = {*, GNN, end-to-end, imitation learning},
	pages = {1452--1459},
}

@inproceedings{deudon_learning_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Heuristics} for the {TSP} by {Policy} {Gradient}},
	isbn = {978-3-319-93031-2},
	doi = {10.1007/978-3-319-93031-2_12},
	abstract = {The aim of the study is to provide interesting insights on how efficient machine learning algorithms could be adapted to solve combinatorial optimization problems in conjunction with existing heuristic procedures. More specifically, we extend the neural combinatorial optimization framework to solve the traveling salesman problem (TSP). In this framework, the city coordinates are used as inputs and the neural network is trained using reinforcement learning to predict a distribution over city permutations. Our proposed framework differs from the one in [1] since we do not make use of the Long Short-Term Memory (LSTM) architecture and we opted to design our own critic to compute a baseline for the tour length which results in more efficient learning. More importantly, we further enhance the solution approach with the well-known 2-opt heuristic. The results show that the performance of the proposed framework alone is generally as good as high performance heuristics (OR-Tools). When the framework is equipped with a simple 2-opt procedure, it could outperform such heuristics and achieve close to optimal results on 2D Euclidean graphs. This demonstrates that our approach based on machine learning techniques could learn good heuristics which, once being enhanced with a simple local search, yield promising results.},
	language = {en},
	booktitle = {Integration of {Constraint} {Programming}, {Artificial} {Intelligence}, and {Operations} {Research}},
	publisher = {Springer International Publishing},
	author = {Deudon, Michel and Cournut, Pierre and Lacoste, Alexandre and Adulyasak, Yossiri and Rousseau, Louis-Martin},
	editor = {van Hoeve, Willem-Jan},
	year = {2018},
	keywords = {Combinatorial optimization, Neural networks, Policy gradient, Pred-and-opt, Reinforcement learning, Traveling salesman, end-to-end},
	pages = {170--181},
}

@inproceedings{mahmood_automated_2018,
	title = {Automated {Treatment} {Planning} in {Radiation} {Therapy} using {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.mlr.press/v85/mahmood18a.html},
	abstract = {Knowledge-based planning (KBP) is an automated approach to radiation therapy treatment planning that involves predicting desirable treatment plans before they are then corrected to deliverable ones. We propose a generative adversarial network (GAN) approach for predicting desirable 3D dose distributions that eschews the previous paradigms of site-specific feature engineering and predicting low-dimensional representations of the plan. Experiments on a dataset of oropharyngeal cancer patients show that our approach significantly outperforms previous methods on several clinical satisfaction criteria and similarity metrics.},
	language = {en},
	urldate = {2023-04-25},
	booktitle = {Proceedings of the 3rd {Machine} {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Mahmood, Rafid and Babier, Aaron and McNiven, Andrea and Diamant, Adam and Chan, Timothy C. Y.},
	month = nov,
	year = {2018},
	note = {[A]},
	keywords = {Pred-and-opt},
	pages = {484--499},
}

@article{zhang_survey_2023,
	title = {A survey for solving mixed integer programming via machine learning},
	volume = {519},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222014035},
	doi = {10.1016/j.neucom.2022.11.024},
	abstract = {Machine learning (ML) has been recently introduced to solving optimization problems, especially for combinatorial optimization (CO) tasks. In this paper, we survey the trend of leveraging ML to solve the mixed-integer programming problem (MIP). Theoretically, MIP is an NP-hard problem, and most CO problems can be formulated as MIP. Like other CO problems, the human-designed heuristic algorithms for MIP rely on good initial solutions and cost a lot of computational resources. Therefore, researchers consider applying machine learning methods to solve MIP since ML-enhanced approaches can provide the solution based on the typical patterns from the training data. Specifically, we first introduce the formulation and preliminaries of MIP and representative traditional solvers. Then, we show the integration of machine learning and MIP with detailed discussions on related learning-based methods, which can be further classified into exact and heuristic algorithms. Finally, we propose the outlook for learning-based MIP solvers, the direction toward more combinatorial optimization problems beyond MIP, and the mutual embrace of traditional solvers and ML components. We maintain a list of papers that utilize machine learning technologies to solve combinatorial optimization problems, which is available at https://github.com/Thinklab-SJTU/awesome-ml4co.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Neurocomputing},
	author = {Zhang, Jiayi and Liu, Chang and Li, Xijun and Zhen, Hui-Ling and Yuan, Mingxuan and Li, Yawen and Yan, Junchi},
	month = jan,
	year = {2023},
	keywords = {Combinatorial optimization, Machine learning, Mixed integer programming, SURVEY},
	pages = {205--217},
}

@misc{li_learning_2022,
	title = {Learning to {Accelerate} {Approximate} {Methods} for {Solving} {Integer} {Programming} via {Early} {Fixing}},
	url = {http://arxiv.org/abs/2207.02087},
	doi = {10.48550/arXiv.2207.02087},
	abstract = {Integer programming (IP) is an important and challenging problem. Approximate methods have shown promising performance on both effectiveness and efficiency for solving the IP problem. However, we observed that a large fraction of variables solved by some iterative approximate methods fluctuate around their final converged discrete states in very long iterations. Inspired by this observation, we aim to accelerate these approximate methods by early fixing these fluctuated variables to their converged states while not significantly harming the solution accuracy. To this end, we propose an early fixing framework along with the approximate method. We formulate the whole early fixing process as a Markov decision process, and train it using imitation learning. A policy network will evaluate the posterior probability of each free variable concerning its discrete candidate states in each block of iterations. Specifically, we adopt the powerful multi-headed attention mechanism in the policy network. Extensive experiments on our proposed early fixing framework are conducted to three different IP applications: constrained linear programming, MRF energy minimization and sparse adversarial attack. The former one is linear IP problem, while the latter two are quadratic IP problems. We extend the problem scale from regular size to significantly large size. The extensive experiments reveal the competitiveness of our early fixing framework: the runtime speeds up significantly, while the solution quality does not degrade much, even in some cases it is available to obtain better solutions. Our proposed early fixing framework can be regarded as an acceleration extension of ADMM methods for solving integer programming. The source codes are available at {\textbackslash}url\{https://github.com/SCLBD/Accelerated-Lpbox-ADMM\}.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Li, Longkang and Wu, Baoyuan},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02087 [cs, math]},
	keywords = {Computer Science - Discrete Mathematics, Computer Science - Machine Learning, GNN, Mathematics - Optimization and Control, imitation learning},
}

@misc{sonnerat_learning_2022,
	title = {Learning a {Large} {Neighborhood} {Search} {Algorithm} for {Mixed} {Integer} {Programs}},
	url = {http://arxiv.org/abs/2107.10201},
	doi = {10.48550/arXiv.2107.10201},
	abstract = {Large Neighborhood Search (LNS) is a combinatorial optimization heuristic that starts with an assignment of values for the variables to be optimized, and iteratively improves it by searching a large neighborhood around the current assignment. In this paper we consider a learning-based LNS approach for mixed integer programs (MIPs). We train a Neural Diving model to represent a probability distribution over assignments, which, together with an off-the-shelf MIP solver, generates an initial assignment. Formulating the subsequent search steps as a Markov Decision Process, we train a Neural Neighborhood Selection policy to select a search neighborhood at each step, which is searched using a MIP solver to find the next assignment. The policy network is trained using imitation learning. We propose a target policy for imitation that, given enough compute resources, is guaranteed to select the neighborhood containing the optimal next assignment amongst all possible choices for the neighborhood of a specified size. Our approach matches or outperforms all the baselines on five real-world MIP datasets with large-scale instances from diverse applications, including two production applications at Google. It achieves \$2{\textbackslash}times\$ to \$37.8{\textbackslash}times\$ better average primal gap than the best baseline on three of the datasets at large running times.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Sonnerat, Nicolas and Wang, Pengming and Ktena, Ira and Bartunov, Sergey and Nair, Vinod},
	month = may,
	year = {2022},
	note = {arXiv:2107.10201 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@inproceedings{wu_learning_2021,
	title = {Learning {Large} {Neighborhood} {Search} {Policy} for {Integer} {Programming}},
	url = {https://openreview.net/forum?id=IaM7U4J-w3c},
	abstract = {We propose a deep reinforcement learning (RL) method to learn large neighborhood search (LNS) policy for integer programming (IP). The RL policy is trained as the destroy operator to select a subset of variables at each step, which is reoptimized by an IP solver as the repair operator. However, the combinatorial number of variable subsets prevents direct application of typical RL algorithms. To tackle this challenge, we represent all subsets by factorizing them into binary decisions on each variable. We then design a neural network to learn policies for each variable in parallel, trained by a customized actor-critic algorithm. We evaluate the proposed method on four representative IP problems. Results show that it can find better solutions than SCIP in much less time, and significantly outperform other LNS baselines with the same runtime. Moreover, these advantages notably persist when the policies generalize to larger problems. Further experiments with Gurobi also reveal that our method can outperform this state-of-the-art commercial solver within the same time limit.},
	language = {en},
	urldate = {2023-05-23},
	author = {Wu, Yaoxin and Song, Wen and Cao, Zhiguang and Zhang, Jie},
	month = nov,
	year = {2021},
	keywords = {neighborhood search},
}

@article{hottung_neural_2022,
	title = {Neural large neighborhood search for routing problems},
	volume = {313},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370222001266},
	doi = {10.1016/j.artint.2022.103786},
	abstract = {Learning how to automatically solve optimization problems has the potential to provide the next big leap in optimization technology. The performance of automatically learned heuristics on routing problems has been steadily improving in recent years, but approaches based purely on machine learning are still outperformed by state-of-the-art optimization methods. To close this performance gap, we propose a novel large neighborhood search (LNS) framework for vehicle routing that integrates learned heuristics for generating new solutions. The learning mechanism is based on a deep neural network with an attention mechanism and has been especially designed to be integrated into an LNS search setting. We evaluate our approach on the capacitated vehicle routing problem (CVRP), the split delivery vehicle routing problem (SDVRP), and the capacitated team orienteering problem (CTOP). We show that the NLNS approach is able to outperform a handcrafted LNS on the CVRP and SDVRP and match the performance of a standard LNS on the CTOP. NLNS is thus able to quickly and effectively learn high performance heuristics to maneuver through the search space of difficult routing problems, coming close to the performance of state-of-the-art optimization approaches.},
	language = {en},
	urldate = {2023-05-23},
	journal = {Artificial Intelligence},
	author = {Hottung, André and Tierney, Kevin},
	month = dec,
	year = {2022},
	keywords = {Combinatorial optimization, Heuristic search, Learning to optimize, Reinforcement learning, Routing problems, neighborhood search},
	pages = {103786},
}

@inproceedings{song_general_2020,
	title = {A {General} {Large} {Neighborhood} {Search} {Framework} for {Solving} {Integer} {Linear} {Programs}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e769e03a9d329b2e864b4bf4ff54ff39-Abstract.html},
	abstract = {This paper studies how to design abstractions of large-scale combinatorial optimization problems that can leverage existing state-of-the-art solvers in general-purpose ways, and that are amenable to data-driven design.  The goal is to arrive at new approaches that can reliably outperform existing solvers in wall-clock time.  We focus on solving integer programs and ground our approach in the large neighborhood search (LNS) paradigm, which iteratively chooses a subset of variables to optimize while leaving the remainder fixed.  The appeal of LNS is that it can easily use any existing solver as a subroutine, and thus can inherit the benefits of carefully engineered heuristic approaches and their software implementations.  We also show that one can learn a good neighborhood selector from training data.  Through an extensive empirical validation, we demonstrate that our LNS framework can significantly outperform, in wall-clock time, compared to state-of-the-art commercial solvers such as Gurobi.},
	urldate = {2023-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Jialin and lanka, ravi and Yue, Yisong and Dilkina, Bistra},
	year = {2020},
	keywords = {Pred-and-opt, neighborhood search},
	pages = {20012--20023},
}

@article{prat_learning_2023,
	title = {Learning {Active} {Constraints} to {Efficiently} {Solve} {Linear} {Bilevel} {Problems}: {Application} to the {Generator} {Strategic} {Bidding} {Problem}},
	volume = {38},
	issn = {1558-0679},
	shorttitle = {Learning {Active} {Constraints} to {Efficiently} {Solve} {Linear} {Bilevel} {Problems}},
	doi = {10.1109/TPWRS.2022.3188432},
	abstract = {Bilevel programming can be used to formulate many problems in the field of power systems, such as strategic bidding. However, common reformulations of bilevel problems to mixed-integer linear programs make solving such problems hard, which impedes their implementation in real-life. In this paper, we significantly improve solution speed and tractability by introducing decision trees to learn the active constraints of the lower-level problem, while avoiding to introduce binaries and big-M constants. The application of machine learning reduces the online solving time, by moving the selection of active constraints to an offline process, and becomes particularly beneficial when the same problem has to be solved multiple times. We apply our approach to the strategic bidding of generators in electricity markets, where generators solve the same problem many times for varying load demand or renewable production. Three methods are developed and applied to the problem of a strategic generator, with a DCOPF in the lower-level. These methods are heuristic and as so, do not provide guarantees of optimality or solution quality. Yet, we show that for networks of varying sizes, the computational burden is significantly reduced, while we also manage to find solutions for strategic bidding problems that were previously intractable.},
	number = {3},
	journal = {IEEE Transactions on Power Systems},
	author = {Prat, Eléa and Chatzivasileiadis, Spyros},
	month = may,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Bilevel programming, Costs, Games, Generators, Machine learning, Power systems, Runtime, Voltage, active set, classifier, mixed-integer linear programming (MILP), stackelberg games, strategic bidding},
	pages = {2376--2387},
}

@misc{jungel_learning-based_2023,
	title = {Learning-based {Online} {Optimization} for {Autonomous} {Mobility}-on-{Demand} {Fleet} {Control}},
	url = {http://arxiv.org/abs/2302.03963},
	doi = {10.48550/arXiv.2302.03963},
	abstract = {Autonomous mobility-on-demand systems are a viable alternative to mitigate many transportation-related externalities in cities, such as rising vehicle volumes in urban areas and transportation-related pollution. However, the success of these systems heavily depends on efficient and effective fleet control strategies. In this context, we study online control algorithms for autonomous mobility-on-demand systems and develop a novel hybrid combinatorial optimization enriched machine learning pipeline which learns online dispatching and rebalancing policies from optimal full-information solutions. We test our hybrid pipeline on large-scale real-world scenarios with different vehicle fleet sizes and various request densities. We show that our approach outperforms state-of-the-art greedy, and model-predictive control approaches with respect to various KPIs, e.g., by up to 17.1\% and on average by 6.3\% in terms of realized profit.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Jungel, Kai and Parmentier, Axel and Schiffer, Maximilian and Vidal, Thibaut},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03963 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, optimization layers},
}

@misc{han_gnn-guided_2023,
	title = {A {GNN}-{Guided} {Predict}-and-{Search} {Framework} for {Mixed}-{Integer} {Linear} {Programming}},
	url = {http://arxiv.org/abs/2302.05636},
	doi = {10.48550/arXiv.2302.05636},
	abstract = {Mixed-integer linear programming (MILP) is widely employed for modeling combinatorial optimization problems. In practice, similar MILP instances with only coefficient variations are routinely solved, and machine learning (ML) algorithms are capable of capturing common patterns across these MILP instances. In this work, we combine ML with optimization and propose a novel predict-and-search framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize graph neural networks to predict the marginal probability of each variable, and then search for the best feasible solution within a properly defined ball around the predicted solution. We conduct extensive experiments on public datasets, and computational results demonstrate that our proposed framework achieves 51.1\% and 9.9\% performance improvements to MILP solvers SCIP and Gurobi on primal gaps, respectively.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Han, Qingyu and Yang, Linxin and Chen, Qian and Zhou, Xiang and Zhang, Dong and Wang, Akang and Sun, Ruoyu and Luo, Xiaodong},
	month = mar,
	year = {2023},
	note = {arXiv:2302.05636 [math]},
	keywords = {GNN, Mathematics - Optimization and Control, Pred-and-opt, end-to-end, imitation learning},
}

@misc{karalias_erdos_2021,
	title = {Erdos {Goes} {Neural}: an {Unsupervised} {Learning} {Framework} for {Combinatorial} {Optimization} on {Graphs}},
	shorttitle = {Erdos {Goes} {Neural}},
	url = {http://arxiv.org/abs/2006.10643},
	doi = {10.48550/arXiv.2006.10643},
	abstract = {Combinatorial optimization problems are notoriously challenging for neural networks, especially in the absence of labeled instances. This work proposes an unsupervised learning framework for CO problems on graphs that can provide integral solutions of certified quality. Inspired by Erdos' probabilistic method, we use a neural network to parametrize a probability distribution over sets. Crucially, we show that when the network is optimized w.r.t. a suitably chosen loss, the learned distribution contains, with controlled probability, a low-cost integral solution that obeys the constraints of the combinatorial problem. The probabilistic proof of existence is then derandomized to decode the desired solutions. We demonstrate the efficacy of this approach to obtain valid solutions to the maximum clique problem and to perform local graph clustering. Our method achieves competitive results on both real datasets and synthetic hard instances.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Karalias, Nikolaos and Loukas, Andreas},
	month = mar,
	year = {2021},
	note = {arXiv:2006.10643 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{deza_machine_2023,
	title = {Machine {Learning} for {Cutting} {Planes} in {Integer} {Programming}: {A} {Survey}},
	shorttitle = {Machine {Learning} for {Cutting} {Planes} in {Integer} {Programming}},
	url = {http://arxiv.org/abs/2302.09166},
	doi = {10.48550/arXiv.2302.09166},
	abstract = {We survey recent work on machine learning (ML) techniques for selecting cutting planes (or cuts) in mixed-integer linear programming (MILP). Despite the availability of various classes of cuts, the task of choosing a set of cuts to add to the linear programming (LP) relaxation at a given node of the branch-and-bound (B\&B) tree has defied both formal and heuristic solutions to date. ML offers a promising approach for improving the cut selection process by using data to identify promising cuts that accelerate the solution of MILP instances. This paper presents an overview of the topic, highlighting recent advances in the literature, common approaches to data collection, evaluation, and ML model architectures. We analyze the empirical results in the literature in an attempt to quantify the progress that has been made and conclude by suggesting avenues for future research.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Deza, Arnaud and Khalil, Elias B.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09166 [cs, math]},
	keywords = {Artificial Intelligence, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{borozan_machine_2023,
	title = {A {Machine} {Learning}-{Enhanced} {Benders} {Decomposition} {Approach} to {Solve} the {Transmission} {Expansion} {Planning} {Problem} under {Uncertainty}},
	url = {http://arxiv.org/abs/2304.07534},
	doi = {10.48550/arXiv.2304.07534},
	abstract = {The necessary decarbonization efforts in energy sectors entail the integration of flexibility assets, as well as increased levels of uncertainty for the planning and operation of power systems. To cope with this in a cost-effective manner, transmission expansion planning (TEP) models need to incorporate progressively more details to represent potential long-term system developments and the operation of power grids with intermittent renewable generation. However, the increased modeling complexities of TEP exercises can easily lead to computationally intractable optimization problems. Currently, most techniques that address computational intractability alter the original problem, thus neglecting critical modeling aspects or affecting the structure of the optimal solution. In this paper, we propose an alternative approach to significantly alleviate the computational burden of large-scale TEP problems. Our approach integrates machine learning (ML) with the well-established Benders decomposition to manage the problem size while preserving solution quality. The proposed ML-enhanced Multicut Benders Decomposition algorithm improves computational efficiency by identifying effective and ineffective optimality cuts via supervised learning techniques. We illustrate the benefits of the proposed methodology by solving a number of multi-stage TEP problems of different sizes, based on the IEEE24 and IEEE118 test systems, while also considering energy storage investment options.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Borozan, Stefan and Giannelos, Spyros and Falugi, Paola and Moreira, Alexandre and Strbac, Goran},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07534 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, imitation learning},
}

@misc{benidis_solving_2023,
	title = {Solving {Recurrent} {MIPs} with {Semi}-supervised {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2302.11992},
	doi = {10.48550/arXiv.2302.11992},
	abstract = {We propose an ML-based model that automates and expedites the solution of MIPs by predicting the values of variables. Our approach is motivated by the observation that many problem instances share salient features and solution structures since they differ only in few (time-varying) parameters. Examples include transportation and routing problems where decisions need to be re-optimized whenever commodity volumes or link costs change. Our method is the first to exploit the sequential nature of the instances being solved periodically, and can be trained with ``unlabeled'' instances, when exact solutions are unavailable, in a semi-supervised setting. Also, we provide a principled way of transforming the probabilistic predictions into integral solutions. Using a battery of experiments with representative binary MIPs, we show the gains of our model over other ML-based optimization approaches.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Benidis, Konstantinos and Rosolia, Ugo and Rangapuram, Syama and Iosifidis, George and Paschos, Georgios},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11992 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, GNN, Mathematics - Optimization and Control, Statistics - Machine Learning, end-to-end, imitation learning},
}

@article{xiao_towards_2022,
	title = {Towards a machine learning-aided metaheuristic framework for a production/distribution system design problem},
	volume = {146},
	issn = {0305-0548},
	url = {https://www.sciencedirect.com/science/article/pii/S0305054822001605},
	doi = {10.1016/j.cor.2022.105897},
	abstract = {Recent advances have seen vast success in the application of metaheuristics in NP-hard combinatorial problems. A generic metaheuristic design usually consists of three core elements that jointly determine the algorithm performance, including an initial candidate solution, a guided search procedure, and a fitness function that approximates the objective value. This paper proposes a data-driven metaheuristic (DDMH) framework that leverages the predictive power of machine learning models, which exploit location information and mine structural knowledge of a supply chain network for intelligent decision making. Specifically, the proposed framework offers three performance boosters, including an initial solution heuristic, a narrowed search space, and an efficient learning-based fitness function. The framework can be readily integrated into existing MHs. As a case study, we apply DDMH to a production/distribution network design problem. Experimental results show that the DDMH outperforms the traditional MHs with better solution quality and comparable running time, especially for hard problems.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Computers \& Operations Research},
	author = {Xiao, Zhifeng and Zhi, Jianing and Keskin, Burcu B.},
	month = oct,
	year = {2022},
	keywords = {Fitness function, Initial solution, Machine learning, Metaheuristics, Network design, Pred-and-opt, Search space, end-to-end},
	pages = {105897},
}

@article{khalil_mip-gnn_2022,
	title = {{MIP}-{GNN}: {A} {Data}-{Driven} {Framework} for {Guiding} {Combinatorial} {Solvers}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{MIP}-{GNN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21262},
	doi = {10.1609/aaai.v36i9.21262},
	abstract = {Mixed-integer programming (MIP) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program (MILP) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary MILPs. Our code and appendix are publicly available at https://github.com/lyeskhalil/mipGNN.},
	language = {en},
	number = {9},
	urldate = {2023-04-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Khalil, Elias B. and Morris, Christopher and Lodi, Andrea},
	month = jun,
	year = {2022},
	note = {Number: 9},
	keywords = {*, GNN, Machine Learning (ML), Pred-and-opt, end-to-end, imitation learning},
	pages = {10219--10227},
}

@article{jimenez-cordero_warm-starting_2022,
	title = {Warm-starting constraint generation for mixed-integer optimization: {A} {Machine} {Learning} approach},
	volume = {253},
	issn = {0950-7051},
	shorttitle = {Warm-starting constraint generation for mixed-integer optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122007894},
	doi = {10.1016/j.knosys.2022.109570},
	abstract = {Mixed Integer Linear Programs (MILP) are well known to be NP-hard (Non-deterministic Polynomial-time hard) problems in general. Even though pure optimization-based methods, such as constraint generation, are guaranteed to provide an optimal solution if enough time is given, their use in online applications remains a great challenge due to their usual excessive time requirements. To alleviate their computational burden, some machine learning techniques (ML) have been proposed in the literature, using the information provided by previously solved MILP instances. Unfortunately, these techniques report a non-negligible percentage of infeasible or suboptimal instances. By linking mathematical optimization and machine learning, this paper proposes a novel approach that speeds up the traditional constraint generation method, preserving feasibility and optimality guarantees. In particular, we first identify offline the so-called invariant constraint set of past MILP instances. We then train (also offline) a machine learning method to learn an invariant constraint set as a function of the problem parameters of each instance. Next, we predict online an invariant constraint set of the new unseen MILP application and use it to initialize the constraint generation method. This warm-started strategy significantly reduces the number of iterations to reach optimality, and therefore, the computational burden to solve online each MILP problem is significantly reduced. Very importantly, all the feasibility and optimality theoretical guarantees of the traditional constraint generation method are inherited by our proposed methodology. The computational performance of the proposed approach is quantified through synthetic and real-life MILP applications.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Knowledge-Based Systems},
	author = {Jiménez-Cordero, Asunción and Morales, Juan Miguel and Pineda, Salvador},
	month = oct,
	year = {2022},
	keywords = {Constraint generation, Feasibility and optimality guarantees, Machine learning, Mixed integer linear programming, Warm-start, imitation learning},
	pages = {109570},
}

@misc{duan_augment_2022,
	title = {Augment with {Care}: {Contrastive} {Learning} for {Combinatorial} {Problems}},
	shorttitle = {Augment with {Care}},
	url = {http://arxiv.org/abs/2202.08396},
	doi = {10.48550/arXiv.2202.08396},
	abstract = {Supervised learning can improve the design of state-of-the-art solvers for combinatorial problems, but labelling large numbers of combinatorial instances is often impractical due to exponential worst-case complexity. Inspired by the recent success of contrastive pre-training for images, we conduct a scientific study of the effect of augmentation design on contrastive pre-training for the Boolean satisfiability problem. While typical graph contrastive pre-training uses label-agnostic augmentations, our key insight is that many combinatorial problems have well-studied invariances, which allow for the design of label-preserving augmentations. We find that label-preserving augmentations are critical for the success of contrastive pre-training. We show that our representations are able to achieve comparable test accuracy to fully-supervised learning while using only 1\% of the labels. We also demonstrate that our representations are more transferable to larger problems from unseen domains. Our code is available at https://github.com/h4duan/contrastive-sat.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Duan, Haonan and Vaezipoor, Pashootan and Paulus, Max B. and Ruan, Yangjun and Maddison, Chris J.},
	month = jun,
	year = {2022},
	note = {arXiv:2202.08396 [cs]},
	keywords = {*, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, GNN, background, end-to-end, imitation learning},
}

@inproceedings{gasse_machine_2022,
	title = {The {Machine} {Learning} for {Combinatorial} {Optimization} {Competition} ({ML4CO}): {Results} and {Insights}},
	shorttitle = {The {Machine} {Learning} for {Combinatorial} {Optimization} {Competition} ({ML4CO})},
	url = {https://proceedings.mlr.press/v176/gasse22a.html},
	abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning as a new approach for solving combinatorial problems, either directly as solvers or by enhancing exact solvers. Based on this context, the ML4CO aims at improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components. The competition featured three challenging tasks: finding the best feasible solution, producing the tightest optimality certificate, and giving an appropriate solver configuration. Three realistic datasets were considered: balanced item placement, workload apportionment, and maritime inventory routing. This last dataset was kept anonymous for the contestants.},
	language = {en},
	urldate = {2023-04-28},
	booktitle = {Proceedings of the {NeurIPS} 2021 {Competitions} and {Demonstrations} {Track}},
	publisher = {PMLR},
	author = {Gasse, Maxime and Bowly, Simon and Cappart, Quentin and Charfreitag, Jonas and Charlin, Laurent and Chételat, Didier and Chmiela, Antonia and Dumouchelle, Justin and Gleixner, Ambros and Kazachkov, Aleksandr M. and Khalil, Elias and Lichocki, Pawel and Lodi, Andrea and Lubin, Miles and Maddison, Chris J. and Christopher, Morris and Papageorgiou, Dimitri J. and Parjadis, Augustin and Pokutta, Sebastian and Prouvost, Antoine and Scavuzzo, Lara and Zarpellon, Giulia and Yang, Linxin and Lai, Sha and Wang, Akang and Luo, Xiaodong and Zhou, Xiang and Huang, Haohan and Shao, Shengcheng and Zhu, Yuanming and Zhang, Dong and Quan, Tao and Cao, Zixuan and Xu, Yang and Huang, Zhewei and Zhou, Shuchang and Binbin, Chen and Minggui, He and Hao, Hao and Zhiyu, Zhang and Zhiwu, An and Kun, Mao},
	month = jul,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {*, end-to-end, imitation learning},
	pages = {220--231},
}

@misc{dalle_learning_2022,
	title = {Learning with {Combinatorial} {Optimization} {Layers}: a {Probabilistic} {Approach}},
	shorttitle = {Learning with {Combinatorial} {Optimization} {Layers}},
	url = {http://arxiv.org/abs/2207.13513},
	doi = {10.48550/arXiv.2207.13513},
	abstract = {Combinatorial optimization (CO) layers in machine learning (ML) pipelines are a powerful tool to tackle data-driven decision tasks, but they come with two main challenges. First, the solution of a CO problem often behaves as a piecewise constant function of its objective parameters. Given that ML pipelines are typically trained using stochastic gradient descent, the absence of slope information is very detrimental. Second, standard ML losses do not work well in combinatorial settings. A growing body of research addresses these challenges through diverse methods. Unfortunately, the lack of well-maintained implementations slows down the adoption of CO layers. In this paper, building upon previous works, we introduce a probabilistic perspective on CO layers, which lends itself naturally to approximate differentiation and the construction of structured losses. We recover many approaches from the literature as special cases, and we also derive new ones. Based on this unifying perspective, we present InferOpt.jl, an open-source Julia package that 1) allows turning any CO oracle with a linear objective into a differentiable layer, and 2) defines adequate losses to train pipelines containing such layers. Our library works with arbitrary optimization algorithms, and it is fully compatible with Julia's ML ecosystem. We demonstrate its abilities using a pathfinding problem on video game maps as guiding example, as well as three other applications from operations research.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Dalle, Guillaume and Baty, Léo and Bouvier, Louis and Parmentier, Axel},
	month = dec,
	year = {2022},
	note = {arXiv:2207.13513 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, optimization layers},
}

@misc{chen_representing_2022,
	title = {On {Representing} {Mixed}-{Integer} {Linear} {Programs} by {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2210.10759},
	doi = {10.48550/arXiv.2210.10759},
	abstract = {While Mixed-integer linear programming (MILP) is NP-hard in general, practical MILP has received roughly 100--fold speedup in the past twenty years. Still, many classes of MILPs quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for MILPs. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks (GNNs) to making decisions in various stages of MILP solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible MILPs that all GNNs will, however, treat equally, indicating GNN's lacking power to express general MILPs. Then, we show that, by restricting the MILPs to unfoldable ones or by adding random features, there exist GNNs that can reliably predict MILP feasibility, optimal objective values, and optimal solutions up to prescribed precision. We conducted small-scale numerical experiments to validate our theoretical findings.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Chen, Ziang and Liu, Jialin and Wang, Xinshang and Lu, Jianfeng and Yin, Wotao},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10759 [cs, math]},
	keywords = {*, Computer Science - Machine Learning, GNN, Mathematics - Optimization and Control, end-to-end, imitation learning},
}

@article{bother_whats_2022,
	title = {What's {Wrong} with {Deep} {Learning} in {Tree} {Search} for {Combinatorial} {Optimization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2201.10494},
	doi = {10.48550/ARXIV.2201.10494},
	abstract = {Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard Maximum Independent Set problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality.},
	urldate = {2023-05-17},
	author = {Böther, Maximilian and Kißig, Otto and Taraz, Martin and Cohen, Sarel and Seidel, Karen and Friedrich, Tobias},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Mathematics, Machine Learning (cs.LG), Optimization and Control (math.OC), Pred-and-opt, background, end-to-end},
}

@article{anderson_generative_2022,
	title = {Generative deep learning for decision making in gas networks},
	volume = {95},
	issn = {1432-5217},
	url = {https://doi.org/10.1007/s00186-022-00777-x},
	doi = {10.1007/s00186-022-00777-x},
	abstract = {A decision support system relies on frequent re-solving of similar problem instances. While the general structure remains the same in corresponding applications, the input parameters are updated on a regular basis. We propose a generative neural network design for learning integer decision variables of mixed-integer linear programming (MILP) formulations of these problems. We utilise a deep neural network discriminator and a MILP solver as our oracle to train our generative neural network. In this article, we present the results of our design applied to the transient gas optimisation problem. The trained generative neural network produces a feasible solution in 2.5s, and when used as a warm start solution, decreases global optimal solution time by 60.5\%.},
	language = {en},
	number = {3},
	urldate = {2023-04-27},
	journal = {Mathematical Methods of Operations Research},
	author = {Anderson, Lovis and Turner, Mark and Koch, Thorsten},
	month = jun,
	year = {2022},
	keywords = {Deep learning, Gas networks, Generative modelling, Mixed-integer programming, Pred-and-opt, Primal heuristic, SKIMMED, end-to-end, imitation learning},
	pages = {503--532},
}

@article{srinivasan_fast_2021,
	series = {Modeling, {Estimation} and {Control} {Conference} {MECC} 2021},
	title = {Fast {Multi}-{Robot} {Motion} {Planning} via {Imitation} {Learning} of {Mixed}-{Integer} {Programs}},
	volume = {54},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896321022813},
	doi = {10.1016/j.ifacol.2021.11.237},
	abstract = {We propose a centralized multi-robot motion planning approach that leverages machine learning and mixed-integer programming (MIP). We train a neural network to imitate optimal MIP solutions and, during execution, the trajectories predicted by the network are used to fix most of the integer variables, resulting in a significantly reduced MIP or even a convex program. If the obtained trajectories are feasible, i.e., collision-free and reaching the goal, they can be used as they are or further refined towards optimality. Since maximizing the likelihood of feasibility is not the standard goal of imitation learning, we propose several techniques aimed at increasing such likelihood. Simulation results show the reduced computational burden associated with the proposed framework and the similarity with the optimal MIP solutions.},
	language = {en},
	number = {20},
	urldate = {2023-04-27},
	journal = {IFAC-PapersOnLine},
	author = {Srinivasan, Mohit and Chakrabarty, Ankush and Quirynen, Rien and Yoshikawa, Nobuyuki and Mariyama, Toshisada and Cairano, Stefano Di},
	month = jan,
	year = {2021},
	keywords = {Machine Learning, Motion Control, Optimal Control, Path Planning, Pred-and-opt, end-to-end, imitation learning},
	pages = {598--604},
}

@misc{prouvost_ecole_2021,
	title = {Ecole: {A} {Library} for {Learning} {Inside} {MILP} {Solvers}},
	shorttitle = {Ecole},
	url = {http://arxiv.org/abs/2104.02828},
	doi = {10.48550/arXiv.2104.02828},
	abstract = {In this paper we describe Ecole (Extensible Combinatorial Optimization Learning Environments), a library to facilitate integration of machine learning in combinatorial optimization solvers. It exposes sequential decision making that must be performed in the process of solving as Markov decision processes. This means that, rather than trying to predict solutions to combinatorial optimization problems directly, Ecole allows machine learning to work in cooperation with a state-of-the-art a mixed-integer linear programming solver that acts as a controllable algorithm. Ecole provides a collection of computationally efficient, ready to use learning environments, which are also easy to extend to define novel training tasks. Documentation and code can be found at https://www.ecole.ai.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Prouvost, Antoine and Dumouchelle, Justin and Gasse, Maxime and Chételat, Didier and Lodi, Andrea},
	month = apr,
	year = {2021},
	note = {arXiv:2104.02828 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, background},
}

@article{peng_graph_2021,
	title = {Graph {Learning} for {Combinatorial} {Optimization}: {A} {Survey} of {State}-of-the-{Art}},
	volume = {6},
	issn = {2364-1541},
	shorttitle = {Graph {Learning} for {Combinatorial} {Optimization}},
	url = {https://doi.org/10.1007/s41019-021-00155-3},
	doi = {10.1007/s41019-021-00155-3},
	abstract = {Graphs have been widely used to represent complex data in many applications, such as e-commerce, social networks, and bioinformatics. Efficient and effective analysis of graph data is important for graph-based applications. However, most graph analysis tasks are combinatorial optimization (CO) problems, which are NP-hard. Recent studies have focused a lot on the potential of using machine learning (ML) to solve graph-based CO problems. Most recent methods follow the two-stage framework. The first stage is graph representation learning, which embeds the graphs into low-dimension vectors. The second stage uses machine learning to solve the CO problems using the embeddings of the graphs learned in the first stage. The works for the first stage can be classified into two categories, graph embedding methods and end-to-end learning methods. For graph embedding methods, the learning of the the embeddings of the graphs has its own objective, which may not rely on the CO problems to be solved. The CO problems are solved by independent downstream tasks. For end-to-end learning methods, the learning of the embeddings of the graphs does not have its own objective and is an intermediate step of the learning procedure of solving the CO problems. The works for the second stage can also be classified into two categories, non-autoregressive methods and autoregressive methods. Non-autoregressive methods predict a solution for a CO problem in one shot. A non-autoregressive method predicts a matrix that denotes the probability of each node/edge being a part of a solution of the CO problem. The solution can be computed from the matrix using search heuristics such as beam search. Autoregressive methods iteratively extend a partial solution step by step. At each step, an autoregressive method predicts a node/edge conditioned to current partial solution, which is used to its extension. In this survey, we provide a thorough overview of recent studies of the graph learning-based CO methods. The survey ends with several remarks on future research directions.},
	language = {en},
	number = {2},
	urldate = {2023-04-28},
	journal = {Data Science and Engineering},
	author = {Peng, Yun and Choi, Byron and Xu, Jianliang},
	month = jun,
	year = {2021},
	keywords = {Combinational optimization, GNN, Graph neural network, Graph representation learning, SURVEY},
	pages = {119--141},
}

@misc{nair_solving_2021,
	title = {Solving {Mixed} {Integer} {Programs} {Using} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2012.13349},
	doi = {10.48550/arXiv.2012.13349},
	abstract = {Mixed Integer Programming (MIP) solvers rely on an array of sophisticated heuristics developed with decades of research to solve large-scale MIP instances encountered in practice. Machine learning offers to automatically construct better heuristics from data by exploiting shared structure among instances in the data. This paper applies learning to the two key sub-tasks of a MIP solver, generating a high-quality joint variable assignment, and bounding the gap in objective value between that assignment and an optimal one. Our approach constructs two corresponding neural network-based components, Neural Diving and Neural Branching, to use in a base MIP solver such as SCIP. Neural Diving learns a deep neural network to generate multiple partial assignments for its integer variables, and the resulting smaller MIPs for un-assigned variables are solved with SCIP to construct high quality joint assignments. Neural Branching learns a deep neural network to make variable selection decisions in branch-and-bound to bound the objective value gap with a small tree. This is done by imitating a new variant of Full Strong Branching we propose that scales to large instances using GPUs. We evaluate our approach on six diverse real-world datasets, including two Google production datasets and MIPLIB, by training separate neural networks on each. Most instances in all the datasets combined have \$10{\textasciicircum}3-10{\textasciicircum}6\$ variables and constraints after presolve, which is significantly larger than previous learning approaches. Comparing solvers with respect to primal-dual gap averaged over a held-out set of instances, the learning-augmented SCIP is 2x to 10x better on all datasets except one on which it is \$10{\textasciicircum}5\$x better, at large time limits. To the best of our knowledge, ours is the first learning approach to demonstrate such large improvements over SCIP on both large-scale real-world application datasets and MIPLIB.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Nair, Vinod and Bartunov, Sergey and Gimeno, Felix and von Glehn, Ingrid and Lichocki, Pawel and Lobov, Ivan and O'Donoghue, Brendan and Sonnerat, Nicolas and Tjandraatmadja, Christian and Wang, Pengming and Addanki, Ravichandra and Hapuarachchi, Tharindi and Keck, Thomas and Keeling, James and Kohli, Pushmeet and Ktena, Ira and Li, Yujia and Vinyals, Oriol and Zwols, Yori},
	month = jul,
	year = {2021},
	note = {arXiv:2012.13349 [cs, math]},
	keywords = {*, Computer Science - Artificial Intelligence, Computer Science - Discrete Mathematics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, end-to-end, imitation learning},
}

@inproceedings{mccarty_nn-baker_2021,
	title = {{NN}-{Baker}: {A} {Neural}-network {Infused} {Algorithmic} {Framework} for {Optimization} {Problems} on {Geometric} {Intersection} {Graphs}},
	volume = {34},
	shorttitle = {{NN}-{Baker}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/c236337b043acf93c7df397fdb9082b3-Abstract.html},
	abstract = {Recent years have witnessed a surge of approaches to use neural networks to help tackle combinatorial optimization problems, including graph optimization problems. However, theoretical understanding of such approaches remains limited. In this paper, we consider the geometric setting, where graphs are induced by points in a fixed dimensional Euclidean space. We show that several graph optimization problems can be approximated by an algorithm that is polynomial in graph size n via a framework we propose, call the Baker-paradigm. More importantly, a key advantage of the Baker-paradigm is that it decomposes the input problem into (at most linear number of) small sub-problems of fixed sizes (independent of the size of the input). For the family of such fixed-size sub-problems, we can now design neural networks with universal approximation guarantees to solve them. This leads to a mixed algorithmic-ML framework, which we call NN-Baker that has the capacity to approximately solve a family of graph optimization problems (e.g, maximum independent set and minimum vertex cover) in time linear to input graph size, and only polynomial to approximation parameter. We instantiate our NN-Baker by a CNN version and GNN version, and demonstrate the effectiveness and efficiency of our approach via a range of experiments.},
	urldate = {2023-04-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {McCarty, Evan and Zhao, Qi and Sidiropoulos, Anastasios and Wang, Yusu},
	year = {2021},
	pages = {23023--23035},
}

@inproceedings{kotary_end--end_2021,
	address = {Montreal, Canada},
	title = {End-to-{End} {Constrained} {Optimization} {Learning}: {A} {Survey}},
	isbn = {978-0-9992411-9-6},
	shorttitle = {End-to-{End} {Constrained} {Optimization} {Learning}},
	url = {https://www.ijcai.org/proceedings/2021/610},
	doi = {10.24963/ijcai.2021/610},
	abstract = {This paper surveys the recent attempts at leveraging machine learning to solve constrained optimization problems. It focuses on surveying the work on integrating combinatorial solvers and optimization methods with machine learning architectures.

These approaches hold the promise to develop new hybrid machine learning and optimization methods to predict fast, approximate, solutions to combinatorial problems and to enable structural logical inference. This paper presents a conceptual review of the recent advancements in this emerging area.},
	language = {en},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
	month = aug,
	year = {2021},
	keywords = {*, SKIMMED, background, constraint learning, optimization layers},
	pages = {4475--4482},
}

@article{detassis_teaching_2021,
	title = {Teaching the {Old} {Dog} {New} {Tricks}: {Supervised} {Learning} with {Constraints}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Teaching the {Old} {Dog} {New} {Tricks}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16491},
	doi = {10.1609/aaai.v35i5.16491},
	abstract = {Adding constraint support in Machine Learning has the potential to address outstanding issues in data-driven AI systems, such as safety and fairness. Existing approaches typically apply constrained optimization techniques to ML training, enforce constraint satisfaction by adjusting the model design, or use constraints to correct the output. Here, we investigate a different, complementary, strategy based on "teaching" constraint satisfaction to a supervised ML method via the direct use of a state-of-the-art constraint solver: this enables taking advantage of decades of research on constrained optimization with limited effort. In practice, we use a decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised ML model and training algorithm can be employed). The process leads to approximate constraint satisfaction in general, and convergence properties are difficult to establish; despite this fact, we found empirically that even a naive setup of our approach performs well on ML tasks with fairness constraints, and on classical datasets with synthetic constraints.},
	language = {en},
	number = {5},
	urldate = {2023-05-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Detassis, Fabrizio and Lombardi, Michele and Milano, Michela},
	month = may,
	year = {2021},
	note = {Number: 5},
	keywords = {background, constraint learning, imitation learning},
	pages = {3742--3749},
}

@article{larsen_predicting_2022,
	title = {Predicting {Tactical} {Solutions} to {Operational} {Planning} {Problems} {Under} {Imperfect} {Information}},
	volume = {34},
	issn = {1091-9856},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2021.1091},
	doi = {10.1287/ijoc.2021.1091},
	abstract = {This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions (TDOSs). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected TDOS associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs.},
	number = {1},
	urldate = {2023-05-22},
	journal = {INFORMS Journal on Computing},
	author = {Larsen, Eric and Lachapelle, Sébastien and Bengio, Yoshua and Frejinger, Emma and Lacoste-Julien, Simon and Lodi, Andrea},
	month = jan,
	year = {2022},
	note = {Publisher: INFORMS},
	keywords = {deep learning, end-to-end, imitation learning, integer linear programming, stochastic programming, supervised learning},
	pages = {227--242},
}

@inproceedings{nowak_revised_2018,
	title = {{REVISED} {NOTE} {ON} {LEARNING} {QUADRATIC} {ASSIGNMENT} {WITH} {GRAPH} {NEURAL} {NETWORKS}},
	doi = {10.1109/DSW.2018.8439919},
	abstract = {Inverse problems correspond to a certain type of optimization problems formulated over appropriate input distributions. Recently, there has been a growing interest in understanding the computational hardness of these optimization problems, not only in the worst case, but in an average-complexity sense under this same input distribution.In this revised note, we are interested in studying another aspect of hardness, related to the ability to learn how to solve a problem by simply observing a collection of previously solved instances. These `planted solutions' are used to supervise the training of an appropriate predictive model that parametrizes a broad class of algorithms, with the hope that the resulting model will provide good accuracy-complexity tradeoffs in the average sense.We illustrate this setup on the Quadratic Assignment Problem, a fundamental problem in Network Science. We observe that data-driven models based on Graph Neural Networks offer intriguingly good performance, even in regimes where standard relaxation based techniques appear to suffer.},
	booktitle = {2018 {IEEE} {Data} {Science} {Workshop} ({DSW})},
	author = {Nowak, Alex and Villar, Soledad and Bandeira, Afonso S and Bruna, Joan},
	month = jun,
	year = {2018},
	keywords = {Complexity theory, Computational modeling, Computer architecture, GNN, Neural networks, Optimization, Symmetric matrices, Task analysis, end-to-end, imitation learning},
	pages = {1--5},
}

@misc{kool_attention_2019,
	title = {Attention, {Learn} to {Solve} {Routing} {Problems}!},
	url = {http://arxiv.org/abs/1803.08475},
	doi = {10.48550/arXiv.1803.08475},
	abstract = {The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
	month = feb,
	year = {2019},
	note = {arXiv:1803.08475 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, end-to-end},
}

@inproceedings{vinyals_pointer_2015,
	title = {Pointer {Networks}},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/29921001f2f04bd3baee84a12e98098f-Abstract.html},
	abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence.Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in eachstep of the output depends on the length of the input, which is variable.Problems such as sorting variable sized sequences, and various combinatorialoptimization problems belong to this class.  Our model solvesthe problem of variable size output dictionaries using a recently proposedmechanism of neural attention. It differs from the previous attentionattempts in that, instead of using attention to blend hidden units of anencoder to a context vector at each decoder step, it uses attention asa pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).We show Ptr-Nets can be used to learn approximate solutions to threechallenging geometric problems -- finding planar convex hulls, computingDelaunay triangulations, and the planar Travelling Salesman Problem-- using training examples alone. Ptr-Nets not only improve oversequence-to-sequence with input attention, butalso allow us to generalize to variable size output dictionaries.We show that the learnt models generalize beyond the maximum lengthsthey were trained on. We hope our results on these taskswill encourage a broader exploration of neural learning for discreteproblems.},
	urldate = {2023-05-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	year = {2015},
	keywords = {*, background, end-to-end, imitation learning},
}

@inproceedings{fischetti_learning_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {MILP} {Resolution} {Outcomes} {Before} {Reaching} {Time}-{Limit}},
	isbn = {978-3-030-19212-9},
	doi = {10.1007/978-3-030-19212-9_18},
	abstract = {The resolution of some Mixed-Integer Linear Programming (MILP) problems still presents challenges for state-of-the-art optimization solvers and may require hours of computations, so that a time-limit to the resolution process is typically provided by a user. Nevertheless, it could be useful to get a sense of the optimization trends after only a fraction of the specified total time has passed, and ideally be able to tailor the use of the remaining resolution time accordingly, in a more strategic and flexible way. Looking at the evolution of a partial branch-and-bound tree for a MILP instance, developed up to a certain fraction of the time-limit, we aim to predict whether the problem will be solved to proven optimality before timing out. We exploit machine learning tools, and summarize the development and progress of a MILP resolution process to cast a prediction within a classification framework. Experiments on benchmark instances show that a valuable statistical pattern can indeed be learned during MILP resolution, with key predictive features reflecting the know-how and experience of field’s practitioners.},
	language = {en},
	booktitle = {Integration of {Constraint} {Programming}, {Artificial} {Intelligence}, and {Operations} {Research}},
	publisher = {Springer International Publishing},
	author = {Fischetti, Martina and Lodi, Andrea and Zarpellon, Giulia},
	editor = {Rousseau, Louis-Martin and Stergiou, Kostas},
	year = {2019},
	keywords = {Branch and Bound, MILP resolution, Machine learning, imitation learning},
	pages = {275--291},
}

@misc{loukas_what_2020,
	title = {What graph neural networks cannot learn: depth vs width},
	shorttitle = {What graph neural networks cannot learn},
	url = {http://arxiv.org/abs/1907.03199},
	doi = {10.48550/arXiv.1907.03199},
	abstract = {This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp's depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Loukas, Andreas},
	month = jan,
	year = {2020},
	note = {arXiv:1907.03199 [cs, stat]},
	keywords = {Computer Science - Machine Learning, GNN, Statistics - Machine Learning, background},
}

@misc{gasse_exact_2019,
	title = {Exact {Combinatorial} {Optimization} with {Graph} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.01629},
	doi = {10.48550/arXiv.1906.01629},
	abstract = {Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Gasse, Maxime and Chételat, Didier and Ferroni, Nicola and Charlin, Laurent and Lodi, Andrea},
	month = oct,
	year = {2019},
	note = {[A]},
	keywords = {*, Computer Science - Machine Learning, GNN, Mathematics - Optimization and Control, Statistics - Machine Learning, background, imitation learning},
}

@article{li_combinatorial_2018,
	title = {Combinatorial {Optimization} with {Graph} {Convolutional} {Networks} and {Guided} {Tree} {Search}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1810.10659},
	doi = {10.48550/ARXIV.1810.10659},
	abstract = {We present a learning-based approach to computing solutions for certain NP-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical NP-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent deep learning work, and performs on par with highly optimized state-of-the-art heuristic solvers for some NP-hard problems. Experiments indicate that our approach generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.},
	urldate = {2023-05-17},
	author = {Li, Zhuwen and Chen, Qifeng and Koltun, Vladlen},
	year = {2018},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1609.02907},
	doi = {10.48550/ARXIV.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-05-17},
	author = {Kipf, Thomas N. and Welling, Max},
	year = {2016},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, GNN, Machine Learning (cs.LG), Machine Learning (stat.ML), background},
}

@article{morris_weisfeiler_2019,
	title = {Weisfeiler and {Leman} {Go} {Neural}: {Higher}-{Order} {Graph} {Neural} {Networks}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Weisfeiler and {Leman} {Go} {Neural}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4384},
	doi = {10.1609/aaai.v33i01.33014602},
	abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
	number = {01},
	urldate = {2023-05-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
	month = jul,
	year = {2019},
	keywords = {GNN, background},
	pages = {4602--4609},
}

@article{prates_learning_2019,
	title = {Learning to {Solve} {NP}-{Complete} {Problems}: {A} {Graph} {Neural} {Network} for {Decision} {TSP}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Learning to {Solve} {NP}-{Complete} {Problems}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4399},
	doi = {10.1609/aaai.v33i01.33014731},
	abstract = {Graph Neural Networks (GNN) are a promising technique for bridging differential programming and combinatorial domains. GNNs employ trainable modules which can be assembled in different configurations that reflect the relational structure of each problem instance. In this paper, we show that GNNs can learn to solve, with very little supervision, the decision variant of the Traveling Salesperson Problem (TSP), a highly relevant NP-Complete problem. Our model is trained to function as an effective message-passing algorithm in which edges (embedded with their weights) communicate with vertices for a number of iterations after which the model is asked to decide whether a route with cost {\textless} C exists. We show that such a network can be trained with sets of dual examples: given the optimal tour cost C∗, we produce one decision instance with target cost x\% smaller and one with target cost x\% larger than C∗. We were able to obtain 80\% accuracy training with −2\%,+2\% deviations, and the same trained model can generalize for more relaxed deviations with increasing performance. We also show that the model is capable of generalizing for larger problem sizes. Finally, we provide a method for predicting the optimal route cost within 2\% deviation from the ground truth. In summary, our work shows that Graph Neural Networks are powerful enough to solve NP-Complete problems which combine symbolic and numeric data.},
	number = {01},
	urldate = {2023-05-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Prates, Marcelo and Avelar, Pedro H. C. and Lemos, Henrique and Lamb, Luis C. and Vardi, Moshe Y.},
	month = jul,
	year = {2019},
	keywords = {*, GNN},
	pages = {4731--4738},
}

@misc{ferber_mipaal_2019,
	title = {{MIPaaL}: {Mixed} {Integer} {Program} as a {Layer}},
	shorttitle = {{MIPaaL}},
	url = {http://arxiv.org/abs/1907.05912},
	doi = {10.48550/arXiv.1907.05912},
	abstract = {Machine learning components commonly appear in larger decision-making pipelines; however, the model training process typically focuses only on a loss that measures accuracy between predicted values and ground truth values. Decision-focused learning explicitly integrates the downstream decision problem when training the predictive model, in order to optimize the quality of decisions induced by the predictions. It has been successfully applied to several limited combinatorial problem classes, such as those that can be expressed as linear programs (LP), and submodular optimization. However, these previous applications have uniformly focused on problems from specific classes with simple constraints. Here, we enable decision-focused learning for the broad class of problems that can be encoded as a Mixed Integer Linear Program (MIP), hence supporting arbitrary linear constraints over discrete and continuous variables. We show how to differentiate through a MIP by employing a cutting planes solution approach, which is an exact algorithm that iteratively adds constraints to a continuous relaxation of the problem until an integral solution is found. We evaluate our new end-to-end approach on several real world domains and show that it outperforms the standard two phase approaches that treat prediction and prescription separately, as well as a baseline approach of simply applying decision-focused learning to the LP relaxation of the MIP.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Ferber, Aaron and Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	month = jul,
	year = {2019},
	note = {arXiv:1907.05912 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, optimization layers},
}

@misc{berthet_learning_2020,
	title = {Learning with {Differentiable} {Perturbed} {Optimizers}},
	url = {http://arxiv.org/abs/2002.08676},
	doi = {10.48550/arXiv.2002.08676},
	abstract = {Machine learning pipelines often rely on optimization procedures to make discrete decisions (e.g., sorting, picking closest neighbors, or shortest paths). Although these discrete decisions are easily computed, they break the back-propagation of computational graphs. In order to expand the scope of learning problems that can be solved in an end-to-end fashion, we propose a systematic method to transform optimizers into operations that are differentiable and never locally constant. Our approach relies on stochastically perturbed optimizers, and can be used readily together with existing solvers. Their derivatives can be evaluated efficiently, and smoothness tuned via the chosen noise amplitude. We also show how this framework can be connected to a family of losses developed in structured prediction, and give theoretical guarantees for their use in learning tasks. We demonstrate experimentally the performance of our approach on various tasks.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Cuturi, Marco and Vert, Jean-Philippe and Bach, Francis},
	month = jun,
	year = {2020},
	note = {arXiv:2002.08676 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, optimization layers},
}

@misc{wilder_melding_2018,
	title = {Melding the {Data}-{Decisions} {Pipeline}: {Decision}-{Focused} {Learning} for {Combinatorial} {Optimization}},
	shorttitle = {Melding the {Data}-{Decisions} {Pipeline}},
	url = {http://arxiv.org/abs/1809.05504},
	doi = {10.48550/arXiv.1809.05504},
	abstract = {Creating impact in real-world settings requires artificial intelligence techniques to span the full pipeline from data, to predictive models, to decisions. These components are typically approached separately: a machine learning model is first trained via a measure of predictive accuracy, and then its predictions are used as input into an optimization algorithm which produces a decision. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible. Hand-tuning the loss function to align with optimization is a difficult and error-prone process (which is often skipped entirely). We focus on combinatorial optimization problems and introduce a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce high-quality decisions. Technically, our contribution is a means of integrating common classes of discrete optimization problems into deep learning or other predictive models, which are typically trained via gradient descent. The main idea is to use a continuous relaxation of the discrete problem to propagate gradients through the optimization procedure. We instantiate this framework for two broad classes of combinatorial problems: linear programs and submodular maximization. Experimental results across a variety of domains show that decision-focused learning often leads to improved optimization performance compared to traditional methods. We find that standard measures of accuracy are not a reliable proxy for a predictive model's utility in optimization, and our method's ability to specify the true goal as the model's training objective yields substantial dividends across a range of decision problems.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	month = nov,
	year = {2018},
	note = {arXiv:1809.05504 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, optimization layers},
}

@article{khalil_learning_2017-1,
	title = {Learning to {Run} {Heuristics} in {Tree} {Search}},
	url = {https://www.ijcai.org/proceedings/2017/92},
	abstract = {Electronic proceedings of IJCAI 2017},
	urldate = {2023-05-08},
	author = {Khalil, Elias B. and Dilkina, Bistra and Nemhauser, George L. and Ahmed, Shabbir and Shao, Yufen},
	year = {2017},
	keywords = {*, primal heuristics},
	pages = {659--666},
}

@article{liberto_dash_2016,
	title = {{DASH}: {Dynamic} {Approach} for {Switching} {Heuristics}},
	volume = {248},
	issn = {0377-2217},
	shorttitle = {{DASH}},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221715007559},
	doi = {10.1016/j.ejor.2015.08.018},
	abstract = {Complete tree search is a highly effective method for tackling Mixed-Integer Programming (MIP) problems, and over the years, a plethora of branching heuristics have been introduced to further refine the technique for varying problems. Yet while each new approach continued to push the state-of-the-art, parallel research began to repeatedly demonstrate that there is no single method that would perform the best on all problem instances. Tackling this issue, portfolio algorithms took the process a step further, by trying to predict the best heuristic for each instance at hand. However, the motivation behind algorithm selection can be taken further still, and used to dynamically choose the most appropriate algorithm for each encountered sub-problem. In this paper we identify a feature space that captures both the evolution of the problem in the branching tree and the similarity among sub-problems of instances from the same MIP models. We show how to exploit these features on-the-fly in order to decide the best time to switch the branching variable selection heuristic and then show how such a system can be trained efficiently. Experiments on a highly heterogeneous collection of hard MIP instances show significant gains over the standard pure approach which commits to a single heuristic throughout the search.},
	language = {en},
	number = {3},
	urldate = {2023-04-19},
	journal = {European Journal of Operational Research},
	author = {Liberto, Giovanni Di and Kadioglu, Serdar and Leo, Kevin and Malitsky, Yuri},
	month = feb,
	year = {2016},
	note = {[B] SKIMMED},
	keywords = {*, Algorithm selection, DONE, Dynamic search heuristics, Mixed-Integer Programming, primal heuristics},
	pages = {943--953},
}

@misc{banitalebi-dehkordi_ml4co_2021,
	title = {{ML4CO}: {Is} {GCNN} {All} {You} {Need}? {Graph} {Convolutional} {Neural} {Networks} {Produce} {Strong} {Baselines} {For} {Combinatorial} {Optimization} {Problems}, {If} {Tuned} and {Trained} {Properly}, on {Appropriate} {Data}},
	shorttitle = {{ML4CO}},
	url = {http://arxiv.org/abs/2112.12251},
	doi = {10.48550/arXiv.2112.12251},
	abstract = {The 2021 NeurIPS Machine Learning for Combinatorial Optimization (ML4CO) competition was designed with the goal of improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning models. The competition's main scientific question was the following: is machine learning a viable option for improving traditional combinatorial optimization solvers on specific problem distributions, when historical data is available? This was motivated by the fact that in many practical scenarios, the data changes only slightly between the repetitions of a combinatorial optimization problem, and this is an area where machine learning models are particularly powerful at. This paper summarizes the solution and lessons learned by the Huawei EI-OROAS team in the dual task of the competition. The submission of our team achieved the second place in the final ranking, with a very close distance to the first spot. In addition, our solution was ranked first consistently for several weekly leaderboard updates before the final evaluation. We provide insights gained from a large number of experiments, and argue that a simple Graph Convolutional Neural Network (GCNNs) can achieve state-of-the-art results if trained and tuned properly.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Banitalebi-Dehkordi, Amin and Zhang, Yong},
	month = dec,
	year = {2021},
	note = {arXiv:2112.12251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Discrete Mathematics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, GNN},
}

@article{lodi_learning_2017,
	title = {On learning and branching: a survey},
	volume = {25},
	issn = {1863-8279},
	shorttitle = {On learning and branching},
	url = {https://doi.org/10.1007/s11750-017-0451-6},
	doi = {10.1007/s11750-017-0451-6},
	abstract = {This paper surveys learning techniques to deal with the two most crucial decisions in the branch-and-bound algorithm for Mixed-Integer Linear Programming, namely variable and node selections. Because of the lack of deep mathematical understanding on those decisions, the classical and vast literature in the field is inherently based on computational studies and heuristic, often problem-specific, strategies. We will both interpret some of those early contributions in the light of modern (machine) learning techniques, and give the details of the recent algorithms that instead explicitly incorporate machine learning paradigms.},
	language = {en},
	number = {2},
	urldate = {2023-04-19},
	journal = {TOP},
	author = {Lodi, Andrea and Zarpellon, Giulia},
	month = jul,
	year = {2017},
	note = {[B]},
	keywords = {68R-02, 68T-02, 90-02, Branch and bound, Machine learning, SURVEY},
	pages = {207--236},
}

@inproceedings{kruber_learning_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {When} to {Use} a {Decomposition}},
	isbn = {978-3-319-59776-8},
	doi = {10.1007/978-3-319-59776-8_16},
	abstract = {Applying a Dantzig-Wolfe decomposition to a mixed-integer program (MIP) aims at exploiting an embedded model structure and can lead to significantly stronger reformulations of the MIP. Recently, automating the process and embedding it in standard MIP solvers have been proposed, with the detection of a decomposable model structure as key element. If the detected structure reflects the (usually unknown) actual structure of the MIP well, the solver may be much faster on the reformulated model than on the original. Otherwise, the solver may completely fail. We propose a supervised learning approach to decide whether or not a reformulation should be applied, and which decomposition to choose when several are possible. Preliminary experiments with a MIP solver equipped with this knowledge show a significant performance improvement on structured instances, with little deterioration on others.},
	language = {en},
	booktitle = {Integration of {AI} and {OR} {Techniques} in {Constraint} {Programming}},
	publisher = {Springer International Publishing},
	author = {Kruber, Markus and Lübbecke, Marco E. and Parmentier, Axel},
	editor = {Salvagnin, Domenico and Lombardi, Michele},
	year = {2017},
	note = {[A]},
	keywords = {Automatic Dantzig-Wolfe decomposition, Branch-and-price, Column generation, DONE, Mixed-integer programming, Supervised learning, primal heuristics},
	pages = {202--210},
}

@phdthesis{gomes_santana_exploring_2022,
	address = {Rio de Janeiro, Brazil},
	type = {{DOUTOR} {EM} {CIÊNCIAS} - {INFORMÁTICA}},
	title = {{EXPLORING} {THE} {FRONTIER} {OF} {COMBINATORIAL} {OPTIMIZATION} {AND} {MACHINE} {LEARNING}: {APPLICATIONS} {TO} {VEHICLE} {ROUTING} {AND} {SUPPORT} {VECTOR} {MACHINES}},
	shorttitle = {{EXPLORING} {THE} {FRONTIER} {OF} {COMBINATORIAL} {OPTIMIZATION} {AND} {MACHINE} {LEARNING}},
	url = {http://www.maxwell.vrac.puc-rio.br/Busca_etds.php?strSecao=resultado&nrSeq=61096@2},
	abstract = {Santana, Ítalo Gomes; Vidal, Thibaut Victor Gaston (Advisor). Exploring the frontier of Combinatorial Optimization and Machine Learning: Applications to Vehicle Routing and Support Vector Machines. Rio de Janeiro, 2022. 93p. Tese de Doutorado – Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.},
	language = {en},
	urldate = {2023-04-28},
	school = {PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO},
	author = {Gomes Santana, Italo},
	month = sep,
	year = {2022},
	doi = {10.17771/PUCRio.acad.61096},
}

@misc{noauthor_euro_nodate,
	title = {{EURO} {Meets} {NeurIPS} 2022 {Vehicle} {Routing} {Competition}},
	url = {https://euro-neurips-vrp-2022.challenges.ortec.com/},
	urldate = {2023-04-28},
}

@misc{bergmeir_comparison_2022,
	title = {Comparison and {Evaluation} of {Methods} for a {Predict}+{Optimize} {Problem} in {Renewable} {Energy}},
	url = {http://arxiv.org/abs/2212.10723},
	doi = {10.48550/arXiv.2212.10723},
	abstract = {Algorithms that involve both forecasting and optimization are at the core of solutions to many difficult real-world problems, such as in supply chains (inventory optimization), traffic, and in the transition towards carbon-free energy generation in battery/load/production scheduling in sustainable energy systems. Typically, in these scenarios we want to solve an optimization problem that depends on unknown future values, which therefore need to be forecast. As both forecasting and optimization are difficult problems in their own right, relatively few research has been done in this area. This paper presents the findings of the ``IEEE-CIS Technical Challenge on Predict+Optimize for Renewable Energy Scheduling," held in 2021. We present a comparison and evaluation of the seven highest-ranked solutions in the competition, to provide researchers with a benchmark problem and to establish the state of the art for this benchmark, with the aim to foster and facilitate research in this area. The competition used data from the Monash Microgrid, as well as weather data and energy market data. It then focused on two main challenges: forecasting renewable energy production and demand, and obtaining an optimal schedule for the activities (lectures) and on-site batteries that lead to the lowest cost of energy. The most accurate forecasts were obtained by gradient-boosted tree and random forest models, and optimization was mostly performed using mixed integer linear and quadratic programming. The winning method predicted different scenarios and optimized over all scenarios jointly using a sample average approximation method.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Bergmeir, Christoph and de Nijs, Frits and Sriramulu, Abishek and Abolghasemi, Mahdi and Bean, Richard and Betts, John and Bui, Quang and Dinh, Nam Trong and Einecke, Nils and Esmaeilbeigi, Rasul and Ferraro, Scott and Galketiya, Priya and Genov, Evgenii and Glasgow, Robert and Godahewa, Rakshitha and Kang, Yanfei and Limmer, Steffen and Magdalena, Luis and Montero-Manso, Pablo and Peralta, Daniel and Kumar, Yogesh Pipada Sunil and Rosales-Pérez, Alejandro and Ruddick, Julian and Stratigakos, Akylas and Stuckey, Peter and Tack, Guido and Triguero, Isaac and Yuan, Rui},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10723 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@phdthesis{sylvestre-decary_neural_2020,
	address = {Canada -- Quebec, CA},
	type = {M.{Appl}.{Sc}.},
	title = {A {Neural} {Network}-{Embedded} {Optimization} {Approach} for {Selecting} {Multiple} {Entries} for {March} {Madness}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2626002651/abstract/E8255EE0092F466CPQ/1},
	abstract = {Sports gambling are expected to grow in popularity in the US as they have been legalized by many states in the last two years. The availability of sports data and the development of new metrics to evaluate the performance of either athletes or teams have allowed the use of statistical approaches to tackle decision-making problems in sports. While most papers in the literature investigate how to predict the outcome of a game, this thesis addresses the development of an optimal strategy to win a sports betting contest. Specifically, we focus on the ESPN Tournament Challenge which is a sport betting contest on the season-ending championship tournaments of American college basketball, also known as the March Madness. The ESPN Tournament Challenge asks participants to pick the winner of each of the 63 games in the March Madness. Thus, there is a total of 263 different ways of filling the tournament which makes the challenge a complex task. Every year, millions of people aim to predict accurately the March Madness. This contest often adopts a top-heavy payoff structure which implies that a single participant needs to beat millions of participant to receive a positive payoff. Kaplan et al. (2001) first introduce an exact approach to the problem by selecting a single-entry that maximizes the expected score. We propose a novel strategy that considers a multi-entry approach to the Tournament Challenge. Such a strategy maximizes the expected score of the maximum scoring entry. We face two main challenge, namely, (1) how to evaluate the objective function and (2) how to optimize it. We then present three approaches for the evaluation of the objective function. This includes an exact approach in a Tree-based algorithm and two approximate models, a simulation approach and a neural network approach. Based on these three different models to evaluate the objective function, we develop both a genetic algorithm and a neural network-embedded algorithm. Finally, we compare the expected score and the empirical score by each approach on each tournament played since 2002. Computational experiments show that the proposed models clearly outperform the single-entry exact approach on every instance.
Alternate abstract:
On peut s’attendre à une croissance en popularité des paris sportifs dans le marché américain suite à la légalisation de ceux-ci dans plusieurs états depuis 2018. De plus, l’augmentation de la quantité de données sur le sport et le développement de nouvelles métriques de performance sportive ont permis depuis quelques années d’avoir une approche statistique pour les problèmes de prise de décision dans le sport. Alors que la littérature sur les paris sportifs couvrent majoritairement des modèles probabilistes pour prédire le résultat d’un évènement, cette thèse s’intéresse plutôt au développement d’une stratégie optimale pour remporter un paris sportif, plus particulièrement le Tournament challenge tenu annuellement par ESPN. Le Tournament Challenge demande aux participants de choisir le gagnant de chacune des 63 parties du March Madness, soit le championnat de fin de saison de basketball collégial américain. Il existe 263 façons de sélectionner les gagnants du tournois. De plus, plusieurs millions de personnes y participent à chaque année. Généralement, seulement un petit pourcentage des meilleurs scores font un gain monétaire ce qui implique qu’un participant doit obtenir un meilleur score que plusieurs millions de personnes pour remporter un gain. Kaplan et al. (2001) ont été les premiers à introduire une approche exacte qui maximise l’espérance de point produit par une entrée. Notre stratégie est la première à considérer plusieurs entrées dépendantes au Tournament Challenge. Notre stratégie cherche à maximiser l’espérance de points produit par le score maximal des k entrées. Deux problèmes découlent de cette stratégie, soit comment évaluer et comment optimiser la fonction objective. Nous présentons trois approches pour évaluer la fonction objective. Cela inclue une méthode exacte qui est un algorithme basé sur un arbre de décision et deux modèles approximatifs, soit une approche par simulation et une approche par apprentissage machine. À partir de ces différents modèles, nous développons deux heuristiques permettant d’optimiser la fonction objective, soit un algorithme génétique et un réseau de neurones intégré à un modèle en nombre entier. Finalement, nous comparons l’espérance de points produits ainsi que le vrai score obtenu par chacune des méthodes pour chaque tournois depuis 2002. Nos deux modèles surpassent pour chaque instance la solution optimal du modèle exacte avec une entrée.},
	language = {English},
	urldate = {2023-04-28},
	school = {Ecole Polytechnique, Montreal (Canada)},
	author = {Sylvestre-Décary, Jeff},
	year = {2020},
	note = {ISBN: 9798759977698},
	keywords = {Applied mathematics, Basketball, Computer science, Expected values, Integer programming, Legalized gambling, Machine learning, Neural networks, Probability, Statistics, Teams, Tournaments \& championships},
}

@phdthesis{gonzalez_jurado_machine_2020,
	address = {Canada -- Quebec, CA},
	type = {Ph.{D}.},
	title = {Machine {Learning}-{Driven} {Hybrid} {Optimization} {Based} on {Decision} {Diagrams}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2626891456/abstract/2CC07595476D41B1PQ/1},
	abstract = {The discrete and finite nature of combinatorial optimization problems arises in many areas of mathematics and computer science as well as in applications such as scheduling and planning. Despite decades of development and remarkable speedups in general-purpose solvers, some combinatorial problems are still difficult to be solved. The design of generic optimization solvers to tackle such challenging problems is a continuous and active research area. In this dissertation, we propose novel hybrid optimization mechanisms that exploit complementary strengths from different paradigms. The integrated mechanisms leverage (i) the decision diagram-based optimization (DDO) solving approach, (ii) a more mature technology such as mixed-integer programming (MIP), and, finally, (iii) the use of machine learning (ML) to enhance optimization methods.
Alternate abstract:
Les problèmes d’optimisation combinatoire se posent dans de nombreux domaines des mathématiques et de l’informatique, ainsi que dans des applications telles que l’ordonnancement et la planification. Malgré des décennies de développement des différentes technologies d’optimisation, certains problèmes combinatoires restent encore difficiles à résoudre. Le développement d’outils d’optimisation génériques pour résoudre ces problèmes difficiles est donc un domaine de recherche actif et continu. Dans cette thèse, nous proposons de nouveaux mécanismes d’optimisation hybrides qui exploitent les avantages complémentaires de différents paradigmes, à savoir, (i) l’optimisation basée sur les diagrammes de décision (ODD), (ii) la programmation en nombres entiers (PNE), et (iii) l’apprentissage automatique (AA) pour améliorer les méthodes d’optimisation.},
	language = {English},
	urldate = {2023-04-28},
	school = {Ecole Polytechnique, Montreal (Canada)},
	author = {Gonzalez Jurado, Jaime Esteban},
	year = {2020},
	note = {ISBN: 9798759978978},
	keywords = {Algorithms, Applied mathematics, Artificial intelligence, Automation, Computer science, Decision making, Decision trees, Dynamic programming, Integer programming, Linear programming, Machine learning, Mathematical programming, Semidefinite programming, Traveling salesman problem},
}

@article{bongiovanni_machine_2022,
	title = {A machine learning-driven two-phase metaheuristic for autonomous ridesharing operations},
	volume = {165},
	issn = {1366-5545},
	url = {https://www.sciencedirect.com/science/article/pii/S1366554522002198},
	doi = {10.1016/j.tre.2022.102835},
	abstract = {This paper contributes to the intersection of operations research and machine learning in the context of autonomous ridesharing. In this work, autonomous ridesharing operations are reproduced through an event-based simulation approach and are modeled as a sequence of static subproblems to be optimized. The optimization framework consists of a novel data-driven metaheuristic within a two phase approach. The first phase consists of a greedy insertion heuristic that assigns new online requests to vehicles. The second phase consists of a local-search based metaheuristic that iteratively revisits previously-made vehicle-trip assignments through intra- and inter-vehicle route exchanges. These exchanges are performed by selecting from a pool of destroy–repair operators using a machine learning approach that is trained offline on a large dataset composed of more than one and a half million examples of previously-solved autonomous ridesharing subproblems. Computational results are performed on multiple dynamic instances extracted from real ridesharing data published by Uber Technologies Inc. Results show that the proposed machine learning-based optimization approach outperforms benchmark state-of-the-art data-driven metaheuristics by up to about nine percent, on average. Managerial insights highlight the correlation between selected vehicle routing features and the performance of the metaheuristics in the context of autonomous ridesharing operations.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Transportation Research Part E: Logistics and Transportation Review},
	author = {Bongiovanni, Claudia and Kaspi, Mor and Cordeau, Jean-François and Geroliminis, Nikolas},
	month = sep,
	year = {2022},
	keywords = {Dial-a-ride problem, Electric autonomous vehicles, Large neighborhood search, Machine learning, Metaheuristics, Online optimization},
	pages = {102835},
}

@misc{kannan_learning_2023,
	title = {Learning to {Accelerate} the {Global} {Optimization} of {Quadratically}-{Constrained} {Quadratic} {Programs}},
	url = {http://arxiv.org/abs/2301.00306},
	doi = {10.48550/arXiv.2301.00306},
	abstract = {We learn optimal instance-specific heuristics for the global minimization of nonconvex quadratically-constrained quadratic programs (QCQPs). Specifically, we consider partitioning-based mixed-integer programming relaxations for nonconvex QCQPs and propose the novel problem of \${\textbackslash}textit\{strong partitioning\}\$ to optimally partition variable domains \${\textbackslash}textit\{without\}\$ sacrificing global optimality. We design a local optimization method for solving this challenging max-min strong partitioning problem and replace this expensive benchmark strategy with a machine learning (ML) approximation for homogeneous families of QCQPs. We present a detailed computational study on randomly generated families of QCQPs, including instances of the pooling problem, using the open-source global solver Alpine. Our numerical experiments demonstrate that strong partitioning and its ML approximation significantly reduce Alpine's solution time by factors of \$3.5 - 16.5\$ and \$2 - 4.5\$ \${\textbackslash}textit\{on average\}\$ and by maximum factors of \$15 - 700\$ and \$10 - 200\$, respectively, over the different QCQP families.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Kannan, Rohit and Nagarajan, Harsha and Deka, Deepjyoti},
	month = feb,
	year = {2023},
	note = {arXiv:2301.00306 [math]},
	keywords = {90C26, 90C31, 65K05, Mathematics - Optimization and Control},
}

@phdthesis{zarpellon_machine_2020,
	address = {Canada -- Quebec, CA},
	type = {Ph.{D}.},
	title = {Machine {Learning} {Algorithms} in {Mixed}-{Integer} {Programming}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2626025254/abstract/C7AB98C4D5F64E0APQ/1},
	abstract = {A variety of real-world tasks involve decisions of discrete nature, and can be mathematically modeled as Mixed-Integer Programming (MIP) optimization problems. Daily deployed in multiple application domains, MIP formulations are nowadays solved effectively and reliably, thanks to the complex and rich algorithmic frameworks developed in modern MIP solvers. With machine learning (ML) being extensively leveraged to “learn from examples”, new attention has been given to the application of learned predictions in optimization settings, especially in the context of MIP solvers’ algorithmic design. The present thesis contributes to this thread of research: we discuss and propose novel methods on the theme of using ML algorithms alongside MIP ones, and explore some opportunities of this fruitful interaction.
First, to document ML attempts addressing the decisions of variable and node selection in Branch and Bound (B\&B), we present a survey on learning and branching. By interpreting previous MIP literature contributions as forerunners of ML-based works, and discussing the assumptions and concerns underlying these critical heuristic decisions, we provide an original canvas to analyze recent learned approaches and outline new points of view. Second, we develop a classification framework to tackle the algorithmic question of whether to linearize convex quadratic MIPs, aiming at a tight integration of the optimization knowledge in the learning pipeline. As experiments practically led to the deployment of a classifier in the IBM-CPLEX optimization solver, the work more generally outlines a reference methodology for the combination of ML and MIP technologies.
Alternate abstract:
De nombreux problèmes de la vie courante comportent des décisions discrètes, et peuvent être modélisés sous la forme de programmes d’optimisation en nombres entiers. De tels modèles peuvent désormais être résolus efficacement à l’aide de solveurs matures comportant un vaste arsenal algorithmique, ce qui explique l’utilisation quotidienne de la programmation mathématique mixte en nombres entiers (PNE) dans de multiples secteurs. Alors que les techniques d’apprentissage automatiques (à partir d’exemples) sont de plus en plus mises en œuvre pour l’analyse de données et la predicion, une attention nouvelle est donnée à l’apprentissage dans le contexte d’algorithmes d’optimisation, notamment dans les choix algorithmiques des solveurs de PNE. Cette thèse s’inscrit dans ce courant de recherche : nous analysons et proposons de nouvelles méthodes pour intégrer des techniques d’apprentissage automatique au sein d’algorithmes d’optimisation, et explorons le potentiel de cette interaction. Premièrement, nous passons en revue l’état de l’art quant à l’utilisation de techniques d’apprentissage automatique pour la sélection de variables et de nœuds dans un arbre de branchement. Plusieurs travaux de PNE sont identifiés comme précurseurs d’approches basées sur l’apprentissage automatique. En discutant les hypothèses et questions sousjacentes de ces décisions, nous proposons un nouveau cadre pour analyser les approches récentes d’apprentissage, et soulignons des nouvelles perspectives quant à l’utilisation de l’apprentissage pour guider le branchement.},
	language = {English},
	urldate = {2023-04-28},
	school = {Ecole Polytechnique, Montreal (Canada)},
	author = {Zarpellon, Giulia},
	year = {2020},
	note = {ISBN: 9798759977353},
	keywords = {Algorithms, Applied mathematics, Artificial intelligence, Computer science, Datasets, Decision making, Distance learning, Feature selection, Integer programming, Linear programming, Machine learning, Neural networks, Optimization},
}

@misc{noauthor_fast_nodate,
	title = {‪{Fast} and {Robust} {Resource}-{Constrained} {Scheduling} with {Graph} {Neural} {Networks}‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=avOLwlYAAAAJ&sortby=pubdate&citation_for_view=avOLwlYAAAAJ:4OULZ7Gr8RgC},
	abstract = {‪F Teichteil-Königsbuch, G Povéda, GG de Garibay Barba, T Luchterhand, S Thiébaux, 2023‬},
	urldate = {2023-04-27},
}

@article{schweitzer_choosing_2023,
	title = {Choosing {Solution} {Strategies} for {Scheduling} {Automated} {Guided} {Vehicles} in {Production} {Using} {Machine} {Learning}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/2/806},
	doi = {10.3390/app13020806},
	abstract = {Artificial intelligence is considered to be a significant technology for driving the future evolution of smart manufacturing environments. At the same time, automated guided vehicles (AGVs) play an essential role in manufacturing systems due to their potential to improve internal logistics by increasing production flexibility. Thereby, the productivity of the entire system relies on the quality of the schedule, which can achieve production cost savings by minimizing delays and the total makespan. However, traditional scheduling algorithms often have difficulties in adapting to changing environment conditions, and the performance of a selected algorithm depends on the individual scheduling problem. Therefore, this paper aimed to analyze the scheduling problem classes of AGVs by applying design science research to develop an algorithm selection approach. The designed artifact addressed a catalogue of characteristics that used several machine learning algorithms to find the optimal solution strategy for the intended scheduling problem. The contribution of this paper is the creation of an algorithm selection method that automatically selects a scheduling algorithm, depending on the problem class and the algorithm space. In this way, production efficiency can be increased by dynamically adapting the AGV schedules. A computational study with benchmark literature instances unveiled the successful implementation of constraint programming solvers for solving JSSP and FJSSP scheduling problems and machine learning algorithms for predicting the most promising solver. The performance of the solvers strongly depended on the given problem class and the problem instance. Consequently, the overall production performance increased by selecting the algorithms per instance. A field experiment in the learning factory at Reutlingen University enabled the validation of the approach within a running production scenario.},
	language = {en},
	number = {2},
	urldate = {2023-04-27},
	journal = {Applied Sciences},
	author = {Schweitzer, Felicia and Bitsch, Günter and Louw, Louis},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AGV scheduling, algorithm selection, constraint programming, machine learning, optimization},
	pages = {806},
}

@misc{ferber_surco_2022,
	title = {{SurCo}: {Learning} {Linear} {Surrogates} {For} {Combinatorial} {Nonlinear} {Optimization} {Problems}},
	shorttitle = {{SurCo}},
	url = {http://arxiv.org/abs/2210.12547},
	doi = {10.48550/arXiv.2210.12547},
	abstract = {Optimization problems with expensive nonlinear cost functions and combinatorial constraints appear in many real-world applications, but remain challenging to solve efficiently. Existing combinatorial solvers like Mixed Integer Linear Programming can be fast in practice but cannot readily optimize nonlinear cost functions, while general nonlinear optimizers like gradient descent often do not handle complex combinatorial structures, may require many queries of the cost function, and are prone to local optima. To bridge this gap, we propose SurCo that learns linear Surrogate costs which can be used by existing Combinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We learn these linear surrogates end-to-end with the nonlinear loss by differentiating through the linear surrogate solver. Three variants of SurCo are proposed: SurCo-zero operates on individual nonlinear problems, SurCo-prior trains a linear surrogate predictor on distributions of problems, and SurCo-hybrid uses a model trained offline to warm start online solving for SurCo-zero. We analyze our method theoretically and empirically, showing smooth convergence and improved performance. Experiments show that compared to state-of-the-art approaches and expert-designed heuristics, SurCo obtains lower cost solutions with comparable or faster solve time for two realworld industry-level applications: embedding table sharding and inverse photonic design.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Ferber, Aaron and Huang, Taoan and Zha, Daochen and Schubert, Martin and Steiner, Benoit and Dilkina, Bistra and Tian, Yuandong},
	month = oct,
	year = {2022},
	note = {arXiv:2210.12547 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{sun_enhancing_2023,
	title = {Enhancing {Constraint} {Programming} via {Supervised} {Learning} for {Job} {Shop} {Scheduling}},
	url = {http://arxiv.org/abs/2211.14492},
	doi = {10.48550/arXiv.2211.14492},
	abstract = {Constraint programming (CP) is a powerful technique for solving constraint satisfaction and optimization problems. In CP solvers, the variable ordering strategy used to select which variable to explore first in the solving process has a significant impact on solver effectiveness. To address this issue, we propose a novel variable ordering strategy based on supervised learning, which we evaluate in the context of job shop scheduling problems. Our learning-based methods predict the optimal solution of a problem instance and use the predicted solution to order variables for CP solvers. {\textbackslash}added[]\{Unlike traditional variable ordering methods, our methods can learn from the characteristics of each problem instance and customize the variable ordering strategy accordingly, leading to improved solver performance.\} Our experiments demonstrate that training machine learning models is highly efficient and can achieve high accuracy. Furthermore, our learned variable ordering methods perform competitively when compared to four existing methods. Finally, we demonstrate that hybridising the machine learning-based variable ordering methods with traditional domain-based methods is beneficial.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Sun, Yuan and Nguyen, Su and Thiruvady, Dhananjay and Li, Xiaodong and Ernst, Andreas T. and Aickelin, Uwe},
	month = apr,
	year = {2023},
	note = {arXiv:2211.14492 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{wang_acceleration_2023,
	title = {Acceleration {Framework} and {Solution} {Algorithm} for {Distribution} {System} {Restoration} based on {End}-to-{End} {Optimization} {Strategy}},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2023.3262189},
	abstract = {The distribution system restoration (DSR) problem is traditionally modeled as a mixed-integer linear programming (MILP) model. However, a significant number of integer variables are introduced to describe the DSR process, which introduce additional complexities in both time and space dimensions. Moreover, an enormous number of constraints are constructed to establish a rational DSR decision, while some of them may not be considered tight in practice. The enormous number of binary variables and inactive constraints could make the DSR problem very hard to solve and apply in real-time. The DSR computation burden would be reduced significantly if binary variables and binding constraints are pre-determined. This paper proposes an acceleration framework and solution algorithm based on the end-to-end optimization, which applies deep neural network (DNN) and gradient boosting decision tree (GBDT) methods to DSR. The DSR problem, which is solved in offline and online stages, will accordingly be reduced to a linear programming problem which can be solved more efficiently and reliably. Case studies are carried out on the modified IEEE 33-bus and 123-bus systems and a practical 1069-bus system. The proposed results indicate that the DSR problem with the proposed end-to-end acceleration framework is solved more than tenfold faster than those of traditional solvers.},
	journal = {IEEE Transactions on Power Systems},
	author = {Wang, Yifei and Yan, Ziheng and Sang, Linwei and Hong, Lucheng and Hu, Qinran and Shahidehpour, Mohammad and Xu, Qingshan},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Power Systems},
	keywords = {Computational modeling, Decision trees, Distribution system restoration, Generators, Indexes, Network topology, Optimization, Voltage, acceleration algorithm, deep neutral network, gradient boosting decision tree},
	pages = {1--13},
}

@article{peng_heavy-head_2023,
	title = {Heavy-{Head} {Sampling} for {Fast} {Imitation} {Learning} of {Machine} {Learning} {Based} {Combinatorial} {Auction} {Solver}},
	volume = {55},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-022-10900-y},
	doi = {10.1007/s11063-022-10900-y},
	abstract = {The winner determination problem of a combinatorial auction can be modeled as mixed-integer linear programming, and is a popular benchmark to evaluate modern solvers. Recent advancements in combinatorial optimization improve the branch-and-bound solving process by replacing the time-consuming heuristics with machine learning models. In this paper, by taking advantage of the heavy-head maximum depth distribution of the branch-and-bound solution trees, a heavy-head sampling strategy is proposed for the imitation learning on the combinatorial auction problems. Experimental results show that, under the small-dataset fast-training scheme and using the heavy-head sampling strategy, the final evaluation results of the trained policy on the combinatorial auction problems are improved significantly (in the sense of statistical testing), compared to using the uniform sampling strategy in previous studies.},
	language = {en},
	number = {1},
	urldate = {2023-04-27},
	journal = {Neural Processing Letters},
	author = {Peng, Chen and Liao, Bolin},
	month = feb,
	year = {2023},
	keywords = {Combinatorial auction, Combinatorial optimization, Imitation learning, Neural network},
	pages = {631--644},
}

@article{baltean-lugojan_selecting_2018,
	title = {Selecting cutting planes for quadratic semidefinite outer-approximation via trained neural networks},
	journal = {URL: http://www. optimization-online. org/DB\_HTML/2018/11/6943. html},
	author = {Baltean-Lugojan, Radu and Bonami, Pierre and Misener, Ruth and Tramontani, Andrea},
	year = {2018},
	note = {[A]},
}
