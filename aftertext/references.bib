@article{bengioMachineLearningCombinatorial2021,
  title = {Machine Learning for Combinatorial Optimization: {{A}} Methodological Tour d'horizon},
  shorttitle = {Machine Learning for Combinatorial Optimization},
  author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
  year = {2021},
  month = apr,
  journal = {European Journal of Operational Research},
  volume = {290},
  number = {2},
  pages = {405--421},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.07.063},
  urldate = {2023-04-19},
  abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
  langid = {english},
  keywords = {*,background,Branch and bound,Combinatorial optimization,DONE,Machine learning,Mixed-integer programming solvers,SURVEY},
  annotation = {[A] DONE},
  file = {/home/bruno/Zotero/storage/CAVDYZCL/Bengio et al. - 2021 - Machine learning for combinatorial optimization A.pdf;/home/bruno/Zotero/storage/DCDLVZKT/S0377221720306895.html}
}

@article{bestuzhevaSCIPOptimizationSuite2021,
  title = {The {{SCIP Optimization Suite}} 8.0},
  author = {Bestuzheva, Ksenia and Besan{\c c}on, Mathieu and Chen, Wei-Kun and Chmiela, Antonia and Donkiewicz, Tim and {van Doornmalen}, Jasper and Eifler, Leon and Gaul, Oliver and Gamrath, Gerald and Gleixner, Ambros and Gottwald, Leona and Graczyk, Christoph and Halbig, Katrin and Hoen, Alexander and Hojny, Christopher and {van der Hulst}, Rolf and Koch, Thorsten and L{\"u}bbecke, Marco and Maher, Stephen J. and Matter, Frederic and M{\"u}hmer, Erik and M{\"u}ller, Benjamin and Pfetsch, Marc E. and Rehfeldt, Daniel and Schlein, Steffan and Schl{\"o}sser, Franziska and Serrano, Felipe and Shinano, Yuji and Sofranac, Boro and Turner, Mark and Vigerske, Stefan and Wegscheider, Fabian and Wellner, Philipp and Weninger, Dieter and Witzig, Jakob},
  year = {2021},
  month = dec,
  issn = {1438-0064},
  urldate = {2024-05-20},
  abstract = {The SCIP Optimization Suite provides a collection of software packages for mathematical optimization centered around the constraint integer programming framework SCIP. This paper discusses enhancements and extensions contained in version 8.0 of the SCIP Optimization Suite. Major updates in SCIP include improvements in symmetry handling and decomposition algorithms, new cutting planes, a new plugin type for cut selection, and a complete rework of the way nonlinear constraints are handled. Additionally, SCIP 8.0 now supports interfaces for Julia as well as Matlab. Further, UG now includes a unified framework to parallelize all solvers, a utility to analyze computational experiments has been added to GCG, dual solutions can be postsolved by PaPILO, new heuristics and presolving methods were added to SCIP-SDP, and additional problem classes and major performance improvements are available in SCIP-Jack.},
  langid = {english},
  file = {/home/bruno/Zotero/storage/48JSB8AB/Bestuzheva et al. - 2021 - The SCIP Optimization Suite 8.0.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification {{And Regression Trees}}},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {2017},
  month = oct,
  edition = {1},
  publisher = {Routledge},
  doi = {10.1201/9781315139470},
  urldate = {2024-04-18},
  isbn = {978-1-315-13947-0},
  langid = {english}
}

@book{breimanClassificationRegressionTrees2017a,
  title = {Classification {{And Regression Trees}}},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {2017},
  month = oct,
  edition = {1},
  publisher = {Routledge},
  doi = {10.1201/9781315139470},
  urldate = {2024-05-16},
  isbn = {978-1-315-13947-0},
  langid = {english}
}

@article{camponogaraContinuoustimeFormulationOptimal2022,
  title = {A Continuous-Time Formulation for Optimal Task Scheduling and Quality-of-Service Assurance in Nanosatellites},
  author = {Camponogara, Eduardo and Seman, Laio Oriel and Rigo, Cezar Ant{\^o}nio and Morsch Filho, Edemar and Ribeiro, Brenda Fernandes and Bezerra, Eduardo Augusto},
  year = {2022},
  month = nov,
  journal = {Computers \& Operations Research},
  volume = {147},
  pages = {105945},
  issn = {03050548},
  doi = {10.1016/j.cor.2022.105945},
  urldate = {2024-05-21},
  langid = {english}
}

@article{camponogaraModelsAlgorithmsOptimal2015,
  title = {Models and {{Algorithms}} for {{Optimal Piecewise-Linear Function Approximation}}},
  author = {Camponogara, Eduardo and Nazari, Luiz Fernando},
  year = {2015},
  journal = {Mathematical Problems in Engineering},
  volume = {2015},
  pages = {1--9},
  issn = {1024-123X, 1563-5147},
  doi = {10.1155/2015/876862},
  urldate = {2024-03-25},
  abstract = {Piecewise-linear functions can approximate nonlinear and unknown functions for which only sample points are available. This paper presents a range of piecewise-linear models and algorithms to aid engineers to find an approximation that fits best their applications. The models include piecewise-linear functions with a fixed and maximum number of linear segments, lower and upper envelopes, strategies to ensure continuity, and a generalization of these models for stochastic functions whose data points are random variables. Derived from recursive formulations, the algorithms are applied to the approximation of the production function of gas-lifted oil wells.},
  langid = {english},
  file = {/home/bruno/Zotero/storage/XFGJL4JA/Camponogara and Nazari - 2015 - Models and Algorithms for Optimal Piecewise-Linear.pdf}
}

@misc{cappartCombinatorialOptimizationReasoning2022,
  title = {Combinatorial Optimization and Reasoning with Graph Neural Networks},
  author = {Cappart, Quentin and Ch{\'e}telat, Didier and Khalil, Elias and Lodi, Andrea and Morris, Christopher and Veli{\v c}kovi{\'c}, Petar},
  year = {2022},
  month = sep,
  number = {arXiv:2102.09544},
  eprint = {2102.09544},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09544},
  urldate = {2023-05-17},
  abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks (GNNs), as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. The inductive bias of GNNs effectively encodes combinatorial and relational input due to their invariance to permutations and awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization and machine learning researchers.},
  archiveprefix = {arxiv},
  keywords = {*,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,GNN,Mathematics - Optimization and Control,Optimization,Statistics - Machine Learning,SURVEY},
  file = {/home/bruno/Zotero/storage/EKEZT8KR/Cappart et al. - 2022 - Combinatorial optimization and reasoning with grap.pdf;/home/bruno/Zotero/storage/4NIR2HK6/2102.html}
}

@article{cauchy1847,
  title = {Methode Generale Pour La Resolution Des Systemes d'equations Simultanees},
  author = {Cauchy, A.},
  year = {1847},
  journal = {C.R. Acad. Sci. Paris},
  volume = {25},
  pages = {536--538}
}

@phdthesis{chamonConstrainedLearningInference2020,
  title = {Constrained {{Learning And Inference}}},
  author = {Chamon, De Oliveira and Fernando, Luiz},
  year = {2020},
  month = jan,
  number = {3818},
  urldate = {2024-05-20},
  abstract = {Data and learning have become core components of the information processing and autonomous systems upon which we increasingly rely on to select job applicants, analyze medical data, and drive cars. As these systems become ubiquitous, so does the need to curtail their behavior. Left untethered, they are susceptible to tampering (adversarial examples) and prone to prejudiced and unsafe actions. Currently, the response of these systems is tailored by leveraging domain expert knowledge to either construct models that embed the desired properties or tune the training objective so as to promote them. While effective, these solutions are often targeted to specific behaviors, contexts, and sometimes even problem instances and are typically not transferable across models and applications. What is more, the growing scale and complexity of modern information processing and autonomous systems renders this manual behavior tuning infeasible. Already today, explainability, interpretability, and transparency combined with human judgment are no longer enough to design systems that perform according to specifications. The present thesis addresses these issues by leveraging constrained statistical optimization. More specifically, it develops the theoretical underpinnings of constrained learning and constrained inference to provide tools that enable solving statistical problems under requirements. Starting with the task of learning under requirements, it develops a generalization theory of constrained learning akin to the existing unconstrained one. By formalizing the concept of probability approximately correct constrained (PACC) learning, it shows that constrained learning is as hard as its unconstrained learning and establishes the constrained counterpart of empirical risk minimization (ERM) as a PACC learner. To overcome challenges involved in solving such non-convex constrained optimization problems, it derives a dual learning rule that enables constrained learning tasks to be tackled by through unconstrained learning problems only. It therefore concludes that if we can deal with classical, unconstrained learning tasks, then we can deal with learning tasks with requirements. The second part of this thesis addresses the issue of constrained inference. In particular, the issue of performing inference using sparse nonlinear function models, combinatorial constrained with quadratic objectives, and risk constraints. Such models arise in nonlinear line spectrum estimation, functional data analysis, sensor selection, actuator scheduling, experimental design, and risk-aware estimation. Although inference problems assume that models and distributions are known, each of these constraints pose serious challenges that hinder their use in practice. Sparse nonlinear functional models lead to infinite dimensional, non-convex optimization programs that cannot be discretized without leading to combinatorial, often NP-hard, problems. Rather than using surrogates and relaxations, this work relies on duality to show that despite their apparent complexity, these models can be fit efficiently, i.e., in polynomial time. While quadratic objectives are typically tractable (often even in closed form), they lead to non-submodular optimization problems when subject to cardinality or matroid constraints. While submodular functions are sometimes used as surrogates, this work instead shows that quadratic functions are close to submodular and can also be optimized near-optimally. The last chapter of this thesis is dedicated to problems involving risk constraints, in particular, bounded predictive mean square error variance estimation. Despite being non-convex, such problems are equivalent to a quadratically constrained quadratic program from which a closed-form estimator can be extracted. These results are used throughout this thesis to tackle problems in signal processing, machine learning, and control, such as fair learning, robust learning, nonlinear line spectrum estimation, actuator scheduling, experimental design, and risk-aware estimation. Yet, they are applicable much beyond these illustrations to perform safe reinforcement learning, sensor selection, multiresolution kernel estimation, and wireless resource allocation, to name a few.},
  langid = {english},
  school = {University of Pennsylvania}
}

@article{chamonConstrainedLearningNonConvex2023,
  title = {Constrained {{Learning With Non-Convex Losses}}},
  author = {Chamon, Luiz F. O. and Paternain, Santiago and {Calvo-Fullana}, Miguel and Ribeiro, Alejandro},
  year = {2023},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {69},
  number = {3},
  pages = {1739--1760},
  issn = {1557-9654},
  doi = {10.1109/TIT.2022.3187948},
  urldate = {2024-05-20},
  abstract = {Though learning has become a core component of modern information processing, there is now ample evidence that it can lead to biased, unsafe, and prejudiced systems. The need to impose requirements on learning is therefore paramount, especially as it reaches critical applications in social, industrial, and medical domains. However, the non-convexity of most modern statistical problems is only exacerbated by the introduction of constraints. Whereas good unconstrained solutions can often be learned using empirical risk minimization, even obtaining a model that satisfies statistical constraints can be challenging. All the more so, a good one. In this paper, we overcome this issue by learning in the empirical dual domain, where constrained statistical learning problems become unconstrained and deterministic. We analyze the generalization properties of this approach by bounding the empirical duality gap---i.e., the difference between our approximate, tractable solution and the solution of the original (non-convex) statistical problem---and provide a practical constrained learning algorithm. These results establish a constrained counterpart to classical learning theory, enabling the explicit use of constraints in learning. We illustrate this theory and algorithm in rate-constrained learning applications arising in fairness and adversarial robustness.},
  keywords = {Approximation algorithms,Complexity theory,Constrained learning,learning theory,machine learning,optimization,Optimization,Picture archiving and communication systems,Robustness,Statistical learning,Stochastic processes},
  file = {/home/bruno/Zotero/storage/MLYHS5KZ/Chamon et al. - 2023 - Constrained Learning With Non-Convex Losses.pdf;/home/bruno/Zotero/storage/8GF46GVK/9813433.html}
}

@misc{chenRepresentingMixedIntegerLinear2022,
  title = {On {{Representing Mixed-Integer Linear Programs}} by {{Graph Neural Networks}}},
  author = {Chen, Ziang and Liu, Jialin and Wang, Xinshang and Lu, Jianfeng and Yin, Wotao},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10759},
  eprint = {2210.10759},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.10759},
  urldate = {2023-04-27},
  abstract = {While Mixed-integer linear programming (MILP) is NP-hard in general, practical MILP has received roughly 100--fold speedup in the past twenty years. Still, many classes of MILPs quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for MILPs. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks (GNNs) to making decisions in various stages of MILP solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible MILPs that all GNNs will, however, treat equally, indicating GNN's lacking power to express general MILPs. Then, we show that, by restricting the MILPs to unfoldable ones or by adding random features, there exist GNNs that can reliably predict MILP feasibility, optimal objective values, and optimal solutions up to prescribed precision. We conducted small-scale numerical experiments to validate our theoretical findings.},
  archiveprefix = {arxiv},
  keywords = {*,Computer Science - Machine Learning,end-to-end,GNN,imitation learning,Mathematics - Optimization and Control},
  file = {/home/bruno/Zotero/storage/GR8JMPDF/Chen et al. - 2022 - On Representing Mixed-Integer Linear Programs by G.pdf;/home/bruno/Zotero/storage/6TXX9PND/2210.html}
}

@article{daigavaneUnderstandingConvolutionsGraphs2021,
  title = {Understanding {{Convolutions}} on {{Graphs}}},
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  year = {2021},
  month = aug,
  journal = {Distill},
  volume = {6},
  number = {8},
  pages = {10.23915/distill.00032},
  issn = {2476-0757},
  doi = {10.23915/distill.00032},
  urldate = {2024-05-02}
}

@article{dantzigSignificanceSolvingLinear1960,
  title = {On the {{Significance}} of {{Solving Linear Programming Problems}} with {{Some Integer Variables}}},
  author = {Dantzig, George B.},
  year = {1960},
  month = jan,
  journal = {Econometrica},
  volume = {28},
  number = {1},
  eprint = {1905292},
  eprinttype = {jstor},
  pages = {30},
  issn = {00129682},
  doi = {10.2307/1905292},
  urldate = {2024-04-09}
}

@article{dingAcceleratingPrimalSolution2020,
  title = {Accelerating {{Primal Solution Findings}} for {{Mixed Integer Programs Based}} on {{Solution Prediction}}},
  author = {Ding, Jian-Ya and Zhang, Chao and Shen, Lei and Li, Shengyin and Wang, Bing and Xu, Yinghui and Song, Le},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {02},
  pages = {1452--1459},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i02.5503},
  urldate = {2023-05-17},
  abstract = {Mixed Integer Programming (MIP) is one of the most widely used modeling techniques for combinatorial optimization problems. In many applications, a similar MIP model is solved on a regular basis, maintaining remarkable similarities in model structures and solution appearances but differing in formulation coefficients. This offers the opportunity for machine learning methods to explore the correlations between model structures and the resulting solution values. To address this issue, we propose to represent a MIP instance using a tripartite graph, based on which a Graph Convolutional Network (GCN) is constructed to predict solution values for binary variables. The predicted solutions are used to generate a local branching type cut which can be either treated as a global (invalid) inequality in the formulation resulting in a heuristic approach to solve the MIP, or as a root branching rule resulting in an exact approach. Computational evaluations on 8 distinct types of MIP problems show that the proposed framework improves the primal solution finding performance significantly on a state-of-the-art open-source MIP solver.},
  keywords = {*,end-to-end,GNN,imitation learning},
  file = {/home/bruno/Zotero/storage/4V5ZTLET/Ding et al. - 2020 - Accelerating Primal Solution Findings for Mixed In.pdf}
}

@inproceedings{fiorettoLagrangianDualityConstrained2021,
  title = {Lagrangian {{Duality}} for {{Constrained Deep Learning}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Applied Data Science}} and {{Demo Track}}},
  author = {Fioretto, Ferdinando and Van Hentenryck, Pascal and Mak, Terrence W. K. and Tran, Cuong and Baldo, Federico and Lombardi, Michele},
  editor = {Dong, Yuxiao and Ifrim, Georgiana and Mladeni{\'c}, Dunja and Saunders, Craig and Van Hoecke, Sofie},
  year = {2021},
  pages = {118--135},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-67670-4_8},
  abstract = {This paper explores the potential of Lagrangian duality for learning applications that feature complex constraints. Such constraints arise in many science and engineering domains, where the task amounts to learning to predict solutions for constraint optimization problems which must be solved repeatedly and include hard physical and operational constraints. The paper also considers applications where the learning task must enforce constraints on the predictor itself, either because they are natural properties of the function to learn or because it is desirable from a societal standpoint to impose them.},
  isbn = {978-3-030-67670-4},
  langid = {english},
  file = {/home/bruno/Zotero/storage/7PV2HWEN/Fioretto et al. - 2021 - Lagrangian Duality for Constrained Deep Learning.pdf}
}

@inbook{fischettiHeuristicsMixedInteger2011,
  title = {Heuristics in {{Mixed Integer Programming}}},
  booktitle = {Wiley {{Encyclopedia}} of {{Operations Research}} and {{Management Science}}},
  author = {Fischetti, Matteo and Lodi, Andrea},
  year = {2011},
  month = jan,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9780470400531.eorms0376},
  urldate = {2024-04-11},
  abstract = {Abstract             MILP heuristics aim at finding a feasible (and hopefully good) solution of the problem above, which is an NP-hard problem by itself. We present the main ideas underlying some of the heuristics proposed in the literature. In particular, in this article we focus on those algorithms developed with the aim of being tightly integrated within MILP solvers.},
  collaborator = {Cochran, James J. and Cox, Louis A. and Keskinocak, Pinar and Kharoufeh, Jeffrey P. and Smith, J. Cole},
  isbn = {978-0-470-40063-0 978-0-470-40053-1},
  langid = {english},
  file = {/home/bruno/Zotero/storage/C67UQP4X/Fischetti and Lodi - 2011 - Heuristics in Mixed Integer Programming.pdf}
}

@incollection{fischettiMatheuristics2016,
  title = {Matheuristics},
  booktitle = {Handbook of {{Heuristics}}},
  author = {Fischetti, Martina and Fischetti, Matteo},
  editor = {Mart{\'i}, Rafael and Panos, Pardalos and Resende, Mauricio G.C.},
  year = {2016},
  pages = {1--33},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-07153-4_14-1},
  urldate = {2024-04-17},
  isbn = {978-3-319-07153-4},
  langid = {english}
}

@book{gareyComputersIntractabilityGuide1979,
  title = {Computers and Intractability: A Guide to the Theory of {{NP-completeness}}},
  shorttitle = {Computers and Intractability},
  author = {Garey, Michael R. and Johnson, David S.},
  year = {1979},
  series = {A Series of Books in the Mathematical Sciences},
  publisher = {W. H. Freeman},
  address = {New York},
  isbn = {978-0-7167-1045-5},
  langid = {english},
  lccn = {004.015 1}
}

@misc{gasseExactCombinatorialOptimization2019,
  title = {Exact {{Combinatorial Optimization}} with {{Graph Convolutional Neural Networks}}},
  author = {Gasse, Maxime and Ch{\'e}telat, Didier and Ferroni, Nicola and Charlin, Laurent and Lodi, Andrea},
  year = {2019},
  month = oct,
  number = {arXiv:1906.01629},
  eprint = {1906.01629},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.01629},
  urldate = {2023-04-27},
  abstract = {Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch.},
  archiveprefix = {arxiv},
  keywords = {*,background,Computer Science - Machine Learning,GNN,imitation learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annotation = {[A]},
  file = {/home/bruno/Zotero/storage/YRTHDXWT/Gasse et al. - 2019 - Exact Combinatorial Optimization with Graph Convol.pdf;/home/bruno/Zotero/storage/8NIYE484/1906.html}
}

@inproceedings{gasseMachineLearningCombinatorial2022,
  title = {The {{Machine Learning}} for {{Combinatorial Optimization Competition}} ({{ML4CO}}): {{Results}} and {{Insights}}},
  shorttitle = {The {{Machine Learning}} for {{Combinatorial Optimization Competition}} ({{ML4CO}})},
  booktitle = {Proceedings of the {{NeurIPS}} 2021 {{Competitions}} and {{Demonstrations Track}}},
  author = {Gasse, Maxime and Bowly, Simon and Cappart, Quentin and Charfreitag, Jonas and Charlin, Laurent and Ch{\'e}telat, Didier and Chmiela, Antonia and Dumouchelle, Justin and Gleixner, Ambros and Kazachkov, Aleksandr M. and Khalil, Elias and Lichocki, Pawel and Lodi, Andrea and Lubin, Miles and Maddison, Chris J. and Christopher, Morris and Papageorgiou, Dimitri J. and Parjadis, Augustin and Pokutta, Sebastian and Prouvost, Antoine and Scavuzzo, Lara and Zarpellon, Giulia and Yang, Linxin and Lai, Sha and Wang, Akang and Luo, Xiaodong and Zhou, Xiang and Huang, Haohan and Shao, Shengcheng and Zhu, Yuanming and Zhang, Dong and Quan, Tao and Cao, Zixuan and Xu, Yang and Huang, Zhewei and Zhou, Shuchang and Binbin, Chen and Minggui, He and Hao, Hao and Zhiyu, Zhang and Zhiwu, An and Kun, Mao},
  year = {2022},
  month = jul,
  pages = {220--231},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-04-28},
  abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning as a new approach for solving combinatorial problems, either directly as solvers or by enhancing exact solvers. Based on this context, the ML4CO aims at improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components. The competition featured three challenging tasks: finding the best feasible solution, producing the tightest optimality certificate, and giving an appropriate solver configuration. Three realistic datasets were considered: balanced item placement, workload apportionment, and maritime inventory routing. This last dataset was kept anonymous for the contestants.},
  langid = {english},
  keywords = {*,end-to-end,imitation learning},
  file = {/home/bruno/Zotero/storage/T24IU4DB/Gasse et al. - 2022 - The Machine Learning for Combinatorial Optimizatio.pdf}
}

@inproceedings{gasseMachineLearningCombinatorial2022a,
  title = {The {{Machine Learning}} for {{Combinatorial Optimization Competition}} ({{ML4CO}}): {{Results}} and {{Insights}}},
  shorttitle = {The {{Machine Learning}} for {{Combinatorial Optimization Competition}} ({{ML4CO}})},
  booktitle = {Proceedings of the {{NeurIPS}} 2021 {{Competitions}} and {{Demonstrations Track}}},
  author = {Gasse, Maxime and Bowly, Simon and Cappart, Quentin and Charfreitag, Jonas and Charlin, Laurent and Ch{\'e}telat, Didier and Chmiela, Antonia and Dumouchelle, Justin and Gleixner, Ambros and Kazachkov, Aleksandr M. and Khalil, Elias and Lichocki, Pawel and Lodi, Andrea and Lubin, Miles and Maddison, Chris J. and Christopher, Morris and Papageorgiou, Dimitri J. and Parjadis, Augustin and Pokutta, Sebastian and Prouvost, Antoine and Scavuzzo, Lara and Zarpellon, Giulia and Yang, Linxin and Lai, Sha and Wang, Akang and Luo, Xiaodong and Zhou, Xiang and Huang, Haohan and Shao, Shengcheng and Zhu, Yuanming and Zhang, Dong and Quan, Tao and Cao, Zixuan and Xu, Yang and Huang, Zhewei and Zhou, Shuchang and Binbin, Chen and Minggui, He and Hao, Hao and Zhiyu, Zhang and Zhiwu, An and Kun, Mao},
  year = {2022},
  month = jul,
  pages = {220--231},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-03},
  abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning as a new approach for solving combinatorial problems, either directly as solvers or by enhancing exact solvers. Based on this context, the ML4CO aims at improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components. The competition featured three challenging tasks: finding the best feasible solution, producing the tightest optimality certificate, and giving an appropriate solver configuration. Three realistic datasets were considered: balanced item placement, workload apportionment, and maritime inventory routing. This last dataset was kept anonymous for the contestants.},
  langid = {english},
  file = {/home/bruno/Zotero/storage/5TEMXUND/Gasse et al. - 2022 - The Machine Learning for Combinatorial Optimizatio.pdf}
}

@inproceedings{gilmerNeuralMessagePassing2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  year = {2017},
  month = jul,
  pages = {1263--1272},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-02},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  langid = {english},
  file = {/home/bruno/Zotero/storage/N27B3BRC/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf}
}

@article{gloverHEURISTICSINTEGERPROGRAMMING1977,
  title = {{{HEURISTICS FOR INTEGER PROGRAMMING USING SURROGATE CONSTRAINTS}}},
  author = {Glover, Fred},
  year = {1977},
  month = jan,
  journal = {Decision Sciences},
  volume = {8},
  number = {1},
  pages = {156--166},
  issn = {0011-7315, 1540-5915},
  doi = {10.1111/j.1540-5915.1977.tb01074.x},
  urldate = {2024-04-11},
  abstract = {ABSTRACT                            This paper proposes a class of surrogate constraint heuristics for obtaining approximate, near optimal solutions to integer programming problems. These heuristics are based on a simple framework that illuminates the character of several earlier heuristic proposals and provides a variety of new alternatives. The paper also proposes additional heuristics that can be used either to supplement the surrogate constraint procedures or to provide independent solution strategies. Preliminary computational results are reported for applying one of these alternatives to a class of nonlinear generalized set covering problems involving approximately 100 constraints and 300--500 integer variables. The solutions obtained by the tested procedure had objective function values twice as good as values obtained by standard approaches (               e.g.               , reducing the best objective function values of other methods from 85 to 40 on the average. Total solution time for the tested procedure ranged from ten to twenty seconds on the CDC 6600.},
  langid = {english},
  file = {/home/bruno/Zotero/storage/VF4I762C/Glover - 1977 - HEURISTICS FOR INTEGER PROGRAMMING USING SURROGATE.pdf}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT Press}
}

@misc{goodfellowQualitativelyCharacterizingNeural2015,
  title = {Qualitatively Characterizing Neural Network Optimization Problems},
  author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
  year = {2015},
  month = may,
  number = {arXiv:1412.6544},
  eprint = {1412.6544},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6544},
  urldate = {2024-05-01},
  abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/bruno/Zotero/storage/EJ8I37L9/1412.html}
}

@article{hamiltonInductiveRepresentationLearning2017,
  title = {Inductive Representation Learning on Large Graphs},
  author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@misc{hanGNNGuidedPredictandSearchFramework2023,
  title = {A {{GNN-Guided Predict-and-Search Framework}} for {{Mixed-Integer Linear Programming}}},
  author = {Han, Qingyu and Yang, Linxin and Chen, Qian and Zhou, Xiang and Zhang, Dong and Wang, Akang and Sun, Ruoyu and Luo, Xiaodong},
  year = {2023},
  month = mar,
  number = {arXiv:2302.05636},
  eprint = {2302.05636},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.05636},
  urldate = {2023-04-27},
  abstract = {Mixed-integer linear programming (MILP) is widely employed for modeling combinatorial optimization problems. In practice, similar MILP instances with only coefficient variations are routinely solved, and machine learning (ML) algorithms are capable of capturing common patterns across these MILP instances. In this work, we combine ML with optimization and propose a novel predict-and-search framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize graph neural networks to predict the marginal probability of each variable, and then search for the best feasible solution within a properly defined ball around the predicted solution. We conduct extensive experiments on public datasets, and computational results demonstrate that our proposed framework achieves 51.1\% and 9.9\% performance improvements to MILP solvers SCIP and Gurobi on primal gaps, respectively.},
  archiveprefix = {arxiv},
  keywords = {end-to-end,GNN,imitation learning,Mathematics - Optimization and Control,Pred-and-opt},
  file = {/home/bruno/Zotero/storage/CYLS7UJ4/Han et al. - 2023 - A GNN-Guided Predict-and-Search Framework for Mixe.pdf;/home/bruno/Zotero/storage/QJDR4L2K/2302.html}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  year = {2009},
  series = {Springer Series in Statistics},
  edition = {2nd ed},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  lccn = {Q325.5 .H39 2009},
  keywords = {Bioinformatics,Computational intelligence,Data mining,Forecasting,Inference,Machine learning,Methodology,Statistics}
}

@article{jacobsIncreasedRatesConvergence1988,
  title = {Increased Rates of Convergence through Learning Rate Adaptation},
  author = {Jacobs, Robert A.},
  year = {1988},
  month = jan,
  journal = {Neural Networks},
  volume = {1},
  number = {4},
  pages = {295--307},
  issn = {08936080},
  doi = {10.1016/0893-6080(88)90003-2},
  urldate = {2024-05-01},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@incollection{karpReducibilityCombinatorialProblems1972,
  title = {Reducibility among {{Combinatorial Problems}}},
  booktitle = {Complexity of {{Computer Computations}}},
  author = {Karp, Richard M.},
  editor = {Miller, Raymond E. and Thatcher, James W. and Bohlinger, Jean D.},
  year = {1972},
  pages = {85--103},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-2001-2_9},
  urldate = {2024-04-11},
  isbn = {978-1-4684-2003-6 978-1-4684-2001-2},
  langid = {english}
}

@article{khalilMIPGNNDataDrivenFramework2022,
  title = {{{MIP-GNN}}: {{A Data-Driven Framework}} for {{Guiding Combinatorial Solvers}}},
  shorttitle = {{{MIP-GNN}}},
  author = {Khalil, Elias B. and Morris, Christopher and Lodi, Andrea},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {9},
  pages = {10219--10227},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i9.21262},
  urldate = {2023-04-27},
  abstract = {Mixed-integer programming (MIP) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program (MILP) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary MILPs. Our code and appendix are publicly available at https://github.com/lyeskhalil/mipGNN.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {*,end-to-end,GNN,imitation learning,Machine Learning (ML),Pred-and-opt},
  file = {/home/bruno/Zotero/storage/QZTXDMDS/Khalil et al. - 2022 - MIP-GNN A Data-Driven Framework for Guiding Combi.pdf}
}

@article{kingma2014adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  archiveprefix = {arxiv}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  publisher = {arXiv},
  urldate = {2023-03-13},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/bruno/Zotero/storage/6AIG5A56/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf}
}

@article{larsenPredictingTacticalSolutions2022,
  title = {Predicting {{Tactical Solutions}} to {{Operational Planning Problems Under Imperfect Information}}},
  author = {Larsen, Eric and Lachapelle, S{\'e}bastien and Bengio, Yoshua and Frejinger, Emma and {Lacoste-Julien}, Simon and Lodi, Andrea},
  year = {2022},
  month = jan,
  journal = {INFORMS Journal on Computing},
  volume = {34},
  number = {1},
  pages = {227--242},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.2021.1091},
  urldate = {2023-05-22},
  abstract = {This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions (TDOSs). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected TDOS associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs.},
  keywords = {deep learning,end-to-end,imitation learning,integer linear programming,stochastic programming,supervised learning},
  file = {/home/bruno/Zotero/storage/T86LLTX9/Larsen et al. - 2022 - Predicting Tactical Solutions to Operational Plann.pdf}
}

@article{luciaComputationalNanosatelliteConstellations2021,
  title = {Computational {{Nanosatellite Constellations}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Computational {{Nanosatellite Constellations}}},
  author = {Lucia, Brandon and Denby, Brad and Manchester, Zachary and Desai, Harsh and Ruppel, Emily and Colin, Alexei},
  year = {2021},
  month = jun,
  journal = {GetMobile: Mobile Computing and Communications},
  volume = {25},
  number = {1},
  pages = {16--23},
  issn = {2375-0529, 2375-0537},
  doi = {10.1145/3471440.3471446},
  urldate = {2024-05-21},
  abstract = {As rocket launch cadences increase, access to space rises dramatically - setting the stage for the next space industry surge. New, smaller, and less expensive satellites - now "nanosatellites" - can be deployed en masse to form constellations of hundreds, thousands, or even tens of thousands of devices [27, 40, 41, 16, 17, 18, 43]. A constellation of nanosatellites equipped with sensors (e.g., visual or hyperspectral cameras, particle detectors, or magnetometers) and radios provides a first-time opportunity for orbital swarm sensing to synthesize data from the unique vantage point of low-Earth orbit (LEO).},
  langid = {english}
}

@book{maniezzoMatheuristicsAlgorithmsImplementations2021,
  title = {Matheuristics: {{Algorithms}} and {{Implementations}}},
  shorttitle = {Matheuristics},
  author = {Maniezzo, Vittorio and Boschetti, Marco Antonio and St{\"u}tzle, Thomas},
  year = {2021},
  series = {{{EURO Advanced Tutorials}} on {{Operational Research}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-70277-9},
  urldate = {2024-04-16},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-030-70276-2 978-3-030-70277-9},
  langid = {english}
}

@article{marcelinoCriticalEmbeddedSystem2020,
  title = {A {{Critical Embedded System Challenge}}: {{The FloripaSat-1 Mission}}},
  shorttitle = {A {{Critical Embedded System Challenge}}},
  author = {Marcelino, Gabriel Mariano and {Vega-Martinez}, Sara and Seman, Laio Oriel and Kessler Slongo, Leonardo and Bezerra, Eduardo Augusto},
  year = {2020},
  month = feb,
  journal = {IEEE Latin America Transactions},
  volume = {18},
  number = {02},
  pages = {249--256},
  issn = {1548-0992},
  doi = {10.1109/TLA.2020.9085277},
  urldate = {2024-06-14},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{millerIntegerProgrammingFormulation1960,
  title = {Integer {{Programming Formulation}} of {{Traveling Salesman Problems}}},
  author = {Miller, C. E. and Tucker, A. W. and Zemlin, R. A.},
  year = {1960},
  month = oct,
  journal = {Journal of the ACM},
  volume = {7},
  number = {4},
  pages = {326--329},
  issn = {0004-5411},
  doi = {10.1145/321043.321046},
  urldate = {2024-05-14},
  abstract = {It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known ``Travelling Salesman Problem'' in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, {\dots} , n. He leaves from a ``base city'' indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman. Note that if t is fixed, then for the problem to have a solution we must have tp {$\geqq$} n. For t = 1, p {$\geqq$} n, we have the standard traveling salesman problem. Let dij (i {$\neq$} j = 0, 1, {\dots} , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form {$\sum$}0{$\leqq$}i{$\neq$}j{$\leqq$}n{$\sum$} dijxij over the set determined by the relations {$\sum$}ni=0i{$\neq$}j xij = 1 (j = 1, {\dots} , n) {$\sum$}nj=0j{$\neq$}i xij = 1 (i = 1, {\dots} , n) ui - uj + pxij {$\leqq$} p - 1 (1 {$\leqq$} i {$\neq$} j {$\leqq$} n) where the xij are non-negative integers and the ui (i = 1, {\dots}, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.) If t is fixed it is necessary to add the additional relation: {$\sum$}nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2). Consider a feasible solution to (2). The number of returns to city 0 is given by {$\sum$}ni=1 xi0. The constraints of the form {$\sum$} xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 {$\neq$} 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 {$<$} k. Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 {$\leqq$} p - 1 or uri - uri + 1 {$\leqq$} - 1. Summing from i = j to k - 1, we have urj - urk = 0 {$\leqq$} j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , {\dots} , xrprp+1 = 1 with all ri {$\neq$} 0. Then, as before, ur1 - urp+1 {$\leqq$} - p or urp+1 - ur1 {$\geqq$} p. But we have urp+1 - ur1 + pxrp+1r1 {$\leqq$} p - 1 or urp+1 - ur1 {$\leqq$} p (1 - xrp+1r1) - 1 {$\leqq$} p - 1, which is a contradiction. Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj {$\leqq$} p - 1. The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables. The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes. The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ] The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution. The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps. The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps. The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function. It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.},
  file = {/home/bruno/Zotero/storage/7ZLIBSSP/Miller et al. - 1960 - Integer Programming Formulation of Traveling Sales.pdf}
}

@article{morschfilhoComprehensiveAttitudeFormulation2020,
  title = {A Comprehensive Attitude Formulation with Spin for Numerical Model of Irradiance for {{CubeSats}} and {{Picosats}}},
  author = {Morsch Filho, Edemar and Nicolau, Vicente De Paulo and Paiva, Kleber Vieira De and Possamai, Talita Sauter},
  year = {2020},
  month = mar,
  journal = {Applied Thermal Engineering},
  volume = {168},
  pages = {114859},
  issn = {13594311},
  doi = {10.1016/j.applthermaleng.2019.114859},
  urldate = {2024-06-14},
  langid = {english}
}

@book{murphyMachineLearningProbabilistic2013,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2013},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {4. print. (fixed many typos)},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-01802-9},
  langid = {english}
}

@article{nagelNanosatellitesAppliedOptical2020,
  title = {Nanosatellites Applied to Optical {{Earth}} Observation: A Review},
  shorttitle = {Nanosatellites Applied to Optical {{Earth}} Observation},
  author = {Nagel, Gustavo Willy and Novo, Evlyn M{\'a}rcia Le{\~a}o De Moraes and Kampel, Milton},
  year = {2020},
  month = jun,
  journal = {Ambiente e Agua - An Interdisciplinary Journal of Applied Science},
  volume = {15},
  number = {3},
  pages = {1},
  issn = {1980-993X},
  doi = {10.4136/ambi-agua.2513},
  urldate = {2024-05-21},
  abstract = {Nanosatellites and CubeSats were first developed for educational purposes. However, their low cost and short development cycle made nanosatellite constellations an affordable option for observing the Earth by remote sensing, increasing the frequency of high-resolution imagery, which is fundamental for studying and monitoring dynamic processes. In this sense, although still incipient, nanosatellite applications and proposed Earth observation missions are steadily growing in number and scientific fields. There are several initiatives from universities, space agencies and private companies to launch new nanosatellite missions. These initiatives are actively investigating new technologies to improve image quality and studying ways to increase acquisition frequency through the launch of larger constellations. So far, the private sector is leading the development of new missions, with proposals ranging from 12 to more than one thousand nanosatellite constellations. Furthermore, new nanosatellite missions have been proposed to tackle specific applications, such as natural disasters, or to test improvements on nanosatellite spatial, temporal and radiometric resolution. The unprecedented combination of high spatial and temporal resolution from nanosatellite constellations associated with improvement efforts in sensor quality is promising and may represent a trend to replace the era of large satellites for smaller and cheaper nanosatellites. This article first reports on the development and new nanosatellite missions of space agencies, universities and private companies. Then a systematic review of published articles using the most successful private constellation (PlanetScope and Doves) is presented and the principal papers are discussed.},
  file = {/home/bruno/Zotero/storage/G2KSG5G6/Nagel et al. - 2020 - Nanosatellites applied to optical Earth observatio.pdf}
}

@misc{nairSolvingMixedInteger2021,
  title = {Solving {{Mixed Integer Programs Using Neural Networks}}},
  author = {Nair, Vinod and Bartunov, Sergey and Gimeno, Felix and {von Glehn}, Ingrid and Lichocki, Pawel and Lobov, Ivan and O'Donoghue, Brendan and Sonnerat, Nicolas and Tjandraatmadja, Christian and Wang, Pengming and Addanki, Ravichandra and Hapuarachchi, Tharindi and Keck, Thomas and Keeling, James and Kohli, Pushmeet and Ktena, Ira and Li, Yujia and Vinyals, Oriol and Zwols, Yori},
  year = {2021},
  month = jul,
  number = {arXiv:2012.13349},
  eprint = {2012.13349},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.13349},
  urldate = {2023-05-08},
  abstract = {Mixed Integer Programming (MIP) solvers rely on an array of sophisticated heuristics developed with decades of research to solve large-scale MIP instances encountered in practice. Machine learning offers to automatically construct better heuristics from data by exploiting shared structure among instances in the data. This paper applies learning to the two key sub-tasks of a MIP solver, generating a high-quality joint variable assignment, and bounding the gap in objective value between that assignment and an optimal one. Our approach constructs two corresponding neural network-based components, Neural Diving and Neural Branching, to use in a base MIP solver such as SCIP. Neural Diving learns a deep neural network to generate multiple partial assignments for its integer variables, and the resulting smaller MIPs for un-assigned variables are solved with SCIP to construct high quality joint assignments. Neural Branching learns a deep neural network to make variable selection decisions in branch-and-bound to bound the objective value gap with a small tree. This is done by imitating a new variant of Full Strong Branching we propose that scales to large instances using GPUs. We evaluate our approach on six diverse real-world datasets, including two Google production datasets and MIPLIB, by training separate neural networks on each. Most instances in all the datasets combined have \$10{\textasciicircum}3-10{\textasciicircum}6\$ variables and constraints after presolve, which is significantly larger than previous learning approaches. Comparing solvers with respect to primal-dual gap averaged over a held-out set of instances, the learning-augmented SCIP is 2x to 10x better on all datasets except one on which it is \$10{\textasciicircum}5\$x better, at large time limits. To the best of our knowledge, ours is the first learning approach to demonstrate such large improvements over SCIP on both large-scale real-world application datasets and MIPLIB.},
  archiveprefix = {arxiv},
  keywords = {*,Computer Science - Artificial Intelligence,Computer Science - Discrete Mathematics,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,end-to-end,imitation learning,Mathematics - Optimization and Control},
  file = {/home/bruno/Zotero/storage/84IBM572/Nair et al. - 2021 - Solving Mixed Integer Programs Using Neural Networ.pdf;/home/bruno/Zotero/storage/7NQQJVA5/2012.html}
}

@book{nemhauserIntegerCombinatorialOptimization1999,
  title = {Integer and Combinatorial Optimization},
  author = {Nemhauser, George L. and Wolsey, Laurence A.},
  year = {1999},
  series = {Wiley-{{Interscience}} Series in Discrete Mathematics and Optimization},
  publisher = {Wiley},
  address = {New York, NY Weinheim},
  isbn = {978-0-471-35943-2 978-0-471-82819-8},
  langid = {english},
  file = {/home/bruno/Zotero/storage/GBD4PUKS/Nemhauser and Wolsey - 1999 - Integer and combinatorial optimization.pdf}
}

@incollection{nemhauserScopeIntegerCombinatorial1988,
  title = {The {{Scope}} of {{Integer}} and {{Combinatorial Optimization}}},
  booktitle = {Integer and {{Combinatorial Optimization}}},
  author = {Nemhauser, George and Wolsey, Laurence},
  year = {1988},
  month = jun,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781118627372},
  urldate = {2024-03-25},
  isbn = {978-0-471-82819-8 978-1-118-62737-2},
  langid = {english}
}

@misc{nesterovMethodSolvingConvex1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}}(1/K**2)},
  author = {Nesterov, Y.},
  year = {1983},
  volume = {269},
  number = {3},
  pages = {543}
}

@article{polyakAccelerationStochasticApproximation1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  year = {1992},
  month = jul,
  journal = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/0330046},
  urldate = {2024-05-01},
  langid = {english}
}

@article{rigoBranchandpriceAlgorithmNanosatellite2022,
  title = {A Branch-and-Price Algorithm for Nanosatellite Task Scheduling to Improve Mission Quality-of-Service},
  author = {Rigo, Cezar Ant{\^o}nio and Seman, Laio Oriel and Camponogara, Eduardo and Morsch Filho, Edemar and Bezerra, Eduardo Augusto and Munari, Pedro},
  year = {2022},
  month = nov,
  journal = {European Journal of Operational Research},
  volume = {303},
  number = {1},
  pages = {168--183},
  issn = {03772217},
  doi = {10.1016/j.ejor.2022.02.040},
  urldate = {2024-05-21},
  langid = {english}
}

@article{rigoNanosatelliteTaskScheduling2021,
  title = {A Nanosatellite Task Scheduling Framework to Improve Mission Value Using Fuzzy Constraints},
  author = {Rigo, Cezar Ant{\^o}nio and Seman, Laio Oriel and Camponogara, Eduardo and Morsch Filho, Edemar and Bezerra, Eduardo Augusto},
  year = {2021},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {175},
  pages = {114784},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.114784},
  urldate = {2024-05-21},
  langid = {english}
}

@article{rigoTaskSchedulingOptimal2021,
  title = {Task Scheduling for Optimal Power Management and Quality-of-Service Assurance in {{CubeSats}}},
  author = {Rigo, Cezar Ant{\^o}nio and Seman, Laio Oriel and Camponogara, Eduardo and Morsch Filho, Edemar and Bezerra, Eduardo Augusto},
  year = {2021},
  month = feb,
  journal = {Acta Astronautica},
  volume = {179},
  pages = {550--560},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2020.11.016},
  urldate = {2024-05-21},
  langid = {english}
}

@article{saeedCubeSatCommunicationsRecent2020,
  title = {{{CubeSat Communications}}: {{Recent Advances}} and {{Future Challenges}}},
  shorttitle = {{{CubeSat Communications}}},
  author = {Saeed, Nasir and Elzanaty, Ahmed and Almorad, Heba and Dahrouj, Hayssam and {Al-Naffouri}, Tareq Y. and Alouini, Mohamed-Slim},
  year = 2020,
  journal = {IEEE Communications Surveys \& Tutorials},
  volume = {22},
  number = {3},
  pages = {1839--1862},
  issn = {1553-877X, 2373-745X},
  doi = {10.1109/COMST.2020.2990499},
  urldate = {2024-05-21},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/bruno/Zotero/storage/PS564JR7/Saeed et al. - 2020 - CubeSat Communications Recent Advances and Future.pdf}
}

@article{sanchez-lengelingGentleIntroductionGraph2021,
  title = {A {{Gentle Introduction}} to {{Graph Neural Networks}}},
  author = {{Sanchez-Lengeling}, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alex},
  year = {2021},
  month = aug,
  journal = {Distill},
  volume = {6},
  number = {8},
  pages = {10.23915/distill.00033},
  issn = {2476-0757},
  doi = {10.23915/distill.00033},
  urldate = {2024-05-02}
}

@article{semanEnergyAwareTaskScheduling2022,
  title = {An {{Energy-Aware Task Scheduling}} for {{Quality-of-Service Assurance}} in {{Constellations}} of {{Nanosatellites}}},
  author = {Seman, Laio Oriel and Ribeiro, Brenda F. and Rigo, Cezar A. and Filho, Edemar Morsch and Camponogara, Eduardo and Leonardi, Rodrigo and Bezerra, Eduardo A.},
  year = {2022},
  month = may,
  journal = {Sensors},
  volume = {22},
  number = {10},
  pages = {3715},
  issn = {1424-8220},
  doi = {10.3390/s22103715},
  urldate = {2024-05-21},
  abstract = {When managing a constellation of nanosatellites, one may leverage this structure to improve the mission's quality-of-service (QoS) by optimally distributing the tasks during an orbit. In this sense, this research proposes an offline energy-aware task scheduling problem formulation regarding the specifics of constellations, by considering whether the tasks are individual, collective, or stimulated to be redundant. By providing such an optimization framework, the idea of estimating an offline task schedule can serve as a baseline for the constellation design phase. For example, given a particular orbit, from the simulation of an irradiance model, the engineer can estimate how the mission value is affected by the inclusion or exclusion of individuals objects. The proposed model, given in the form of a multi-objective mixed-integer linear programming model, is illustrated in this work for several illustrative scenarios considering different sets of tasks and constellations. We also perform an analysis of the Pareto-optimal frontier of the problem, identifying the feasible trade-off points between constellation and individual tasks. This information can be useful to the decision-maker (mission operator) when planning the behavior in orbit.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/bruno/Zotero/storage/2JBL6TVZ/Seman et al. - 2022 - An Energy-Aware Task Scheduling for Quality-of-Ser.pdf}
}

@article{shiromaCubeSatsBrightFuture2011,
  title = {{{CubeSats}}: {{A}} Bright Future for Nanosatellites},
  shorttitle = {{{CubeSats}}},
  author = {Shiroma, Wayne A. and Martin, Larry K. and Akagi, Justin M. and Akagi, Jason T. and Wolfe, Byron L. and Fewell, Bryan A. and Ohta, Aaron T.},
  year = {2011},
  month = mar,
  journal = {Central European Journal of Engineering},
  volume = {1},
  number = {1},
  pages = {9--15},
  issn = {2081-9927},
  doi = {10.2478/s13531-011-0007-8},
  urldate = {2023-08-02},
  abstract = {Interest in the CubeSat class of nanosatellites has surged in recent years. The reduced development time and launch cost of these platforms, compared to conventional large satellites, have made them attractive for applications ranging from technology demonstrations to biological experiments to space weather research. This paper surveys recent and upcoming missions pursued by a variety of academic and government groups around the world.},
  langid = {english},
  keywords = {CubeSat,Nanosatellite,Picosatellite,Small Satellite},
  file = {/home/bruno/Zotero/storage/HEBQNHG8/Shiroma et al. - 2011 - CubeSats A bright future for nanosatellites.pdf}
}

@book{sierksmaLinearIntegerOptimization2015,
  title = {Linear and Integer Optimization: Theory and Practice},
  shorttitle = {Linear and Integer Optimization},
  author = {Sierksma, Gerard and Zwols, Yori},
  year = {2015},
  series = {Advances in Applied Mathematics},
  edition = {3rd ed},
  publisher = {CRC Chapman \& Hall},
  address = {Boca Raton},
  isbn = {978-1-4987-1016-9},
  langid = {english},
  lccn = {519.72}
}

@book{vanderbeiLinearProgrammingFoundations1998,
  title = {Linear Programming: Foundations and Extensions},
  shorttitle = {Linear Programming},
  author = {Vanderbei, Robert J.},
  year = {1998},
  series = {International Series in Operations Research \& Management Science},
  edition = {3. printing},
  number = {4},
  publisher = {Kluwer Acad. Publ},
  address = {Boston},
  isbn = {978-0-7923-8141-9 978-0-7923-9804-2},
  langid = {english},
  file = {/home/bruno/Zotero/storage/DN4XE2B6/Vanderbei - 1998 - Linear programming foundations and extensions.pdf}
}

@book{vapnikNatureStatisticalLearning2000,
  title = {The {{Nature}} of {{Statistical Learning Theory}}},
  author = {Vapnik, V. N.},
  year = {2000},
  edition = {Second edition},
  publisher = {Springer New York : Imprint : Springer},
  address = {New York, NY},
  abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader AT \& T Labs-Research and Professor of London University. He is one of the founders of},
  isbn = {978-1-4757-3264-1},
  langid = {english},
  annotation = {OCLC: 864225872}
}

@inproceedings{velickovic2018graph,
  title = {Graph Attention Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018}
}

@incollection{wolseyFormulations2020,
  title = {Formulations},
  booktitle = {Integer {{Programming}}},
  author = {Wolsey, Laurence},
  year = {2020},
  month = oct,
  edition = {1},
  pages = {1--23},
  publisher = {Wiley},
  doi = {10.1002/9781119606475},
  urldate = {2024-03-25},
  isbn = {978-1-119-60653-6 978-1-119-60647-5},
  langid = {english}
}

@book{wolseyIntegerProgramming1998,
  title = {Integer Programming},
  author = {Wolsey, Laurence A.},
  year = {1998},
  series = {Wiley-{{Interscience}} Series in Discrete Mathematics and Optimization},
  publisher = {Wiley},
  address = {New York},
  isbn = {978-0-471-28366-9},
  lccn = {T57.74 .W67 1998},
  keywords = {Integer programming}
}
