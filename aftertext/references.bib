@article{bengioMachineLearningCombinatorial2021,
  title = {Machine Learning for Combinatorial Optimization: {{A}} Methodological Tour d’horizon},
  shorttitle = {Machine Learning for Combinatorial Optimization},
  author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
  date = {2021-04-16},
  journaltitle = {European Journal of Operational Research},
  shortjournal = {European Journal of Operational Research},
  volume = {290},
  number = {2},
  pages = {405--421},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.07.063},
  url = {https://www.sciencedirect.com/science/article/pii/S0377221720306895},
  urldate = {2023-04-19},
  abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
  langid = {english},
  keywords = {*,background,Branch and bound,Combinatorial optimization,DONE,Machine learning,Mixed-integer programming solvers,SURVEY},
  annotation = {[A] DONE},
  file = {/home/bruno/Zotero/storage/CAVDYZCL/Bengio et al. - 2021 - Machine learning for combinatorial optimization A.pdf;/home/bruno/Zotero/storage/DCDLVZKT/S0377221720306895.html}
}

@article{camponogaraModelsAlgorithmsOptimal2015,
  title = {Models and {{Algorithms}} for {{Optimal Piecewise-Linear Function Approximation}}},
  author = {Camponogara, Eduardo and Nazari, Luiz Fernando},
  date = {2015},
  journaltitle = {Mathematical Problems in Engineering},
  shortjournal = {Mathematical Problems in Engineering},
  volume = {2015},
  pages = {1--9},
  issn = {1024-123X, 1563-5147},
  doi = {10.1155/2015/876862},
  url = {http://www.hindawi.com/journals/mpe/2015/876862/},
  urldate = {2024-03-25},
  abstract = {Piecewise-linear functions can approximate nonlinear and unknown functions for which only sample points are available. This paper presents a range of piecewise-linear models and algorithms to aid engineers to find an approximation that fits best their applications. The models include piecewise-linear functions with a fixed and maximum number of linear segments, lower and upper envelopes, strategies to ensure continuity, and a generalization of these models for stochastic functions whose data points are random variables. Derived from recursive formulations, the algorithms are applied to the approximation of the production function of gas-lifted oil wells.},
  langid = {english},
  file = {/home/bruno/Zotero/storage/XFGJL4JA/Camponogara and Nazari - 2015 - Models and Algorithms for Optimal Piecewise-Linear.pdf}
}

@inproceedings{gasseMachineLearningCombinatorial2022,
  title = {The {{Machine Learning}} for {{Combinatorial Optimization Competition}} ({{ML4CO}}): {{Results}} and {{Insights}}},
  shorttitle = {The {{Machine Learning}} for {{Combinatorial Optimization Competition}} ({{ML4CO}})},
  booktitle = {Proceedings of the {{NeurIPS}} 2021 {{Competitions}} and {{Demonstrations Track}}},
  author = {Gasse, Maxime and Bowly, Simon and Cappart, Quentin and Charfreitag, Jonas and Charlin, Laurent and Chételat, Didier and Chmiela, Antonia and Dumouchelle, Justin and Gleixner, Ambros and Kazachkov, Aleksandr M. and Khalil, Elias and Lichocki, Pawel and Lodi, Andrea and Lubin, Miles and Maddison, Chris J. and Christopher, Morris and Papageorgiou, Dimitri J. and Parjadis, Augustin and Pokutta, Sebastian and Prouvost, Antoine and Scavuzzo, Lara and Zarpellon, Giulia and Yang, Linxin and Lai, Sha and Wang, Akang and Luo, Xiaodong and Zhou, Xiang and Huang, Haohan and Shao, Shengcheng and Zhu, Yuanming and Zhang, Dong and Quan, Tao and Cao, Zixuan and Xu, Yang and Huang, Zhewei and Zhou, Shuchang and Binbin, Chen and Minggui, He and Hao, Hao and Zhiyu, Zhang and Zhiwu, An and Kun, Mao},
  date = {2022-07-20},
  pages = {220--231},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v176/gasse22a.html},
  urldate = {2023-04-28},
  abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning as a new approach for solving combinatorial problems, either directly as solvers or by enhancing exact solvers. Based on this context, the ML4CO aims at improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components. The competition featured three challenging tasks: finding the best feasible solution, producing the tightest optimality certificate, and giving an appropriate solver configuration. Three realistic datasets were considered: balanced item placement, workload apportionment, and maritime inventory routing. This last dataset was kept anonymous for the contestants.},
  eventtitle = {{{NeurIPS}} 2021 {{Competitions}} and {{Demonstrations Track}}},
  langid = {english},
  keywords = {*,end-to-end,imitation learning},
  file = {/home/bruno/Zotero/storage/T24IU4DB/Gasse et al. - 2022 - The Machine Learning for Combinatorial Optimizatio.pdf}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press}
}

@online{hanGNNGuidedPredictandSearchFramework2023,
  title = {A {{GNN-Guided Predict-and-Search Framework}} for {{Mixed-Integer Linear Programming}}},
  author = {Han, Qingyu and Yang, Linxin and Chen, Qian and Zhou, Xiang and Zhang, Dong and Wang, Akang and Sun, Ruoyu and Luo, Xiaodong},
  date = {2023-03-06},
  eprint = {2302.05636},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2302.05636},
  url = {http://arxiv.org/abs/2302.05636},
  urldate = {2023-04-27},
  abstract = {Mixed-integer linear programming (MILP) is widely employed for modeling combinatorial optimization problems. In practice, similar MILP instances with only coefficient variations are routinely solved, and machine learning (ML) algorithms are capable of capturing common patterns across these MILP instances. In this work, we combine ML with optimization and propose a novel predict-and-search framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize graph neural networks to predict the marginal probability of each variable, and then search for the best feasible solution within a properly defined ball around the predicted solution. We conduct extensive experiments on public datasets, and computational results demonstrate that our proposed framework achieves 51.1\% and 9.9\% performance improvements to MILP solvers SCIP and Gurobi on primal gaps, respectively.},
  pubstate = {preprint},
  keywords = {end-to-end,GNN,imitation learning,Mathematics - Optimization and Control,Pred-and-opt},
  file = {/home/bruno/Zotero/storage/CYLS7UJ4/Han et al. - 2023 - A GNN-Guided Predict-and-Search Framework for Mixe.pdf;/home/bruno/Zotero/storage/QJDR4L2K/2302.html}
}

@article{khalilMIPGNNDataDrivenFramework2022,
  title = {{{MIP-GNN}}: {{A Data-Driven Framework}} for {{Guiding Combinatorial Solvers}}},
  shorttitle = {{{MIP-GNN}}},
  author = {Khalil, Elias B. and Morris, Christopher and Lodi, Andrea},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {9},
  pages = {10219--10227},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i9.21262},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/21262},
  urldate = {2023-04-27},
  abstract = {Mixed-integer programming (MIP) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program (MILP) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary MILPs. Our code and appendix are publicly available at https://github.com/lyeskhalil/mipGNN.},
  issue = {9},
  langid = {english},
  keywords = {*,end-to-end,GNN,imitation learning,Machine Learning (ML),Pred-and-opt},
  file = {/home/bruno/Zotero/storage/QZTXDMDS/Khalil et al. - 2022 - MIP-GNN A Data-Driven Framework for Guiding Combi.pdf}
}

@article{larsenPredictingTacticalSolutions2022,
  title = {Predicting {{Tactical Solutions}} to {{Operational Planning Problems Under Imperfect Information}}},
  author = {Larsen, Eric and Lachapelle, Sébastien and Bengio, Yoshua and Frejinger, Emma and Lacoste-Julien, Simon and Lodi, Andrea},
  date = {2022-01},
  journaltitle = {INFORMS Journal on Computing},
  volume = {34},
  number = {1},
  pages = {227--242},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.2021.1091},
  url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2021.1091},
  urldate = {2023-05-22},
  abstract = {This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions (TDOSs). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected TDOS associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs.},
  keywords = {deep learning,end-to-end,imitation learning,integer linear programming,stochastic programming,supervised learning},
  file = {/home/bruno/Zotero/storage/T86LLTX9/Larsen et al. - 2022 - Predicting Tactical Solutions to Operational Plann.pdf}
}

@online{nairSolvingMixedInteger2021,
  title = {Solving {{Mixed Integer Programs Using Neural Networks}}},
  author = {Nair, Vinod and Bartunov, Sergey and Gimeno, Felix and family=Glehn, given=Ingrid, prefix=von, useprefix=true and Lichocki, Pawel and Lobov, Ivan and O'Donoghue, Brendan and Sonnerat, Nicolas and Tjandraatmadja, Christian and Wang, Pengming and Addanki, Ravichandra and Hapuarachchi, Tharindi and Keck, Thomas and Keeling, James and Kohli, Pushmeet and Ktena, Ira and Li, Yujia and Vinyals, Oriol and Zwols, Yori},
  date = {2021-07-29},
  eprint = {2012.13349},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2012.13349},
  url = {http://arxiv.org/abs/2012.13349},
  urldate = {2023-05-08},
  abstract = {Mixed Integer Programming (MIP) solvers rely on an array of sophisticated heuristics developed with decades of research to solve large-scale MIP instances encountered in practice. Machine learning offers to automatically construct better heuristics from data by exploiting shared structure among instances in the data. This paper applies learning to the two key sub-tasks of a MIP solver, generating a high-quality joint variable assignment, and bounding the gap in objective value between that assignment and an optimal one. Our approach constructs two corresponding neural network-based components, Neural Diving and Neural Branching, to use in a base MIP solver such as SCIP. Neural Diving learns a deep neural network to generate multiple partial assignments for its integer variables, and the resulting smaller MIPs for un-assigned variables are solved with SCIP to construct high quality joint assignments. Neural Branching learns a deep neural network to make variable selection decisions in branch-and-bound to bound the objective value gap with a small tree. This is done by imitating a new variant of Full Strong Branching we propose that scales to large instances using GPUs. We evaluate our approach on six diverse real-world datasets, including two Google production datasets and MIPLIB, by training separate neural networks on each. Most instances in all the datasets combined have \$10\^{}3-10\^{}6\$ variables and constraints after presolve, which is significantly larger than previous learning approaches. Comparing solvers with respect to primal-dual gap averaged over a held-out set of instances, the learning-augmented SCIP is 2x to 10x better on all datasets except one on which it is \$10\^{}5\$x better, at large time limits. To the best of our knowledge, ours is the first learning approach to demonstrate such large improvements over SCIP on both large-scale real-world application datasets and MIPLIB.},
  pubstate = {preprint},
  keywords = {*,Computer Science - Artificial Intelligence,Computer Science - Discrete Mathematics,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,end-to-end,imitation learning,Mathematics - Optimization and Control},
  file = {/home/bruno/Zotero/storage/84IBM572/Nair et al. - 2021 - Solving Mixed Integer Programs Using Neural Networ.pdf;/home/bruno/Zotero/storage/7NQQJVA5/2012.html}
}

@book{nemhauserIntegerCombinatorialOptimization1999,
  title = {Integer and Combinatorial Optimization},
  author = {Nemhauser, George L. and Wolsey, Laurence A.},
  date = {1999},
  series = {Wiley-{{Interscience}} Series in Discrete Mathematics and Optimization},
  publisher = {Wiley},
  location = {New York, NY Weinheim},
  isbn = {978-0-471-35943-2 978-0-471-82819-8},
  langid = {english},
  pagetotal = {763},
  file = {/home/bruno/Zotero/storage/GBD4PUKS/Nemhauser and Wolsey - 1999 - Integer and combinatorial optimization.pdf}
}

@incollection{nemhauserScopeIntegerCombinatorial1988,
  title = {The {{Scope}} of {{Integer}} and {{Combinatorial Optimization}}},
  booktitle = {Integer and {{Combinatorial Optimization}}},
  author = {Nemhauser, George and Wolsey, Laurence},
  date = {1988-06-16},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781118627372},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118627372},
  urldate = {2024-03-25},
  isbn = {978-0-471-82819-8 978-1-118-62737-2},
  langid = {english}
}

@incollection{wolseyFormulations2020,
  title = {Formulations},
  booktitle = {Integer {{Programming}}},
  author = {Wolsey, Laurence},
  date = {2020-10-20},
  edition = {1},
  pages = {1--23},
  publisher = {Wiley},
  doi = {10.1002/9781119606475},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119606475},
  urldate = {2024-03-25},
  isbn = {978-1-119-60653-6 978-1-119-60647-5},
  langid = {english}
}

@book{wolseyIntegerProgramming1998,
  title = {Integer Programming},
  author = {Wolsey, Laurence A.},
  date = {1998},
  series = {Wiley-{{Interscience}} Series in Discrete Mathematics and Optimization},
  publisher = {Wiley},
  location = {New York},
  isbn = {978-0-471-28366-9},
  pagetotal = {264},
  keywords = {Integer programming}
}
